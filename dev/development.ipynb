{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to develop the model for the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All our imports\n",
    "import torch \n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from random import randint\n",
    "import PIL\n",
    "import os\n",
    "#for all the plots to be inline\n",
    "%matplotlib inline \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, csv_path, images_folder, transform):\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.images_folder = images_folder\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        filename = self.df.values[index][0]\n",
    "        labels = np.array([1, 1, 1, 1, 1, 1, 1, 1])\n",
    "        for x in range(0, 8):\n",
    "            labels[x] = self.df.values[index][x+1]\n",
    "        image = PIL.Image.open(os.path.join(self.images_folder, filename))\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        return image, labels\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -2 -10  -1  -3  -9   8   7   8]\n",
      "[  8  -8  -9 -10  -5  -1   6   7]\n"
     ]
    }
   ],
   "source": [
    "'''Data Set manipulation'''\n",
    "transform = transforms.Compose([transforms.Grayscale(num_output_channels=1),\n",
    "                                transforms.ToTensor()])\n",
    "\n",
    "root = \"C:\\\\Users\\\\the_3\\\\Desktop\"\n",
    "\n",
    "trainDataset = CustomDataset(root + \"\\\\poly-curve-detector\\\\DataGeneration\\\\plotData\\\\labels\\\\trainPlots.csv\",\n",
    "                               root + \"\\\\poly-curve-detector\\\\DataGeneration\\\\plotData\\\\trainPlots\", transform)\n",
    "\n",
    "testDataset = CustomDataset(root + \"\\\\poly-curve-detector\\\\DataGeneration\\\\plotData\\\\labels\\\\testPlots.csv\",\n",
    "                               root + \"\\\\poly-curve-detector\\\\DataGeneration\\\\plotData\\\\testPlots\", transform)\n",
    "\n",
    "#print first label in each dataset\n",
    "#labels in order [a8, a7, a6, a5, a4, a3, a2, a1]\n",
    "image, labels = trainDataset[0]\n",
    "print(labels[0:9])\n",
    "image, labels = testDataset[0]\n",
    "print(labels[0:9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = torch.utils.data.DataLoader(trainDataset, shuffle=True, batch_size=128)\n",
    "test_set = torch.utils.data.DataLoader(testDataset, shuffle=False, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "        # more analysis required to determine the specifics of the architecture\n",
    "       \n",
    "        self.n_output = 8\n",
    "        self.n_channel = 1\n",
    "    \n",
    "        self.cnn_layers = nn.Sequential(\n",
    "        #         imageInputLayer([28 28 1])\n",
    "        #         convolution2dLayer(3,8,'Padding','same')\n",
    "                nn.Conv2d(1, 8, 3), \n",
    "        #         batchNormalizationLayer\n",
    "                nn.BatchNorm2d(8),\n",
    "        #         reluLayer\n",
    "                nn.LeakyReLU(),\n",
    "        #         averagePooling2dLayer(2,'Stride',2)\n",
    "                nn.MaxPool2d(2, 2),\n",
    "        #         convolution2dLayer(3,16,'Padding','same')\n",
    "                nn.Conv2d(8, 16, 3),\n",
    "        #         batchNormalizationLayer\n",
    "                nn.BatchNorm2d(16),\n",
    "        #         reluLayer\n",
    "                nn.LeakyReLU(),\n",
    "        #         averagePooling2dLayer(2,'Stride',2)\n",
    "                nn.MaxPool2d(2, 2),\n",
    "        #         convolution2dLayer(3,32,'Padding','same')\n",
    "                nn.Conv2d(16, 32, 3),\n",
    "        #         batchNormalizationLayer\n",
    "                nn.BatchNorm2d(32),\n",
    "        #         reluLayer\n",
    "                nn.LeakyReLU(),\n",
    "        #         convolution2dLayer(3,32,'Padding','same')\n",
    "                nn.Conv2d(32, 32, 3),\n",
    "        #         batchNormalizationLayer\n",
    "                nn.BatchNorm2d(32),\n",
    "        #         reluLayer\n",
    "                nn.LeakyReLU(),\n",
    "        #         Max pooling layer\n",
    "                nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "\n",
    "        self.n_input = 2592 # the output of maxpool 96*96 \n",
    "        #TODO:actual value might be determined from the computed output of the cnn layers\n",
    "        # https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n",
    "\n",
    "        self.fc_layers = nn.Sequential(\n",
    "#         fullyConnectedLayer(1)\n",
    "                nn.Linear(self.n_input,  1024),\n",
    "                nn.LeakyReLU(),\n",
    "                nn.Linear(1024, 256),\n",
    "                nn.LeakyReLU(),\n",
    "                nn.Linear(256, 64),\n",
    "                nn.LeakyReLU(),\n",
    "                nn.Linear(64, self.n_output),\n",
    "                nn.LeakyReLU()\n",
    "        )\n",
    "\n",
    "\n",
    "#         regressionLayer\n",
    "        self.criterion = nn.MSELoss()       \n",
    "#         dropoutLayer(0.2)\n",
    "        self.dropout =  nn.Dropout(p=0.2)\n",
    "        \n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #feedword pass through our network\n",
    "        x = self.cnn_layers(x)\n",
    "        x = x.view(x.shape[0], -1) #flatten the input tensor\n",
    "        x = self.fc_layers(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def load_checkpoint(new_model, filepath):\n",
    "        checkpoint = torch.load(filepath)\n",
    "        # model = Network(\n",
    "        #         checkpoint['input_size'], \n",
    "        #         checkpoint['output_size'],\n",
    "        #         # checkpoint['cnn_layers'],\n",
    "        #         # checkpoint['fc_layers']\n",
    "        # )\n",
    "        new_model.load_state_dict(checkpoint['state_dict'])\n",
    "        return new_model\n",
    "    \n",
    "    def save(self, dirpath):\n",
    "        self.checkpoint = {\n",
    "            'input_size': self.n_input, \n",
    "            'output_size': self.n_output,\n",
    "        #     'cnn_layers': [each. for each in model.cnn_layers],\n",
    "        #     'fc_layers': [each.out_features for each in model.fc_layers],\n",
    "            'state_dict': model.state_dict()\n",
    "        }\n",
    "        torch.save(self.checkpoint, f'{dirpath}\\\\model_checkpoint.pth')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "def validation(model, testloader, criterion):\n",
    "    accuracy = 0\n",
    "    test_loss = 0\n",
    "    for images, labels in testloader:\n",
    "\n",
    "        images = images.view(images.shape[0], -1)\n",
    "\n",
    "        impages = images.to(device)\n",
    "        labels = images.to(device)\n",
    "\n",
    "        output = model.forward(images)\n",
    "        test_loss += criterion(output, labels).item()\n",
    "\n",
    "        ## Calculating the accuracy \n",
    "        # Model's output is log-softmax, take exponential to get the probabilities\n",
    "        ps = torch.exp(output)\n",
    "        # Class with highest probability is our predicted class, compare with true label\n",
    "        equality = (labels.data == ps.max(1)[1])\n",
    "        # Accuracy is number of correct predictions divided by all predictions, just take the mean\n",
    "        accuracy += equality.type_as(torch.FloatTensor()).mean()\n",
    "\n",
    "    return test_loss, accuracy\n",
    "\n",
    "\n",
    "def train(model, trainloader, testloader, criterion, optimizer, epochs=5, print_every=40):\n",
    "     \n",
    "    model.to(device)\n",
    "\n",
    "    steps = 0\n",
    "    running_loss = 0\n",
    "    for e in range(epochs):\n",
    "        # Model in training mode, dropout is on\n",
    "        model.train()\n",
    "        for images, labels in trainloader:\n",
    "            steps += 1\n",
    "            \n",
    "            # Flatten images into a 784 long vector\n",
    "            images = images.view(images.shape[0], -1)\n",
    "            images.to(device)\n",
    "            labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            output = model.forward(images)\n",
    "            loss = criterion(output, labels)\n",
    "            loss.backward() # computes gradient and backpropagation\n",
    "            optimizer.step() # update of weights and biases happenss\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if steps % print_every == 0:\n",
    "                # Model in inference mode, dropout is off\n",
    "                model.eval()\n",
    "                \n",
    "                # Turn off gradients for validation, will speed up inference\n",
    "                with torch.no_grad():\n",
    "                    test_loss, accuracy = validation(model, testloader, criterion)\n",
    "                \n",
    "                print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
    "                      \"Training Loss: {:.3f}.. \".format(running_loss/print_every),\n",
    "                      \"Test Loss: {:.3f}.. \".format(test_loss/len(testloader)),\n",
    "                      \"Test Accuracy: {:.3f}\".format(accuracy/len(testloader)))\n",
    "                \n",
    "                running_loss = 0\n",
    "                \n",
    "                # Make sure dropout and grads are on for training\n",
    "                model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 1, 96, 96]) torch.Size([128, 8])\n",
      "Train Set 0\n",
      "Image: tensor([[[0.6745, 0.8235, 0.8235,  ..., 0.8235, 0.8235, 0.6745],\n",
      "         [0.8235, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 0.8235],\n",
      "         [0.8235, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 0.8235],\n",
      "         ...,\n",
      "         [0.8235, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 0.8235],\n",
      "         [0.8235, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 0.8235],\n",
      "         [0.6745, 0.8235, 0.8235,  ..., 0.8235, 0.8235, 0.6745]]])\n",
      "Labels: tensor([-10,  -2,   0,  -8,  -9,   9,  -9,   0], dtype=torch.int32)\n",
      "\n",
      "torch.Size([1, 96, 96]) torch.Size([8])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAADnCAYAAADl9EEgAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAafklEQVR4nO3d627byBkG4JdniqSOttdyLCfKAlugKNALaNHb6UX1FnoT+3uBLbotel5kE59kW5FFSZREkRxOfwQzK8my1tbBGsHfAwS7YRxqwuGrIYczQ41zDkKIevRdF4AQshiFkxBFUTgJURSFkxBFUTgJUZS57A//9Kc/ccMw0Gg0UKlU1vogzjlarRba7bbcVigU8PbtW7iuu9a+t6HdbqPVakH0Ztu2jbOzMwRBsOOSPdTtdnF1dQXGGABgU3W2DVEU4fz8HGmaAgA0TcPJyQmOjo52XLLHMcZwdXWFbrcrtxWLRZydncGyrJX2OV1nf/zjH7VFP7M0nIZhzPxa1/x+NrnvTRPlEuE0DAOmaSpZ1vly7cNxzfMcwJdwqnpcBU3TFp6765T7KX93aTgbjQYMw0Cz2UStVlupEALnHJZlwXEcuc3zPDSbTXiet9a+t8HzvJlw2raNZrOJYrG445I91Ol0AGCm5dxEnW1Dv98H5xxJkgD4cuKfnZ3h+Ph4xyV7HGMMuq7D9325rVwu4/379yu3nPN1tsjScFYqFRiGgVqttpFwDodDxHEst/m+j4ODAxQKhbX2vQ1JkiCKIhlOx3FQq9WUDCfnHL1ebyacm6izbTBNE91udyac1WpVybIKjDH0+/2ZIJXLZdRqtZXDOV9ni1CHECGKonASoigKJyGKonASoigKJyGKonASoigKJyGKonASoigKJyGKonASoigKJyGKonASoigKJyGKWjorJU1T5HmOJEnk5NhVcc6RpunMfsTvTXNpMXYiSRI5cwIAdF3fyHHYBnEcxQyHTdXZNohyLToPVJVl2YPjmaYpJpPJyvucr7NFlqbi4uJCTggNw3DlggBfwnl9fY27uzu5rVAoQNM0JVdCuLu7w/X19cx8Tk3TlF0J4fz8XE5g3lSdbcNgMHiwEkKe5xgOhzsu2eOyLMPl5eXMSgji2K6zEsJ0nS2yNJyDwQCGYaDf70PX17sC5pyj3+9jMBjIbVmWodfrrfUNtC2irNPh/KX5d7vS6/UQRdHMfM5N1Nk2RFGEwWAwE85+vz8zCV81jDEMBoOZcxf4ctxXDed8nS2iXu0RQgBQOAlR1tLL2mq1CsMwcHBwgGq1utYHiXVjsiyT2wqFgrLLlOR5jjiOZy5rDw8PlbznNAwDo9Fo5rJ2E3W2DY7jIIqimWVKDg4OcHh4uOOSPY4xhvF4PLOtVCrh8PBw5cva+TpbZGk4T09PYZom3r17h4ODg5UKIeR5LlcsE3zfV3KBL845HMeRnRXAl5Pq3bt3KJVKOy7dQ8ViEXmeyy++TdXZNoj7dtHPoOs6Go0GTk5OoGkLV4jcOXFcpxuRSqWCZrMJ27ZX2ud8nS3yi0tj6rou/7uOx5YX3MS+t8E0Tei6Lk8Ylcs6XU8ANlZn2zBf1n1YGnPRUqPrHuP547CIerVHCAFA4SREWRROQhRF4SREURROQhRF4SREURROQhRF4SREURROQhRF4SREURROQhRF4SREURROQhRF4SREURROQhRF4SREURROQhRF4SREURROQhRF4SREURROQhRF4SREURROQhRF4SREURROQhRF4SREURROQhRF4SREURROQhRF4SREURROQhRF4SREURROQhRF4SREURROQhRF4SREURROQhRF4SREUeayP2y32zAMA57nYTKZrPVBnHPc3t6i3W7LbcPhEL7vw3Xdtfa9Dbe3t7i7u5O/t20bvu8jiqIdlmqxTqeDdrsNxhgAbKzOtmEwGKDdbiNJErnNcRxwzndYquUYY7i9vcX9/b3cNplM4HkeLMtaaZ/zdbbI0nC2Wi0YhgHDMNY+KTnnaLVaM+EsFAowTVPJcN7d3eHm5kaeNLZtwzRNBEGw45I9FIYhWq3WTDg3UWfbEEURrq+vkaYpAEDTNABQ8otEYIzh+voa3W5XbouiCLquw7btlfY5X2eLLA0n53zm1zoW7WdT+96W6bKpXNY8z/emrI+VTcWyCo+du9P/fa75OluE7jkJUdTSltO2bRiGAcdx4DjOWh/EOYdlWTOXAbZtb2Tf22DbNmzbnrmsVbWsjuPAtu2Zy1pVy5okCSzLkpezmqYpW1aBMSbPB0GcD6vec87X2SJLw3l2dgbTNNFsNlGr1VYqhCDCWSgU5Dbf9/H111/PbFOF7/swTVOG03EcNJtNlMvlHZfsoU6nA03TZEVvqs62odfrAYDsENJ1HY1GA/V6fZfFWooxBsMwZvobyuUy3r9/v/I953ydLbI0nEEQwDAMFItFFIvFlQohcM5RKpUwHA7lNt/3USwWlQzncDhEEAQz4SyXy2sfh21IkgRBEMy0nJuos23gnCMIAhlOTdOULavAGEOxWJzptAqCAKVSaeWWc77OFqF7TkIUReEkRFEUTkIUReEkRFEUTkIUReEkRFEUTkIUReEkRFEUTkIUReEkRFEUTkIUReEkRFEUTkIUReEkRFEUTkIUReEkRFEUTkIUReEkRFEUTkIUReEkRFEUTkIUReEkRFEUTkIUReEkRFEUTkIUReEkRFEUTkIUReEkRFEUTkIUReEkRFEUTkIUReEkRFEUTkIUReEkRFEUTkIUReEkRFHmsj/sdrswTROdTgec87U+iHOO+/t7hGEo95UkCTqdDgqFwlr73gZR1jzPAQCO46DT6SBJkh2X7KFWq4WPHz8ijmMMBgPoug5N05BlGTRNg66r8x3c7/cRhiEmkwkAQNd1BEEAy7J2XLLHMcZwf3+Pbrcrt+V5jk6ns3K5xfmVZdmjP7M0nFdXVzAMAwDQ6/VWKoTAOUer1UK73ZbbCoUCNE2D67pr7Xsb2u02Wq2W/CKxLAuapiEIgh2X7KGPHz/iu+++QxiG+PHHHwEAaZpiPB7DNE2Y5tJqflFRFOHi4gJpmgIANE0DYwyj0WjHJXscYwyXl5cIw1Bu6/f7ALByOMMwxMXFhfzyX2RprTHG5H/F/6+Kc/5gP+L36+57G0S5RDgNw1CurJxzcM4RRRFubm4wGAyQpil0XUen08HFxQUqlQqq1So0Tdt1cQE8rHMRTpWO6zzGGPI8X3jurnpVkmXZg33OU+d6hzxblmWYTCa4vLzEt99+i7/85S9wXRfVahV//etf8ec//xl///vf174lIbuxtOU0DGPm17rm97PJfW+aKNd0y2maplJlTZIEaZoijmNEUQTLsuB5HoIgwN3dHQaDAaIokvedKrSe4riKyzlN05Q7rvM0TVt47q5T7qf83aXhbDQaMAwDzWYTtVptpUIInHNYlgXHceQ2z/PQbDbhed5a+94Gz/NmwmnbNprNJorF4o5L9kWe5/jvf/+L29tbOI6D3/3ud/B9H7///e/heR6+/fZb9Pt92LaNcrkM13VRLBZ33jnU7/fBOZcda5qm4ezsDMfHxzst1zLi8tX3fbmtXC7j/fv3K99zdjodue/HLA1npVKBYRio1WobCedwOEQcx3Kb7/s4ODhQsrc2SRJEUSTD6TgOarWaMuHknEPTNPT7fRiGgffv36NYLOJXv/oVHMfB999/D+DLN7TruvB9H7VabefhNE0T3W53JpzVanXt82ubGGPo9/szQSqXy6jVaiuHk3OOXq+3ejiJujjnuL29xT//+U/4vo93796hWCzCtm2YpolvvvkGjuPAsix8//33ePPmDcrl8syVC1EbdQjtKc45bm5u8MMPPyAMQ3z99ddoNpsynL/+9a/xhz/8AbZt47vvvsO///1v+fiC7AcK5x7LsgxJkiDPc9i2DcuyYBgGdF1HpVLByckJNE3Dzc0Nut3u0mdqRD10WbvHkiTBaDQCYwye58HzPNmz+PbtWxSLRfzwww/429/+hlKpRC3nnqGWcw/NP7zWdf3BoxLLsuC6LnRdn3nwT63n/qBw7hnOOUajEQaDAQCgVCotHP4oWlDXdVEqlWDbNqIoQhRFSo/GIT+jcO4ZzrkcGaRpmuwAmh9goGmaDKjjODAMA0mSYDKZUOu5Jyice4YxhpubG3z69Ek+3/zqq68efX55fHyM3/72t6jVavjf//6H//znP0oPMic/o3DumTzPEYYhbm9voWka6vU6SqXSo0PzyuWyfAZ6fX2Ny8tLOV2LqI16a/dMnufodru4vb2FYRio1+uoVCqPtpzlchnNZhNhGOKnn35CFEUUzj1BLeeeYYzh7u4Onz59gq7raDabSy9rj46O8Jvf/AbVahUfPnzAjz/+SJe1e4Jazj3DOcd4PMZwOJSTv5dNphYzVUzTxGg0mpkRQtRGLeeeYYzh8+fPuL6+hq7rOD09XTqg3fd9HB8fw/M8dLtd3N/f02CEPUEt554R063G4zE0TfvFGT1icIKu68iyDFmW0eTrPUEt557J8xz9fh+dTgeapslpbMsmUk8/89Q0DXmeI89zCqniKJx7KEkSxHEsW06x+NgyIqDAl9aX7jvVR+Hcc09dfsRxHBwfH+Pw8BCDwQA3NzcYDocvUEKyKgrnnpm+FBWt4VPDeXh4iHK5jOFwiE6nQ887FUfh3EOr3Cvato1qtYpisYjRaIRut0vhVBz11u4RsU7tKgqFAhqNBuI4RhiGiOMYb9682XAJySZROPeEmI3CGJPLST5nsS7DMOD7PjRNQ5IkYIwtfRUA2T0K555gjGEwGGAwGMB13Wcv1lUoFHB6eoowDPGPf/xDrnVL1EX3nHsiz3OkaSpft2Db9rMWNDYMA57nwXVdpGmK4XCo5EuZyM+o5dwTWZah2+2i1+uhUCjg8PDwWev9FgoFueBXt9tFq9WiAfCKo5ZzTzDGMB6PMR6PYVkWfN9/1oLGpmmiVCrB8zyMx2P0ej3qrVUchXNPMMYQRRGGwyFs20apVHrWPadYT8h1XWRZhvF4TGsJKY7CuSemw+k4jlzd/al0XYfrurBtW778iMKpNgrnnsjzHHEcI45jmKYJz/OUfhs0WR91CO2JNE0RhiH6/T4KhQKCIFDyjeBkc6jl3BN5nsv3cU4vd/lc87NTiLoonHuCMYYwDNHr9eA4zlqvTtR1HaZpylFHNH1MTRTOPSFaziRJYJrmyi0n8PPrGwDQKxoURuHcE1mWodfrod/vw3VdVKvVle45DcNAqVRCtVqVAxtoMIKaqENoTzDGMBqNMB6PYZrmswchCLquo1AowPd9MMYwHA7lJe5T5oWSl0Ph3BNZlmE0GmE0GsGyLARBsHI4gyBAsVhEnufo9XrPel5KXg6Fc0+Ihb3iOJYjhFYJla7rKJVKqNVqYIzh/v4evu9Ty6mgpeGMogiGYaDf7y9duPgpOOfo9/sz05TECafiOqqirOJxQ5Ik6PV6O3v8IIIZxzGGwyGGwyEmk4msn+lX+y2rM/FznHNEUYTPnz/D9330+/1nzQ9d598RRZGcEaNpGgaDATzP2/pnr0pM15s+d8UxXnUgyHydLbI0cefn5zAMA5xzdLvdlQohcM5xfX2Ndrstt4kKUfFhervdxvX1tQyjqIQgCHZSnvPzc7TbbaRpipubG1xcXMiWLgxDXFxczITzsTobDAZyXO3l5aWcnWLb9ouEM4oifPr0SX4hi/V0VZ5bmuc5Li4uZo5nqVQC53zlRmu+zhZZuuc0TWe68NfBOZfzEQWx35c4KZ5LPPAX4RQrCOxqDqSoiyzLHhzHyWSCNE1lRS+rszRN5SLTSZIgz3OMRiPZCm+bOK6i/Ls+rk/BGFt47q7zrtP5OltEvVSQrdJ1HcViEZVKBVmW4f7+nh6lKGppyzm99OImOgvm97PJfW/DfFmfukbsNsuy6JiJci37GWF66hjnHJPJRK5L9BL/tvnjuOvj+hSLjue65Z6vs0WWhvPk5ASmaeLs7AzVanWlQgiccxiGMXON7nke3r59q+Q95/xcScdx0Gg0UCwWd1KeXq+HUqmEOI5xcnKCt2/fyj8LggCMMXmJtKzOxATrXq+HMAxxd3eHIAhwdna2dqffUwwGA2RZJi9jxcuYjo+Pt/7ZqxILoU2fp+VyGWdnZyt3CM3X2SJLa+Po6AiGYeD4+Bi1Wm2lQgjinnO6MOINWKuOEd22yWQi7zkdx0G9Xt9ZOG9ublAoFKBpGg4ODlCv1+WfWZaF0Wg00yH0WJ2Je85+v49//etfAH6uh5eYguZ53oPe2uPj45l/j2oYY5hMJjN9I+VyGfV6feVjNl9ni9A95yujaRocx0GhUJCdQSp3xrxmNAjhldF1Xa5fK8brxnG862KRBajl3APidX2GYaz9uEMsSC3uL2nKmLqo5VQcYwxJkiDLMpimCcuy1gqopmmwbVuGMo7jmee5RB3Uciouz3M551L0dq/72EHXdRlw8SJdoh5qORWXpimiKMJkMpEvyt3EIw+VnyuSLyicihPd+FmWwbKsmVUMNoFaTXVROBUn5nGmaSpH9bzEGFiyexROxYlwJkkCx3GgaRqF85WgcCpu+rJWTOtScRYP2TwKp+LSNMVgMJAdQqZpUsv5StBXsOLEeznFc85NT4oW+6LnnOqhllNxSZJgMBggSRL58ttNzR6ZX7+WMUatskKo5VScaDkZY3LY3SafUYow0uLS6qFwKk68Il7ccwZBsLFBCJ7noVQqQdd1DAYDjEYjCqhCKJyKY4zJd2lalgXHcTZ2z2lZFlzXha7riON4Zv4q2T0Kp+LEW6jTNIXjOGu9I2WapmmwLAue50HTNEwmEyRJQuFUCIVTcWLZyCRJ5GsUNtVpY9s2PM+DrusYjUaI45jCqRAKp+LEsozTU8Y2dVk73cEkPoPCqQ56lKI4MStFXIIWi8WNXdbato0gCKBpGobDIRzHoQ4hhVDLqTjxKGV6JYRNtZzT80OzLFu62BR5eRROxYnhe3Ecy5ZzU4MQHMdBEAQz95zUcqqDwqk4sUyJGIRgWdbGBiHoui73l2WZXJ+VqIHCqThxz5kkCVzX3VhvrVgic7q3dp13f5DNo3AqTgxCEPM5Xdfd2KMUMahh+jknUQeFU3Gcc/mIQyzMtanLWtM04bquDCetwqcWCqfixGv/8jzf6MB38ShFXNaOx2MahKAYCucrpuv6zOLS4o3XRA0UzlfMdV35KCWKIozHY+oQUgiF85US75cUl8mMMXqUohgK5x7Y1stlpx+lDIdDuudUDIXzFTMMA7ZtAwANfFcQhfOVEm8bEy0nTbZWD4XzFRMv0dU0DePxmCZbK4bC+YoZhiHH1qZpSoMQFEPh3APi5bmbJAYhiLdc00oI6lk69yiOYxiGgdFohEKhsNYHcc7lKBRBjExR0fyImTzPMRqNNjZd66niOEaWZUjTFHEcLzxeoqxiPuZT6oxzLsfTpmmKyWSCOI4xGo22WieirGIcr7ikVvU8AL6Mb54/dx3HwXg8Xvnx03ydLbL0TDs/P4dhGOCco9vtrlQIgXOOVquFu7s7uU2cPK7rrrXvbWi327i+vpbhFL2aQRC8aDlarRaiKEK328VPP/208CTudru4vLycCedT6kysVXtzc4Pb21uUy2V8+PABURRt5d8CAFEU4dOnT0jTFADkdLVtfua6GGO4vLycOZ7FYhGc85W/rOfrbJGlex6Px/JbWJycq+KcL/xWVnWtVFFWEU7GGEaj0Yu/RGi65RyPxxiNRo+Wdb7l/KU6E5fLogWdTCYYjUYLP2NTRFmnw7ntz1yXaDmnz13TNDEcDmFZ1kr7nK+zRWgNIUWJ4IjZKNsYhKBpmtzvNu5ryXoonIrinMvATIdokVWDK/7eS7+Cnl55/7RjsDScX331FQzDQL1eR61WW6swohWYLpTneTg5OVm7s2kbDMOYmaVh2zZOTk5QLBa3/tniUjNNU9RqNVSrVRwcHODk5AT1ev3Bz7uuK5cyEWV/Tp3VajV4ngff91Gv1xd+xqb0er2Zid2apqFer+P4+Hhrn7kuMe54+hK2VCrhzZs3K1/WztfZIkvDWa/XYZomGo0GDg4OViqEMN0KCL7vo9FowPO8tfa9aeJGfzqcjuOg0WigVCq9yOdHUYQ4jnF4eIiDgwMcHR3h9PQUb968efDzhUJhpqKfW2dHR0fwPA+VSgUnJydoNBob/fdMK5VKsncY+BLO09NTnJycbO0z1yXWV5q+h69UKmg0Giv3xczX2SJLw7nJyx5xaTa9r1+6XNuVXZdV0zS5AgLw8+LPj3VGPVbe53yesO0Or/lzStVzYNqi4yu2b3qf02gQgoI450iSRPbmeZ4n1/ohrweFU1FieRIAcjFp8rpQOBWVJIl8BlwoFOSr+raBHqGoicKpqOmVCSzL2nrLKXrTiToonArinMsRQXmew3Xdjb40l+wHqm1FZVkmJz/btr3y8zSyvyicisqyDEmSIM9zWJa10XekLLIPjzReGwqnouYva23b3lp4KJRqonAqSjxKEaOVNrXSO9kfFE4FiUEIw+EQeZ7TIIRXisKpKMbYzED2Tb7AiOwHCqeiFg1C2GY4xWOaPM+VnPz+GlE4FZXnOdI0lb2127znnO6pFYMRaEDC7lE4FSQGIYgFxkRv7baI189bloU0TWn9WkVQOBU1HU7btmHb9tZGCBmGAcdxYBiGXK+ILm13b+l8zm63C9M00el01v4m5Zzj/v4eYRjKfSVJgk6no+RKCKKs4iR1HAedTudFXs2eZRm63S56vR5c18VwOEShUMD9/f3C1fdEWafnfz6nzgaDAbIsw3g8xufPn6HrOoIg2MqopH6/jzAM5WTrbX7WpjDGcH9/P7P6Xp7n6HQ6K5d7vs4WWRrOq6srOeC61+utVAhBLI3ZbrflNvEqAFWXxmy1WvIEFyN0XmJpzCzLcHFxgaurK2iahkqlAs45Pn78uPBYhWGIi4sL+UXy3Dq7u7tDHMfo9Xr48OEDwjBEqVTaSr1EUYSLi4uZ1ffEyoaqEktjhmEot/X7fQBYOZzzdbbI0nCKrvzpbv1Vcc4f7Ef8ft19b4MolwinWFPoJcoq3jItek6nfy36fPFa+kXH9immT5Dpz97Gv3W+zkU4VTwHhEXHQ5R51VuNRXU2T6Mbf0LURB1ChCiKwkmIoiichCiKwkmIoiichCiKwkmIov4PnVWxR/OWcNoAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 1, 96, 96]) torch.Size([128, 8])\n",
      "Test Set 0\n",
      "Image: tensor([[[0.6745, 0.8235, 0.8235,  ..., 0.8235, 0.8235, 0.6745],\n",
      "         [0.8235, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 0.8235],\n",
      "         [0.8235, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 0.8235],\n",
      "         ...,\n",
      "         [0.8235, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 0.8235],\n",
      "         [0.8235, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 0.8235],\n",
      "         [0.6745, 0.8235, 0.8235,  ..., 0.8235, 0.8235, 0.6745]]])\n",
      "Labels: tensor([  8,  -8,  -9, -10,  -5,  -1,   6,   7], dtype=torch.int32)\n",
      "\n",
      "torch.Size([1, 96, 96]) torch.Size([8])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#one way to see batch size\n",
    "train_batch = next(iter(train_set))\n",
    "img, lbls = train_batch\n",
    "print(img.shape, lbls.shape)\n",
    "\n",
    "#display the first image in train_set\n",
    "#note: image changes each time run because shuffle is set to true\n",
    "for images, labels in train_set:\n",
    "    image = images[0]\n",
    "    label = labels[0]\n",
    "    print(f\"Train Set 0\\nImage: {image}\\nLabels: {label}\\n\")\n",
    "    print(image.shape, label.shape)\n",
    "    figure = plt.figure(figsize=(4,4))\n",
    "    figure.add_subplot()\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(image.permute(1,2,0), cmap=\"gray\")\n",
    "    plt.show()\n",
    "    \n",
    "    break\n",
    "    \n",
    "#test batch size\n",
    "test_batch = next(iter(test_set))\n",
    "img, lbls = test_batch\n",
    "print(img.shape, lbls.shape)\n",
    "\n",
    "#display the first image in test_set\n",
    "for images, labels in test_set:\n",
    "    image = images[0]\n",
    "    label = labels[0]\n",
    "    print(f\"Test Set 0\\nImage: {image}\\nLabels: {label}\\n\")\n",
    "    print(image.shape, label.shape)\n",
    "    figure = plt.figure(figsize=(4,4))\n",
    "    figure.add_subplot()\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(image.permute(1,2,0), cmap=\"gray\")\n",
    "    plt.show()\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-5.1028e-04, -1.6850e-03,  5.4790e-02,  ...,  1.2417e-02,\n",
      "         -8.6938e-04,  2.5380e-02],\n",
      "        [-8.4791e-04, -1.4556e-03,  5.6844e-02,  ..., -7.6837e-05,\n",
      "         -8.5092e-04, -1.0607e-04],\n",
      "        [-1.0314e-03, -1.4722e-03,  1.1500e-01,  ..., -1.8273e-05,\n",
      "         -1.2339e-03,  4.2265e-02],\n",
      "        ...,\n",
      "        [-9.2175e-04, -1.7447e-03,  6.4806e-02,  ..., -2.2199e-05,\n",
      "         -1.2304e-03,  2.1569e-02],\n",
      "        [-1.0581e-03, -1.3160e-03,  7.7483e-02,  ..., -4.4028e-05,\n",
      "         -7.6275e-04,  6.7537e-02],\n",
      "        [-1.0727e-03, -1.6991e-03,  1.0903e-01,  ..., -1.3601e-04,\n",
      "         -1.1014e-03,  5.2325e-02]], grad_fn=<LeakyReluBackward0>)\n",
      "tensor([[  8,  -8,  -9,  ...,  -1,   6,   7],\n",
      "        [  9,   4,   1,  ...,  -1,  -8,  -4],\n",
      "        [  1,  -5,   7,  ...,  -9,  10,   9],\n",
      "        ...,\n",
      "        [ -9,  -8,   4,  ...,   4,   9,  -7],\n",
      "        [  6,   9,  -5,  ...,   7,   0,  -3],\n",
      "        [  5,   7,  -2,  ...,   6, -10,   6]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "model = Network()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "output = model.forward(img)\n",
    "print(output)\n",
    "print(lbls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 8])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([-0.0005, -0.0017,  0.0548,  0.1480,  0.0273,  0.0124, -0.0009,  0.0254],\n",
       "       grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(output.shape)\n",
    "output[0,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO A: Train model on the data set\n",
    "    # TODO 1: dataloader be able load (image: tensor, label:string) example of the label tensor([10,-4,-5,3,-2,-3,-2,2]) \n",
    "    # DONE 2: fix dimensionality so the net can feedforward a batch of images correctly, move the training to gpu when available\n",
    "    # DONE 3: make trainloader and testloader dataloaders\n",
    "# TODO B: Improve model\n",
    "    # DONE 4: update checkpoint structure to save model and load model\n",
    "# Brandon: 1, 3 \n",
    "# Josias: 2, 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected 4-dimensional input for 4-dimensional weight [8, 1, 3, 3], but got 2-dimensional input of size [128, 9216] instead",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-3676c1af5657>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestloader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-3874294f3d15>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, trainloader, testloader, criterion, optimizer, epochs, print_every)\u001b[0m\n\u001b[0;32m     42\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# computes gradient and backpropagation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-94158e5bedf5>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     67\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m         \u001b[1;31m#feedword pass through our network\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcnn_layers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#flatten the input tensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc_layers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    137\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 139\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    140\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 443\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    445\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    437\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    438\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m--> 439\u001b[1;33m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[0;32m    440\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected 4-dimensional input for 4-dimensional weight [8, 1, 3, 3], but got 2-dimensional input of size [128, 9216] instead"
     ]
    }
   ],
   "source": [
    "train(model=model, trainloader=train_set, testloader=test_set, criterion=model.criterion, optimizer=optimizer, epochs=30)\n",
    "output = model.forward(images)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('C:\\\\Users\\\\the_3\\\\Desktop\\\\poly-curve-detector\\\\Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network(\n",
       "  (cnn_layers): Sequential(\n",
       "    (0): Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): LeakyReLU(negative_slope=0.01)\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (4): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): LeakyReLU(negative_slope=0.01)\n",
       "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (8): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (9): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): LeakyReLU(negative_slope=0.01)\n",
       "    (11): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (12): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (13): LeakyReLU(negative_slope=0.01)\n",
       "    (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (fc_layers): Sequential(\n",
       "    (0): Linear(in_features=2592, out_features=1024, bias=True)\n",
       "    (1): LeakyReLU(negative_slope=0.01)\n",
       "    (2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "    (3): LeakyReLU(negative_slope=0.01)\n",
       "    (4): Linear(in_features=256, out_features=64, bias=True)\n",
       "    (5): LeakyReLU(negative_slope=0.01)\n",
       "    (6): Linear(in_features=64, out_features=8, bias=True)\n",
       "    (7): LeakyReLU(negative_slope=0.01)\n",
       "  )\n",
       "  (criterion): MSELoss()\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model = Network()\n",
    "Network.load_checkpoint(new_model, \"C:\\\\Users\\\\the_3\\\\Desktop\\\\poly-curve-detector\\\\Model\\\\model_checkpoint.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3a1829a56db40c3ca63cc5d173ccaf89ee3791672440d362287656aaeb413643"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit (conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
