{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to develop the model for the project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List of ideas to improve the network\n",
    "\n",
    "1. ## Add a grid with finer blocks to the input as transform\n",
    "2. ## Revise the architecture\n",
    "3. ## Add dropout to the network\n",
    "4. ## Find a way to measure accuracy of multiple regression network with some tolerance factor\n",
    "5. ## List all hyperparameters of the network (affecting net performmance)\n",
    "    >### Learning rate\n",
    "    >### number of epochs\n",
    "    >### batch size\n",
    "    >### Dropout rate\n",
    "    >### Loss function\n",
    "    >### Optimmizer\n",
    "    >### Network architecture\n",
    "    - CNN layers NofElement, arrangment, and parameters\n",
    "    \n",
    "    - FC layers NofElement, arrangment, and parameters\n",
    "6. ## Augment the dataset with transforms to more than 10^5 sample, insure dataset is statistically sound (no extreme bias, etc...)\n",
    "7. ## Use a genetic algorithm to search best set of hyper parameters \n",
    "    >### Encode all hyperparameters listed into a string represention \"chromosome/gene\"\n",
    "    >### Use the new measure of accuracy to gauge the \"fitness\" of the net\n",
    "    >### Design a function to allow for \"crossover\" between nets' genes \n",
    "    >### Design a \"mutation\" function that allows for variations in hyperparameters.\n",
    "    >### Put the environment together to run the algorithm\n",
    "7. ## Investigate if autoencoders might performe better   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All our imports\n",
    "import torch \n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from random import randint\n",
    "import PIL\n",
    "import os\n",
    "\n",
    "#for all the plots to be inline\n",
    "%matplotlib inline \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, csv_path, images_folder, transform):\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.images_folder = images_folder\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        filename = self.df.values[index][0]\n",
    "        labels = np.array([1, 1, 1, 1, 1, 1, 1, 1])\n",
    "        for x in range(0, 8):\n",
    "            labels[x] = self.df.values[index][x+1]\n",
    "        image = PIL.Image.open(os.path.join(self.images_folder, filename))\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        return image, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Select device to train on\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "# device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Data Set manipulation'''\n",
    "transform = transforms.Compose([transforms.Grayscale(num_output_channels=1),\n",
    "                                transforms.ToTensor()])\n",
    "\n",
    "root = \"C:\\\\Users\\\\the_3\\\\Desktop\"\n",
    "trainDataset = CustomDataset(root + \"\\\\poly-curve-detector\\\\data\\\\plotData\\\\labels\\\\trainPlots.csv\",\n",
    "                               root + \"\\\\poly-curve-detector\\\\data\\\\plotData\\\\trainPlots\", transform)\n",
    "\n",
    "testDataset = CustomDataset(root + \"\\\\poly-curve-detector\\\\data\\\\plotData\\\\labels\\\\testPlots.csv\",\n",
    "                               root + \"\\\\poly-curve-detector\\\\data\\\\plotData\\\\testPlots\", transform)\n",
    "\n",
    "#print first label in each dataset\n",
    "#labels in order [a1,a2,a3,a4,a5,a6,a7,a8]\n",
    "# image, labels = trainDataset[0]\n",
    "# print(labels[0:9])\n",
    "# image, labels = testDataset[0]\n",
    "# print(labels[0:9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = torch.utils.data.DataLoader(trainDataset, shuffle=True, batch_size=128)\n",
    "test_set = torch.utils.data.DataLoader(testDataset, shuffle=False, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 1, 96, 96]) torch.Size([128, 8])\n",
      "Train Set 0\n",
      "Image: tensor([[[0.6745, 0.8235, 0.8235,  ..., 0.8235, 0.8235, 0.6745],\n",
      "         [0.8235, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 0.8235],\n",
      "         [0.8235, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 0.8235],\n",
      "         ...,\n",
      "         [0.8235, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 0.8235],\n",
      "         [0.8235, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 0.8235],\n",
      "         [0.6745, 0.8235, 0.8235,  ..., 0.8235, 0.8235, 0.6745]]])\n",
      "Labels: tensor([ 9,  1,  3,  6, -1, -6,  3,  4], dtype=torch.int32)\n",
      "\n",
      "torch.Size([1, 96, 96]) torch.Size([8])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 1, 96, 96]) torch.Size([128, 8])\n",
      "Test Set 0\n",
      "Image: tensor([[[0.6745, 0.8235, 0.8235,  ..., 0.8235, 0.8235, 0.6745],\n",
      "         [0.8235, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 0.8235],\n",
      "         [0.8235, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 0.8235],\n",
      "         ...,\n",
      "         [0.8235, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 0.8235],\n",
      "         [0.8235, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 0.8235],\n",
      "         [0.6745, 0.8235, 0.8235,  ..., 0.8235, 0.8235, 0.6745]]])\n",
      "Labels: tensor([  5, -10,  -4,   3,  -7,   7,  -9,  -7], dtype=torch.int32)\n",
      "\n",
      "torch.Size([1, 96, 96]) torch.Size([8])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#one way to see batch size\n",
    "train_batch = next(iter(train_set))\n",
    "img, lbls = train_batch\n",
    "print(img.shape, lbls.shape)\n",
    "\n",
    "#display the first image in train_set\n",
    "#note: image changes each time run because shuffle is set to true\n",
    "for images, labels in train_set:\n",
    "    image, label = images[0], labels[0]\n",
    "    print(f\"Train Set 0\\nImage: {image}\\nLabels: {label}\\n\")\n",
    "    print(image.shape, label.shape)\n",
    "    figure = plt.figure(figsize=(4,4))\n",
    "    figure.add_subplot()\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(image.permute(1,2,0), cmap=\"gray\")\n",
    "    plt.show()\n",
    "    break\n",
    "    \n",
    "#test batch size\n",
    "test_batch = next(iter(test_set))\n",
    "img, lbls = test_batch\n",
    "print(img.shape, lbls.shape)\n",
    "\n",
    "#display the first image in test_set\n",
    "for images, labels in test_set:\n",
    "    image, label = images[0], labels[0]\n",
    "    print(f\"Test Set 0\\nImage: {image}\\nLabels: {label}\\n\")\n",
    "    print(image.shape, label.shape)\n",
    "    figure = plt.figure(figsize=(4,4))\n",
    "    figure.add_subplot()\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(image.permute(1,2,0), cmap=\"gray\")\n",
    "    plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "        # more analysis required to determine the specifics of the architecture\n",
    "       \n",
    "        self.n_output = 8\n",
    "        self.n_channel = 1\n",
    "    \n",
    "        self.cnn_layers = nn.Sequential(\n",
    "                nn.Conv2d(1, 8, 3), # convolution2dLayer(3,8,'Padding','same')\n",
    "                nn.BatchNorm2d(8),   # batchNormalizationLayer\n",
    "                nn.LeakyReLU(), # reluLayer\n",
    "                nn.MaxPool2d(2, 2), # averagePooling2dLayer(2,'Stride',2)\n",
    "                nn.Conv2d(8, 16, 3), # convolution2dLayer(3,16,'Padding','same')\n",
    "                nn.BatchNorm2d(16), # batchNormalizationLayer\n",
    "                nn.LeakyReLU(), # reluLayer\n",
    "                nn.MaxPool2d(2, 2), # averagePooling2dLayer(2,'Stride',2)\n",
    "                nn.Conv2d(16, 32, 3), # convolution2dLayer(3,32,'Padding','same')\n",
    "                nn.BatchNorm2d(32), # batchNormalizationLayer\n",
    "                nn.LeakyReLU(), # reluLayer\n",
    "                nn.Conv2d(32, 32, 3), # convolution2dLayer(3,32,'Padding','same')\n",
    "                nn.Conv2d(32, 32, 3), # convolution2dLayer(3,32,'Padding','same')\n",
    "                nn.Conv2d(32, 32, 3), # convolution2dLayer(3,32,'Padding','same')\n",
    "                nn.BatchNorm2d(32), # batchNormalizationLayer\n",
    "                nn.LeakyReLU(), # reluLayer\n",
    "                nn.MaxPool2d(2, 2) # Max pooling layer\n",
    "        )\n",
    "\n",
    "        self.n_input = 1568 # the output of maxpool 96*96 \n",
    "        #TODO:actual value might be determined from the computed output of the cnn layers\n",
    "        # https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n",
    "\n",
    "        self.fc_layers = nn.Sequential(\n",
    "#         fullyConnectedLayer(1)\n",
    "                nn.Linear(self.n_input,  4096),\n",
    "                nn.LeakyReLU(),\n",
    "                nn.Linear(4096, 4096),\n",
    "                nn.LeakyReLU(),\n",
    "                nn.Linear(4096, 128),\n",
    "                nn.LeakyReLU(),\n",
    "                nn.Linear(128, 64),\n",
    "                nn.LeakyReLU(),\n",
    "                nn.Linear(64, self.n_output),\n",
    "                nn.LeakyReLU()\n",
    "        )\n",
    "\n",
    "\n",
    "#         regressionLayer\n",
    "        self.criterion = nn.MSELoss()       \n",
    "#         dropoutLayer(0.2)\n",
    "        self.dropout =  nn.Dropout(p=0.2)\n",
    "        \n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #feedword pass through our network\n",
    "        x = self.cnn_layers(x)\n",
    "        x = x.view(x.shape[0], -1) #flatten the input tensor\n",
    "        x = self.fc_layers(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def load_checkpoint(new_model, filepath):\n",
    "        checkpoint = torch.load(filepath)\n",
    "        # model = Network(\n",
    "        #         checkpoint['input_size'], \n",
    "        #         checkpoint['output_size'],\n",
    "        #         # checkpoint['cnn_layers'],\n",
    "        #         # checkpoint['fc_layers']\n",
    "        # )\n",
    "        new_model.load_state_dict(checkpoint['state_dict'])\n",
    "        return new_model\n",
    "    \n",
    "    def save(self, dirpath):\n",
    "        self.checkpoint = {\n",
    "        #     'input_size': self.n_input, \n",
    "        #     'output_size': self.n_output,\n",
    "        #     'cnn_layers': [each. for each in model.cnn_layers],\n",
    "        #     'fc_layers': [each.out_features for each in model.fc_layers],\n",
    "            'state_dict': model.state_dict()\n",
    "        }\n",
    "        torch.save(self.checkpoint, f'{dirpath}\\\\model_checkpoint.pth')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "def validation(model, testloader, criterion):\n",
    "    accuracy = 0\n",
    "    test_loss = 0\n",
    "    for images, labels in testloader:\n",
    "        # if(labels.shape != torch.Size([128, 1, 96, 96])): continue\n",
    "        # images = images.view(images.shape[0], -1)\n",
    "        labels = labels.float()\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        output = model.forward(images)\n",
    "\n",
    "        # print(f'output={output.shape}')\n",
    "        # print(f'label={labels.shape}')\n",
    "        test_loss += criterion(output, labels)\n",
    "\n",
    "        # Calculate accuracy\n",
    "        # ps = torch.exp(output)\n",
    "        # top_p, top_class = ps.topk(1, dim=1)\n",
    "        # equals = top_class == labels.view(*top_class.shape)\n",
    "        # accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "        # accuracy += 1 - test_loss\n",
    "        ## Calculating the accuracy \n",
    "        # Model's output is log-softmax, take exponential to get the probabilities\n",
    "        # ps = torch.exp(output)\n",
    "        # Class with highest probability is our predicted class, compare with true label\n",
    "        # equality = (labels.data == ps.max(1)[1])\n",
    "        # Accuracy is number of correct predictions divided by all predictions, just take the mean\n",
    "        # accuracy += equality.type_as(torch.FloatTensor()).mean()\n",
    "        # accuracy = r2_score(labels, output)   # r2_score is the scikit learn r2 score function.\n",
    "        # print(\"accuracy = \", accuracy)   # here i get wierd values and it doesn't get better over time, in contrast the loss decreased over time\n",
    "\n",
    "    return test_loss, accuracy\n",
    "\n",
    "\n",
    "def train(model, trainloader, testloader, criterion, optimizer, epochs=5, print_every=40):\n",
    "\n",
    "    steps = 0\n",
    "    running_loss = 0\n",
    "    test_losses = []\n",
    "    train_losses = []\n",
    "    for e in range(epochs):\n",
    "        # Model in training mode, dropout is on\n",
    "        model.train()\n",
    "        for images, labels in trainloader:\n",
    "            steps += 1\n",
    "            # if(labels.shape != torch.Size([128, 1, 96, 96])): continue\n",
    "            # Flatten images into a 784 long vector\n",
    "            # images = images.view(images.shape[0], -1)\n",
    "            labels = labels.float()\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            output = model.forward(images)\n",
    "            # print(f'output={output.shape}')\n",
    "            # print(f'label={labels.shape}')\n",
    "            loss = criterion(output, labels)\n",
    "            loss.backward() # computes gradient and backpropagation\n",
    "            optimizer.step() # update of weights and biases happenss\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if steps % print_every == 0:\n",
    "                # Model in inference mode, dropout is off\n",
    "                model.eval()\n",
    "                \n",
    "                # Turn off gradients for validation, will speed up inference\n",
    "                with torch.no_grad():\n",
    "                    test_loss, accuracy = validation(model, testloader, criterion)\n",
    "                \n",
    "                train_losses.append(running_loss/len(trainloader))\n",
    "                test_losses.append(test_loss/len(testloader))\n",
    "\n",
    "                print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
    "                      \"Training Loss: {:.3f}.. \".format(running_loss/print_every),\n",
    "                      \"Test Loss: {:.3f}.. \".format(test_loss/len(testloader)),\n",
    "                    #   \"Test Accuracy: {:.3f}\".format(accuracy/len(testloader))\n",
    "                      )\n",
    "                \n",
    "                running_loss = 0\n",
    "                \n",
    "                # Make sure dropout and grads are on for training\n",
    "                model.train()\n",
    "    plt.xlim([0, 100])          \n",
    "    plt.plot(train_losses, label='Training loss')\n",
    "    plt.plot(test_losses, label='Validation loss')\n",
    "    plt.legend(frameon=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/1000..  Training Loss: 76.209..  Test Loss: 112.101.. \n",
      "Epoch: 3/1000..  Training Loss: 51.680..  Test Loss: 39.169.. \n",
      "Epoch: 4/1000..  Training Loss: 68.190..  Test Loss: 55.742.. \n",
      "Epoch: 5/1000..  Training Loss: 94.149..  Test Loss: 116.945.. \n",
      "Epoch: 6/1000..  Training Loss: 61.166..  Test Loss: 62.336.. \n",
      "Epoch: 7/1000..  Training Loss: 51.218..  Test Loss: 46.838.. \n",
      "Epoch: 8/1000..  Training Loss: 38.165..  Test Loss: 49.594.. \n",
      "Epoch: 9/1000..  Training Loss: 39.893..  Test Loss: 40.822.. \n",
      "Epoch: 10/1000..  Training Loss: 37.310..  Test Loss: 41.760.. \n",
      "Epoch: 12/1000..  Training Loss: 36.431..  Test Loss: 40.960.. \n",
      "Epoch: 13/1000..  Training Loss: 36.795..  Test Loss: 37.514.. \n",
      "Epoch: 14/1000..  Training Loss: 37.586..  Test Loss: 36.944.. \n",
      "Epoch: 15/1000..  Training Loss: 35.780..  Test Loss: 35.867.. \n",
      "Epoch: 16/1000..  Training Loss: 35.513..  Test Loss: 35.489.. \n",
      "Epoch: 17/1000..  Training Loss: 36.013..  Test Loss: 75.289.. \n",
      "Epoch: 18/1000..  Training Loss: 36.972..  Test Loss: 49.173.. \n",
      "Epoch: 19/1000..  Training Loss: 35.410..  Test Loss: 36.299.. \n",
      "Epoch: 20/1000..  Training Loss: 34.710..  Test Loss: 37.921.. \n",
      "Epoch: 22/1000..  Training Loss: 33.377..  Test Loss: 39.046.. \n",
      "Epoch: 23/1000..  Training Loss: 33.191..  Test Loss: 33.074.. \n",
      "Epoch: 24/1000..  Training Loss: 32.275..  Test Loss: 34.343.. \n",
      "Epoch: 25/1000..  Training Loss: 34.488..  Test Loss: 48.987.. \n",
      "Epoch: 26/1000..  Training Loss: 36.961..  Test Loss: 38.786.. \n",
      "Epoch: 27/1000..  Training Loss: 37.795..  Test Loss: 38.377.. \n",
      "Epoch: 28/1000..  Training Loss: 36.451..  Test Loss: 37.185.. \n",
      "Epoch: 29/1000..  Training Loss: 36.624..  Test Loss: 36.936.. \n",
      "Epoch: 30/1000..  Training Loss: 36.631..  Test Loss: 37.077.. \n",
      "Epoch: 32/1000..  Training Loss: 36.562..  Test Loss: 37.025.. \n",
      "Epoch: 33/1000..  Training Loss: 36.565..  Test Loss: 36.703.. \n",
      "Epoch: 34/1000..  Training Loss: 36.715..  Test Loss: 37.214.. \n",
      "Epoch: 35/1000..  Training Loss: 36.430..  Test Loss: 36.983.. \n",
      "Epoch: 36/1000..  Training Loss: 36.639..  Test Loss: 36.743.. \n",
      "Epoch: 37/1000..  Training Loss: 36.616..  Test Loss: 36.563.. \n",
      "Epoch: 38/1000..  Training Loss: 36.089..  Test Loss: 36.643.. \n",
      "Epoch: 39/1000..  Training Loss: 36.303..  Test Loss: 36.643.. \n",
      "Epoch: 40/1000..  Training Loss: 36.160..  Test Loss: 36.395.. \n",
      "Epoch: 42/1000..  Training Loss: 35.821..  Test Loss: 36.544.. \n",
      "Epoch: 43/1000..  Training Loss: 35.390..  Test Loss: 36.917.. \n",
      "Epoch: 44/1000..  Training Loss: 35.766..  Test Loss: 35.716.. \n",
      "Epoch: 45/1000..  Training Loss: 35.183..  Test Loss: 37.958.. \n",
      "Epoch: 46/1000..  Training Loss: 35.451..  Test Loss: 36.882.. \n",
      "Epoch: 47/1000..  Training Loss: 35.376..  Test Loss: 35.607.. \n",
      "Epoch: 48/1000..  Training Loss: 33.877..  Test Loss: 36.483.. \n",
      "Epoch: 49/1000..  Training Loss: 33.625..  Test Loss: 34.346.. \n",
      "Epoch: 50/1000..  Training Loss: 33.509..  Test Loss: 34.262.. \n",
      "Epoch: 52/1000..  Training Loss: 33.279..  Test Loss: 33.993.. \n",
      "Epoch: 53/1000..  Training Loss: 32.841..  Test Loss: 33.898.. \n",
      "Epoch: 54/1000..  Training Loss: 32.803..  Test Loss: 33.814.. \n",
      "Epoch: 55/1000..  Training Loss: 32.618..  Test Loss: 33.516.. \n",
      "Epoch: 56/1000..  Training Loss: 32.033..  Test Loss: 32.940.. \n",
      "Epoch: 57/1000..  Training Loss: 31.965..  Test Loss: 32.486.. \n",
      "Epoch: 58/1000..  Training Loss: 31.554..  Test Loss: 32.041.. \n",
      "Epoch: 59/1000..  Training Loss: 31.252..  Test Loss: 33.442.. \n",
      "Epoch: 60/1000..  Training Loss: 30.922..  Test Loss: 31.875.. \n",
      "Epoch: 62/1000..  Training Loss: 30.660..  Test Loss: 31.095.. \n",
      "Epoch: 63/1000..  Training Loss: 30.384..  Test Loss: 33.678.. \n",
      "Epoch: 64/1000..  Training Loss: 30.485..  Test Loss: 31.753.. \n",
      "Epoch: 65/1000..  Training Loss: 29.608..  Test Loss: 30.646.. \n",
      "Epoch: 66/1000..  Training Loss: 29.260..  Test Loss: 30.445.. \n",
      "Epoch: 67/1000..  Training Loss: 28.422..  Test Loss: 30.611.. \n",
      "Epoch: 68/1000..  Training Loss: 29.197..  Test Loss: 29.968.. \n",
      "Epoch: 69/1000..  Training Loss: 29.390..  Test Loss: 32.484.. \n",
      "Epoch: 70/1000..  Training Loss: 29.238..  Test Loss: 30.403.. \n",
      "Epoch: 72/1000..  Training Loss: 28.952..  Test Loss: 30.099.. \n",
      "Epoch: 73/1000..  Training Loss: 28.375..  Test Loss: 29.512.. \n",
      "Epoch: 74/1000..  Training Loss: 28.403..  Test Loss: 29.655.. \n",
      "Epoch: 75/1000..  Training Loss: 27.967..  Test Loss: 29.195.. \n",
      "Epoch: 76/1000..  Training Loss: 28.175..  Test Loss: 29.179.. \n",
      "Epoch: 77/1000..  Training Loss: 27.608..  Test Loss: 29.744.. \n",
      "Epoch: 78/1000..  Training Loss: 31.037..  Test Loss: 42.441.. \n",
      "Epoch: 79/1000..  Training Loss: 32.533..  Test Loss: 36.027.. \n",
      "Epoch: 80/1000..  Training Loss: 30.971..  Test Loss: 30.964.. \n",
      "Epoch: 82/1000..  Training Loss: 29.947..  Test Loss: 30.451.. \n",
      "Epoch: 83/1000..  Training Loss: 29.051..  Test Loss: 29.418.. \n",
      "Epoch: 84/1000..  Training Loss: 28.566..  Test Loss: 30.207.. \n",
      "Epoch: 85/1000..  Training Loss: 28.092..  Test Loss: 33.646.. \n",
      "Epoch: 86/1000..  Training Loss: 28.172..  Test Loss: 31.043.. \n",
      "Epoch: 87/1000..  Training Loss: 28.442..  Test Loss: 29.085.. \n",
      "Epoch: 88/1000..  Training Loss: 27.733..  Test Loss: 28.735.. \n",
      "Epoch: 89/1000..  Training Loss: 27.659..  Test Loss: 32.408.. \n",
      "Epoch: 90/1000..  Training Loss: 27.976..  Test Loss: 29.673.. \n",
      "Epoch: 92/1000..  Training Loss: 27.494..  Test Loss: 28.021.. \n",
      "Epoch: 93/1000..  Training Loss: 27.903..  Test Loss: 28.259.. \n",
      "Epoch: 94/1000..  Training Loss: 27.246..  Test Loss: 28.632.. \n",
      "Epoch: 95/1000..  Training Loss: 27.606..  Test Loss: 28.885.. \n",
      "Epoch: 96/1000..  Training Loss: 27.326..  Test Loss: 28.280.. \n",
      "Epoch: 97/1000..  Training Loss: 27.303..  Test Loss: 27.985.. \n",
      "Epoch: 98/1000..  Training Loss: 27.906..  Test Loss: 29.249.. \n",
      "Epoch: 99/1000..  Training Loss: 27.114..  Test Loss: 32.076.. \n",
      "Epoch: 100/1000..  Training Loss: 27.198..  Test Loss: 29.612.. \n",
      "Epoch: 102/1000..  Training Loss: 27.310..  Test Loss: 28.714.. \n",
      "Epoch: 103/1000..  Training Loss: 26.878..  Test Loss: 27.789.. \n",
      "Epoch: 104/1000..  Training Loss: 26.761..  Test Loss: 30.191.. \n",
      "Epoch: 105/1000..  Training Loss: 26.917..  Test Loss: 33.035.. \n",
      "Epoch: 106/1000..  Training Loss: 27.142..  Test Loss: 30.036.. \n",
      "Epoch: 107/1000..  Training Loss: 27.223..  Test Loss: 29.457.. \n",
      "Epoch: 108/1000..  Training Loss: 26.799..  Test Loss: 27.981.. \n",
      "Epoch: 109/1000..  Training Loss: 26.197..  Test Loss: 28.747.. \n",
      "Epoch: 110/1000..  Training Loss: 26.317..  Test Loss: 28.359.. \n",
      "Epoch: 112/1000..  Training Loss: 26.178..  Test Loss: 27.997.. \n",
      "Epoch: 113/1000..  Training Loss: 25.696..  Test Loss: 32.532.. \n",
      "Epoch: 114/1000..  Training Loss: 26.155..  Test Loss: 32.588.. \n",
      "Epoch: 115/1000..  Training Loss: 25.557..  Test Loss: 27.323.. \n",
      "Epoch: 116/1000..  Training Loss: 26.049..  Test Loss: 27.844.. \n",
      "Epoch: 117/1000..  Training Loss: 26.334..  Test Loss: 28.075.. \n",
      "Epoch: 118/1000..  Training Loss: 26.009..  Test Loss: 28.011.. \n",
      "Epoch: 119/1000..  Training Loss: 25.802..  Test Loss: 28.637.. \n",
      "Epoch: 120/1000..  Training Loss: 26.140..  Test Loss: 28.305.. \n",
      "Epoch: 122/1000..  Training Loss: 26.198..  Test Loss: 28.336.. \n",
      "Epoch: 123/1000..  Training Loss: 25.855..  Test Loss: 30.230.. \n",
      "Epoch: 124/1000..  Training Loss: 25.709..  Test Loss: 28.510.. \n",
      "Epoch: 125/1000..  Training Loss: 25.282..  Test Loss: 26.883.. \n",
      "Epoch: 126/1000..  Training Loss: 25.847..  Test Loss: 28.417.. \n",
      "Epoch: 127/1000..  Training Loss: 25.841..  Test Loss: 27.560.. \n",
      "Epoch: 128/1000..  Training Loss: 25.949..  Test Loss: 27.796.. \n",
      "Epoch: 129/1000..  Training Loss: 25.210..  Test Loss: 27.065.. \n",
      "Epoch: 130/1000..  Training Loss: 25.465..  Test Loss: 27.825.. \n",
      "Epoch: 132/1000..  Training Loss: 25.422..  Test Loss: 32.455.. \n",
      "Epoch: 133/1000..  Training Loss: 26.994..  Test Loss: 35.463.. \n",
      "Epoch: 134/1000..  Training Loss: 30.172..  Test Loss: 35.621.. \n",
      "Epoch: 135/1000..  Training Loss: 29.345..  Test Loss: 31.464.. \n",
      "Epoch: 136/1000..  Training Loss: 29.454..  Test Loss: 30.474.. \n",
      "Epoch: 137/1000..  Training Loss: 28.919..  Test Loss: 30.137.. \n",
      "Epoch: 138/1000..  Training Loss: 27.819..  Test Loss: 29.362.. \n",
      "Epoch: 139/1000..  Training Loss: 27.611..  Test Loss: 28.858.. \n",
      "Epoch: 140/1000..  Training Loss: 27.577..  Test Loss: 28.639.. \n",
      "Epoch: 142/1000..  Training Loss: 27.276..  Test Loss: 28.868.. \n",
      "Epoch: 143/1000..  Training Loss: 26.677..  Test Loss: 29.002.. \n",
      "Epoch: 144/1000..  Training Loss: 27.337..  Test Loss: 29.155.. \n",
      "Epoch: 145/1000..  Training Loss: 26.683..  Test Loss: 28.609.. \n",
      "Epoch: 146/1000..  Training Loss: 26.432..  Test Loss: 28.793.. \n",
      "Epoch: 147/1000..  Training Loss: 26.186..  Test Loss: 29.119.. \n",
      "Epoch: 148/1000..  Training Loss: 26.064..  Test Loss: 28.807.. \n",
      "Epoch: 149/1000..  Training Loss: 25.903..  Test Loss: 28.248.. \n",
      "Epoch: 150/1000..  Training Loss: 25.933..  Test Loss: 27.982.. \n",
      "Epoch: 152/1000..  Training Loss: 25.727..  Test Loss: 27.287.. \n",
      "Epoch: 153/1000..  Training Loss: 25.632..  Test Loss: 27.050.. \n",
      "Epoch: 154/1000..  Training Loss: 25.376..  Test Loss: 27.409.. \n",
      "Epoch: 155/1000..  Training Loss: 25.244..  Test Loss: 27.140.. \n",
      "Epoch: 156/1000..  Training Loss: 24.898..  Test Loss: 26.696.. \n",
      "Epoch: 157/1000..  Training Loss: 24.636..  Test Loss: 26.551.. \n",
      "Epoch: 158/1000..  Training Loss: 25.480..  Test Loss: 27.574.. \n",
      "Epoch: 159/1000..  Training Loss: 25.030..  Test Loss: 27.880.. \n",
      "Epoch: 160/1000..  Training Loss: 25.294..  Test Loss: 27.245.. \n",
      "Epoch: 162/1000..  Training Loss: 25.105..  Test Loss: 27.462.. \n",
      "Epoch: 163/1000..  Training Loss: 24.693..  Test Loss: 27.733.. \n",
      "Epoch: 164/1000..  Training Loss: 24.536..  Test Loss: 26.290.. \n",
      "Epoch: 165/1000..  Training Loss: 24.470..  Test Loss: 27.436.. \n",
      "Epoch: 166/1000..  Training Loss: 24.432..  Test Loss: 27.041.. \n",
      "Epoch: 167/1000..  Training Loss: 25.077..  Test Loss: 27.534.. \n",
      "Epoch: 168/1000..  Training Loss: 25.412..  Test Loss: 29.041.. \n",
      "Epoch: 169/1000..  Training Loss: 25.338..  Test Loss: 27.198.. \n",
      "Epoch: 170/1000..  Training Loss: 24.806..  Test Loss: 28.328.. \n",
      "Epoch: 172/1000..  Training Loss: 24.779..  Test Loss: 26.949.. \n",
      "Epoch: 173/1000..  Training Loss: 24.371..  Test Loss: 26.475.. \n",
      "Epoch: 174/1000..  Training Loss: 24.427..  Test Loss: 27.145.. \n",
      "Epoch: 175/1000..  Training Loss: 24.614..  Test Loss: 27.186.. \n",
      "Epoch: 176/1000..  Training Loss: 24.011..  Test Loss: 26.218.. \n",
      "Epoch: 177/1000..  Training Loss: 24.398..  Test Loss: 26.583.. \n",
      "Epoch: 178/1000..  Training Loss: 24.548..  Test Loss: 26.346.. \n",
      "Epoch: 179/1000..  Training Loss: 24.291..  Test Loss: 26.327.. \n",
      "Epoch: 180/1000..  Training Loss: 24.392..  Test Loss: 26.467.. \n",
      "Epoch: 182/1000..  Training Loss: 24.318..  Test Loss: 26.595.. \n",
      "Epoch: 183/1000..  Training Loss: 24.088..  Test Loss: 28.809.. \n",
      "Epoch: 184/1000..  Training Loss: 23.875..  Test Loss: 28.366.. \n",
      "Epoch: 185/1000..  Training Loss: 23.568..  Test Loss: 25.560.. \n",
      "Epoch: 186/1000..  Training Loss: 23.088..  Test Loss: 25.935.. \n",
      "Epoch: 187/1000..  Training Loss: 22.940..  Test Loss: 26.038.. \n",
      "Epoch: 188/1000..  Training Loss: 23.068..  Test Loss: 25.620.. \n",
      "Epoch: 189/1000..  Training Loss: 23.079..  Test Loss: 26.243.. \n",
      "Epoch: 190/1000..  Training Loss: 22.704..  Test Loss: 25.827.. \n",
      "Epoch: 192/1000..  Training Loss: 22.623..  Test Loss: 26.358.. \n",
      "Epoch: 193/1000..  Training Loss: 23.361..  Test Loss: 27.393.. \n",
      "Epoch: 194/1000..  Training Loss: 23.024..  Test Loss: 27.063.. \n",
      "Epoch: 195/1000..  Training Loss: 23.286..  Test Loss: 27.263.. \n",
      "Epoch: 196/1000..  Training Loss: 22.615..  Test Loss: 25.628.. \n",
      "Epoch: 197/1000..  Training Loss: 23.006..  Test Loss: 26.461.. \n",
      "Epoch: 198/1000..  Training Loss: 23.083..  Test Loss: 26.035.. \n",
      "Epoch: 199/1000..  Training Loss: 23.061..  Test Loss: 25.879.. \n",
      "Epoch: 200/1000..  Training Loss: 22.756..  Test Loss: 27.431.. \n",
      "Epoch: 202/1000..  Training Loss: 22.654..  Test Loss: 25.466.. \n",
      "Epoch: 203/1000..  Training Loss: 22.876..  Test Loss: 25.398.. \n",
      "Epoch: 204/1000..  Training Loss: 22.948..  Test Loss: 26.783.. \n",
      "Epoch: 205/1000..  Training Loss: 22.553..  Test Loss: 26.407.. \n",
      "Epoch: 206/1000..  Training Loss: 22.371..  Test Loss: 25.502.. \n",
      "Epoch: 207/1000..  Training Loss: 22.386..  Test Loss: 25.928.. \n",
      "Epoch: 208/1000..  Training Loss: 22.531..  Test Loss: 25.529.. \n",
      "Epoch: 209/1000..  Training Loss: 22.055..  Test Loss: 25.476.. \n",
      "Epoch: 210/1000..  Training Loss: 21.743..  Test Loss: 24.733.. \n",
      "Epoch: 212/1000..  Training Loss: 21.731..  Test Loss: 25.619.. \n",
      "Epoch: 213/1000..  Training Loss: 21.958..  Test Loss: 25.909.. \n",
      "Epoch: 214/1000..  Training Loss: 21.802..  Test Loss: 26.262.. \n",
      "Epoch: 215/1000..  Training Loss: 22.084..  Test Loss: 26.173.. \n",
      "Epoch: 216/1000..  Training Loss: 22.071..  Test Loss: 26.544.. \n",
      "Epoch: 217/1000..  Training Loss: 21.859..  Test Loss: 26.503.. \n",
      "Epoch: 218/1000..  Training Loss: 21.685..  Test Loss: 26.008.. \n",
      "Epoch: 219/1000..  Training Loss: 22.003..  Test Loss: 26.731.. \n",
      "Epoch: 220/1000..  Training Loss: 22.597..  Test Loss: 26.624.. \n",
      "Epoch: 222/1000..  Training Loss: 22.293..  Test Loss: 25.927.. \n",
      "Epoch: 223/1000..  Training Loss: 22.130..  Test Loss: 25.140.. \n",
      "Epoch: 224/1000..  Training Loss: 21.895..  Test Loss: 25.842.. \n",
      "Epoch: 225/1000..  Training Loss: 22.286..  Test Loss: 29.167.. \n",
      "Epoch: 226/1000..  Training Loss: 22.711..  Test Loss: 27.133.. \n",
      "Epoch: 227/1000..  Training Loss: 21.862..  Test Loss: 26.284.. \n",
      "Epoch: 228/1000..  Training Loss: 21.835..  Test Loss: 25.163.. \n",
      "Epoch: 229/1000..  Training Loss: 21.206..  Test Loss: 26.740.. \n",
      "Epoch: 230/1000..  Training Loss: 21.602..  Test Loss: 24.842.. \n",
      "Epoch: 232/1000..  Training Loss: 21.640..  Test Loss: 25.311.. \n",
      "Epoch: 233/1000..  Training Loss: 21.469..  Test Loss: 24.969.. \n",
      "Epoch: 234/1000..  Training Loss: 21.306..  Test Loss: 24.339.. \n",
      "Epoch: 235/1000..  Training Loss: 21.181..  Test Loss: 24.536.. \n",
      "Epoch: 236/1000..  Training Loss: 21.054..  Test Loss: 24.886.. \n",
      "Epoch: 237/1000..  Training Loss: 20.879..  Test Loss: 25.969.. \n",
      "Epoch: 238/1000..  Training Loss: 21.316..  Test Loss: 26.327.. \n",
      "Epoch: 239/1000..  Training Loss: 21.800..  Test Loss: 26.184.. \n",
      "Epoch: 240/1000..  Training Loss: 22.329..  Test Loss: 26.440.. \n",
      "Epoch: 242/1000..  Training Loss: 22.003..  Test Loss: 26.833.. \n",
      "Epoch: 243/1000..  Training Loss: 21.564..  Test Loss: 25.226.. \n",
      "Epoch: 244/1000..  Training Loss: 20.783..  Test Loss: 25.018.. \n",
      "Epoch: 245/1000..  Training Loss: 20.793..  Test Loss: 24.912.. \n",
      "Epoch: 246/1000..  Training Loss: 20.902..  Test Loss: 24.679.. \n",
      "Epoch: 247/1000..  Training Loss: 20.487..  Test Loss: 24.493.. \n",
      "Epoch: 248/1000..  Training Loss: 20.616..  Test Loss: 26.345.. \n",
      "Epoch: 249/1000..  Training Loss: 20.978..  Test Loss: 24.533.. \n",
      "Epoch: 250/1000..  Training Loss: 20.562..  Test Loss: 31.661.. \n",
      "Epoch: 252/1000..  Training Loss: 22.166..  Test Loss: 33.476.. \n",
      "Epoch: 253/1000..  Training Loss: 25.161..  Test Loss: 34.092.. \n",
      "Epoch: 254/1000..  Training Loss: 25.033..  Test Loss: 26.924.. \n",
      "Epoch: 255/1000..  Training Loss: 24.399..  Test Loss: 26.654.. \n",
      "Epoch: 256/1000..  Training Loss: 23.457..  Test Loss: 26.123.. \n",
      "Epoch: 257/1000..  Training Loss: 22.786..  Test Loss: 25.674.. \n",
      "Epoch: 258/1000..  Training Loss: 22.307..  Test Loss: 25.936.. \n",
      "Epoch: 259/1000..  Training Loss: 22.143..  Test Loss: 25.182.. \n",
      "Epoch: 260/1000..  Training Loss: 21.480..  Test Loss: 25.323.. \n",
      "Epoch: 262/1000..  Training Loss: 21.535..  Test Loss: 26.704.. \n",
      "Epoch: 263/1000..  Training Loss: 21.037..  Test Loss: 25.337.. \n",
      "Epoch: 264/1000..  Training Loss: 20.988..  Test Loss: 24.668.. \n",
      "Epoch: 265/1000..  Training Loss: 20.695..  Test Loss: 24.270.. \n",
      "Epoch: 266/1000..  Training Loss: 20.649..  Test Loss: 25.606.. \n",
      "Epoch: 267/1000..  Training Loss: 21.217..  Test Loss: 26.969.. \n",
      "Epoch: 268/1000..  Training Loss: 21.538..  Test Loss: 24.761.. \n",
      "Epoch: 269/1000..  Training Loss: 21.015..  Test Loss: 25.257.. \n",
      "Epoch: 270/1000..  Training Loss: 20.733..  Test Loss: 25.035.. \n",
      "Epoch: 272/1000..  Training Loss: 20.976..  Test Loss: 25.433.. \n",
      "Epoch: 273/1000..  Training Loss: 20.313..  Test Loss: 24.950.. \n",
      "Epoch: 274/1000..  Training Loss: 20.926..  Test Loss: 26.527.. \n",
      "Epoch: 275/1000..  Training Loss: 21.019..  Test Loss: 24.922.. \n",
      "Epoch: 276/1000..  Training Loss: 20.930..  Test Loss: 23.904.. \n",
      "Epoch: 277/1000..  Training Loss: 20.376..  Test Loss: 24.121.. \n",
      "Epoch: 278/1000..  Training Loss: 19.639..  Test Loss: 23.866.. \n",
      "Epoch: 279/1000..  Training Loss: 19.828..  Test Loss: 26.391.. \n",
      "Epoch: 280/1000..  Training Loss: 19.890..  Test Loss: 24.508.. \n",
      "Epoch: 282/1000..  Training Loss: 19.947..  Test Loss: 24.043.. \n",
      "Epoch: 283/1000..  Training Loss: 19.542..  Test Loss: 24.866.. \n",
      "Epoch: 284/1000..  Training Loss: 19.851..  Test Loss: 25.128.. \n",
      "Epoch: 285/1000..  Training Loss: 19.489..  Test Loss: 24.154.. \n",
      "Epoch: 286/1000..  Training Loss: 19.649..  Test Loss: 23.661.. \n",
      "Epoch: 287/1000..  Training Loss: 19.217..  Test Loss: 23.648.. \n",
      "Epoch: 288/1000..  Training Loss: 19.492..  Test Loss: 24.232.. \n",
      "Epoch: 289/1000..  Training Loss: 19.663..  Test Loss: 25.160.. \n",
      "Epoch: 290/1000..  Training Loss: 19.250..  Test Loss: 23.630.. \n",
      "Epoch: 292/1000..  Training Loss: 19.144..  Test Loss: 24.409.. \n",
      "Epoch: 293/1000..  Training Loss: 19.133..  Test Loss: 25.119.. \n",
      "Epoch: 294/1000..  Training Loss: 19.067..  Test Loss: 24.162.. \n",
      "Epoch: 295/1000..  Training Loss: 19.012..  Test Loss: 24.571.. \n",
      "Epoch: 296/1000..  Training Loss: 19.093..  Test Loss: 24.146.. \n",
      "Epoch: 297/1000..  Training Loss: 18.947..  Test Loss: 24.524.. \n",
      "Epoch: 298/1000..  Training Loss: 18.797..  Test Loss: 24.007.. \n",
      "Epoch: 299/1000..  Training Loss: 18.821..  Test Loss: 23.592.. \n",
      "Epoch: 300/1000..  Training Loss: 18.936..  Test Loss: 23.835.. \n",
      "Epoch: 302/1000..  Training Loss: 18.897..  Test Loss: 24.455.. \n",
      "Epoch: 303/1000..  Training Loss: 19.455..  Test Loss: 27.429.. \n",
      "Epoch: 304/1000..  Training Loss: 20.085..  Test Loss: 23.757.. \n",
      "Epoch: 305/1000..  Training Loss: 19.444..  Test Loss: 24.146.. \n",
      "Epoch: 306/1000..  Training Loss: 19.443..  Test Loss: 24.261.. \n",
      "Epoch: 307/1000..  Training Loss: 19.293..  Test Loss: 27.019.. \n",
      "Epoch: 308/1000..  Training Loss: 19.422..  Test Loss: 23.829.. \n",
      "Epoch: 309/1000..  Training Loss: 18.828..  Test Loss: 24.200.. \n",
      "Epoch: 310/1000..  Training Loss: 18.697..  Test Loss: 23.657.. \n",
      "Epoch: 312/1000..  Training Loss: 18.439..  Test Loss: 23.279.. \n",
      "Epoch: 313/1000..  Training Loss: 18.119..  Test Loss: 26.325.. \n",
      "Epoch: 314/1000..  Training Loss: 18.707..  Test Loss: 24.767.. \n",
      "Epoch: 315/1000..  Training Loss: 18.674..  Test Loss: 24.538.. \n",
      "Epoch: 316/1000..  Training Loss: 18.430..  Test Loss: 25.012.. \n",
      "Epoch: 317/1000..  Training Loss: 18.582..  Test Loss: 24.390.. \n",
      "Epoch: 318/1000..  Training Loss: 18.975..  Test Loss: 25.476.. \n",
      "Epoch: 319/1000..  Training Loss: 19.222..  Test Loss: 24.956.. \n",
      "Epoch: 320/1000..  Training Loss: 19.082..  Test Loss: 24.465.. \n",
      "Epoch: 322/1000..  Training Loss: 18.286..  Test Loss: 23.311.. \n",
      "Epoch: 323/1000..  Training Loss: 17.670..  Test Loss: 23.925.. \n",
      "Epoch: 324/1000..  Training Loss: 18.077..  Test Loss: 23.151.. \n",
      "Epoch: 325/1000..  Training Loss: 18.189..  Test Loss: 25.433.. \n",
      "Epoch: 326/1000..  Training Loss: 18.696..  Test Loss: 24.457.. \n",
      "Epoch: 327/1000..  Training Loss: 19.000..  Test Loss: 24.237.. \n",
      "Epoch: 328/1000..  Training Loss: 18.499..  Test Loss: 24.672.. \n",
      "Epoch: 329/1000..  Training Loss: 18.247..  Test Loss: 24.299.. \n",
      "Epoch: 330/1000..  Training Loss: 17.715..  Test Loss: 23.539.. \n",
      "Epoch: 332/1000..  Training Loss: 18.319..  Test Loss: 24.198.. \n",
      "Epoch: 333/1000..  Training Loss: 18.001..  Test Loss: 23.361.. \n",
      "Epoch: 334/1000..  Training Loss: 18.456..  Test Loss: 24.195.. \n",
      "Epoch: 335/1000..  Training Loss: 18.170..  Test Loss: 23.455.. \n",
      "Epoch: 336/1000..  Training Loss: 17.797..  Test Loss: 23.556.. \n",
      "Epoch: 337/1000..  Training Loss: 17.768..  Test Loss: 23.556.. \n",
      "Epoch: 338/1000..  Training Loss: 17.227..  Test Loss: 23.095.. \n",
      "Epoch: 339/1000..  Training Loss: 17.179..  Test Loss: 23.076.. \n",
      "Epoch: 340/1000..  Training Loss: 17.204..  Test Loss: 23.646.. \n",
      "Epoch: 342/1000..  Training Loss: 17.487..  Test Loss: 23.673.. \n",
      "Epoch: 343/1000..  Training Loss: 17.191..  Test Loss: 24.905.. \n",
      "Epoch: 344/1000..  Training Loss: 17.152..  Test Loss: 22.987.. \n",
      "Epoch: 345/1000..  Training Loss: 17.383..  Test Loss: 23.828.. \n",
      "Epoch: 346/1000..  Training Loss: 17.150..  Test Loss: 23.575.. \n",
      "Epoch: 347/1000..  Training Loss: 17.451..  Test Loss: 23.442.. \n",
      "Epoch: 348/1000..  Training Loss: 16.884..  Test Loss: 22.766.. \n",
      "Epoch: 349/1000..  Training Loss: 17.203..  Test Loss: 23.303.. \n",
      "Epoch: 350/1000..  Training Loss: 17.216..  Test Loss: 23.101.. \n",
      "Epoch: 352/1000..  Training Loss: 17.446..  Test Loss: 24.354.. \n",
      "Epoch: 353/1000..  Training Loss: 17.462..  Test Loss: 23.921.. \n",
      "Epoch: 354/1000..  Training Loss: 17.099..  Test Loss: 23.884.. \n",
      "Epoch: 355/1000..  Training Loss: 16.595..  Test Loss: 24.276.. \n",
      "Epoch: 356/1000..  Training Loss: 16.763..  Test Loss: 25.467.. \n",
      "Epoch: 357/1000..  Training Loss: 16.000..  Test Loss: 22.523.. \n",
      "Epoch: 358/1000..  Training Loss: 16.256..  Test Loss: 22.818.. \n",
      "Epoch: 359/1000..  Training Loss: 15.906..  Test Loss: 23.396.. \n",
      "Epoch: 360/1000..  Training Loss: 16.424..  Test Loss: 24.031.. \n",
      "Epoch: 362/1000..  Training Loss: 16.202..  Test Loss: 23.431.. \n",
      "Epoch: 363/1000..  Training Loss: 15.991..  Test Loss: 23.393.. \n",
      "Epoch: 364/1000..  Training Loss: 16.281..  Test Loss: 22.969.. \n",
      "Epoch: 365/1000..  Training Loss: 16.840..  Test Loss: 23.751.. \n",
      "Epoch: 366/1000..  Training Loss: 16.578..  Test Loss: 23.232.. \n",
      "Epoch: 367/1000..  Training Loss: 16.770..  Test Loss: 23.817.. \n",
      "Epoch: 368/1000..  Training Loss: 16.453..  Test Loss: 24.364.. \n",
      "Epoch: 369/1000..  Training Loss: 16.106..  Test Loss: 24.124.. \n",
      "Epoch: 370/1000..  Training Loss: 16.885..  Test Loss: 24.578.. \n",
      "Epoch: 372/1000..  Training Loss: 16.405..  Test Loss: 24.793.. \n",
      "Epoch: 373/1000..  Training Loss: 16.629..  Test Loss: 23.693.. \n",
      "Epoch: 374/1000..  Training Loss: 16.652..  Test Loss: 23.502.. \n",
      "Epoch: 375/1000..  Training Loss: 16.515..  Test Loss: 26.213.. \n",
      "Epoch: 376/1000..  Training Loss: 17.051..  Test Loss: 24.842.. \n",
      "Epoch: 377/1000..  Training Loss: 17.051..  Test Loss: 23.438.. \n",
      "Epoch: 378/1000..  Training Loss: 16.189..  Test Loss: 23.051.. \n",
      "Epoch: 379/1000..  Training Loss: 16.461..  Test Loss: 22.493.. \n",
      "Epoch: 380/1000..  Training Loss: 15.884..  Test Loss: 23.761.. \n",
      "Epoch: 382/1000..  Training Loss: 16.073..  Test Loss: 23.551.. \n",
      "Epoch: 383/1000..  Training Loss: 16.876..  Test Loss: 24.108.. \n",
      "Epoch: 384/1000..  Training Loss: 17.367..  Test Loss: 23.734.. \n",
      "Epoch: 385/1000..  Training Loss: 17.076..  Test Loss: 23.641.. \n",
      "Epoch: 386/1000..  Training Loss: 16.677..  Test Loss: 23.323.. \n",
      "Epoch: 387/1000..  Training Loss: 16.745..  Test Loss: 23.977.. \n",
      "Epoch: 388/1000..  Training Loss: 16.405..  Test Loss: 23.033.. \n",
      "Epoch: 389/1000..  Training Loss: 16.118..  Test Loss: 23.245.. \n",
      "Epoch: 390/1000..  Training Loss: 16.186..  Test Loss: 22.853.. \n",
      "Epoch: 392/1000..  Training Loss: 16.230..  Test Loss: 22.716.. \n",
      "Epoch: 393/1000..  Training Loss: 15.829..  Test Loss: 22.837.. \n",
      "Epoch: 394/1000..  Training Loss: 15.625..  Test Loss: 23.041.. \n",
      "Epoch: 395/1000..  Training Loss: 15.276..  Test Loss: 23.530.. \n",
      "Epoch: 396/1000..  Training Loss: 15.125..  Test Loss: 22.701.. \n",
      "Epoch: 397/1000..  Training Loss: 15.950..  Test Loss: 22.863.. \n",
      "Epoch: 398/1000..  Training Loss: 15.621..  Test Loss: 22.780.. \n",
      "Epoch: 399/1000..  Training Loss: 15.812..  Test Loss: 23.195.. \n",
      "Epoch: 400/1000..  Training Loss: 15.443..  Test Loss: 23.152.. \n",
      "Epoch: 402/1000..  Training Loss: 15.458..  Test Loss: 23.100.. \n",
      "Epoch: 403/1000..  Training Loss: 14.944..  Test Loss: 22.847.. \n",
      "Epoch: 404/1000..  Training Loss: 15.118..  Test Loss: 23.819.. \n",
      "Epoch: 405/1000..  Training Loss: 15.265..  Test Loss: 22.872.. \n",
      "Epoch: 406/1000..  Training Loss: 15.470..  Test Loss: 23.127.. \n",
      "Epoch: 407/1000..  Training Loss: 15.960..  Test Loss: 23.401.. \n",
      "Epoch: 408/1000..  Training Loss: 16.275..  Test Loss: 23.673.. \n",
      "Epoch: 409/1000..  Training Loss: 15.931..  Test Loss: 22.680.. \n",
      "Epoch: 410/1000..  Training Loss: 15.380..  Test Loss: 22.535.. \n",
      "Epoch: 412/1000..  Training Loss: 15.602..  Test Loss: 23.450.. \n",
      "Epoch: 413/1000..  Training Loss: 15.309..  Test Loss: 23.222.. \n",
      "Epoch: 414/1000..  Training Loss: 15.456..  Test Loss: 23.689.. \n",
      "Epoch: 415/1000..  Training Loss: 15.322..  Test Loss: 22.351.. \n",
      "Epoch: 416/1000..  Training Loss: 15.681..  Test Loss: 22.541.. \n",
      "Epoch: 417/1000..  Training Loss: 15.177..  Test Loss: 22.405.. \n",
      "Epoch: 418/1000..  Training Loss: 14.860..  Test Loss: 22.407.. \n",
      "Epoch: 419/1000..  Training Loss: 14.550..  Test Loss: 22.373.. \n",
      "Epoch: 420/1000..  Training Loss: 15.005..  Test Loss: 23.587.. \n",
      "Epoch: 422/1000..  Training Loss: 14.760..  Test Loss: 23.085.. \n",
      "Epoch: 423/1000..  Training Loss: 14.702..  Test Loss: 24.161.. \n",
      "Epoch: 424/1000..  Training Loss: 17.469..  Test Loss: 23.591.. \n",
      "Epoch: 425/1000..  Training Loss: 16.813..  Test Loss: 22.634.. \n",
      "Epoch: 426/1000..  Training Loss: 16.407..  Test Loss: 27.556.. \n",
      "Epoch: 427/1000..  Training Loss: 16.656..  Test Loss: 24.224.. \n",
      "Epoch: 428/1000..  Training Loss: 15.539..  Test Loss: 23.891.. \n",
      "Epoch: 429/1000..  Training Loss: 15.682..  Test Loss: 22.782.. \n",
      "Epoch: 430/1000..  Training Loss: 15.279..  Test Loss: 22.687.. \n",
      "Epoch: 432/1000..  Training Loss: 15.229..  Test Loss: 22.125.. \n",
      "Epoch: 433/1000..  Training Loss: 14.589..  Test Loss: 25.967.. \n",
      "Epoch: 434/1000..  Training Loss: 15.202..  Test Loss: 22.521.. \n",
      "Epoch: 435/1000..  Training Loss: 14.520..  Test Loss: 22.238.. \n",
      "Epoch: 436/1000..  Training Loss: 14.600..  Test Loss: 24.296.. \n",
      "Epoch: 437/1000..  Training Loss: 15.474..  Test Loss: 23.200.. \n",
      "Epoch: 438/1000..  Training Loss: 15.125..  Test Loss: 22.803.. \n",
      "Epoch: 439/1000..  Training Loss: 15.128..  Test Loss: 23.830.. \n",
      "Epoch: 440/1000..  Training Loss: 15.168..  Test Loss: 23.278.. \n",
      "Epoch: 442/1000..  Training Loss: 14.451..  Test Loss: 22.712.. \n",
      "Epoch: 443/1000..  Training Loss: 14.147..  Test Loss: 22.078.. \n",
      "Epoch: 444/1000..  Training Loss: 14.046..  Test Loss: 22.409.. \n",
      "Epoch: 445/1000..  Training Loss: 13.856..  Test Loss: 22.754.. \n",
      "Epoch: 446/1000..  Training Loss: 14.036..  Test Loss: 22.014.. \n",
      "Epoch: 447/1000..  Training Loss: 14.036..  Test Loss: 23.490.. \n",
      "Epoch: 448/1000..  Training Loss: 14.159..  Test Loss: 22.740.. \n",
      "Epoch: 449/1000..  Training Loss: 14.267..  Test Loss: 22.786.. \n",
      "Epoch: 450/1000..  Training Loss: 13.807..  Test Loss: 23.355.. \n",
      "Epoch: 452/1000..  Training Loss: 13.551..  Test Loss: 22.082.. \n",
      "Epoch: 453/1000..  Training Loss: 13.330..  Test Loss: 22.660.. \n",
      "Epoch: 454/1000..  Training Loss: 13.404..  Test Loss: 22.243.. \n",
      "Epoch: 455/1000..  Training Loss: 13.610..  Test Loss: 22.497.. \n",
      "Epoch: 456/1000..  Training Loss: 14.352..  Test Loss: 24.068.. \n",
      "Epoch: 457/1000..  Training Loss: 14.330..  Test Loss: 23.097.. \n",
      "Epoch: 458/1000..  Training Loss: 14.134..  Test Loss: 22.605.. \n",
      "Epoch: 459/1000..  Training Loss: 14.133..  Test Loss: 22.844.. \n",
      "Epoch: 460/1000..  Training Loss: 14.073..  Test Loss: 21.992.. \n",
      "Epoch: 462/1000..  Training Loss: 13.937..  Test Loss: 22.837.. \n",
      "Epoch: 463/1000..  Training Loss: 13.502..  Test Loss: 23.123.. \n",
      "Epoch: 464/1000..  Training Loss: 14.828..  Test Loss: 23.181.. \n",
      "Epoch: 465/1000..  Training Loss: 15.067..  Test Loss: 23.388.. \n",
      "Epoch: 466/1000..  Training Loss: 13.676..  Test Loss: 22.167.. \n",
      "Epoch: 467/1000..  Training Loss: 13.939..  Test Loss: 23.166.. \n",
      "Epoch: 468/1000..  Training Loss: 14.061..  Test Loss: 22.429.. \n",
      "Epoch: 469/1000..  Training Loss: 13.472..  Test Loss: 22.571.. \n",
      "Epoch: 470/1000..  Training Loss: 13.527..  Test Loss: 22.247.. \n",
      "Epoch: 472/1000..  Training Loss: 13.128..  Test Loss: 22.779.. \n",
      "Epoch: 473/1000..  Training Loss: 13.362..  Test Loss: 22.615.. \n",
      "Epoch: 474/1000..  Training Loss: 14.030..  Test Loss: 22.928.. \n",
      "Epoch: 475/1000..  Training Loss: 13.993..  Test Loss: 23.114.. \n",
      "Epoch: 476/1000..  Training Loss: 13.606..  Test Loss: 22.091.. \n",
      "Epoch: 477/1000..  Training Loss: 13.811..  Test Loss: 22.392.. \n",
      "Epoch: 478/1000..  Training Loss: 14.212..  Test Loss: 22.147.. \n",
      "Epoch: 479/1000..  Training Loss: 14.216..  Test Loss: 22.335.. \n",
      "Epoch: 480/1000..  Training Loss: 14.325..  Test Loss: 22.823.. \n",
      "Epoch: 482/1000..  Training Loss: 13.757..  Test Loss: 22.761.. \n",
      "Epoch: 483/1000..  Training Loss: 13.268..  Test Loss: 23.789.. \n",
      "Epoch: 484/1000..  Training Loss: 13.482..  Test Loss: 22.207.. \n",
      "Epoch: 485/1000..  Training Loss: 12.988..  Test Loss: 21.799.. \n",
      "Epoch: 486/1000..  Training Loss: 13.337..  Test Loss: 22.780.. \n",
      "Epoch: 487/1000..  Training Loss: 13.471..  Test Loss: 22.779.. \n",
      "Epoch: 488/1000..  Training Loss: 13.753..  Test Loss: 22.697.. \n",
      "Epoch: 489/1000..  Training Loss: 13.253..  Test Loss: 21.701.. \n",
      "Epoch: 490/1000..  Training Loss: 12.860..  Test Loss: 22.254.. \n",
      "Epoch: 492/1000..  Training Loss: 12.616..  Test Loss: 22.126.. \n",
      "Epoch: 493/1000..  Training Loss: 12.814..  Test Loss: 22.311.. \n",
      "Epoch: 494/1000..  Training Loss: 12.644..  Test Loss: 23.082.. \n",
      "Epoch: 495/1000..  Training Loss: 12.643..  Test Loss: 21.935.. \n",
      "Epoch: 496/1000..  Training Loss: 13.411..  Test Loss: 22.619.. \n",
      "Epoch: 497/1000..  Training Loss: 13.320..  Test Loss: 22.395.. \n",
      "Epoch: 498/1000..  Training Loss: 13.402..  Test Loss: 22.258.. \n",
      "Epoch: 499/1000..  Training Loss: 12.684..  Test Loss: 22.182.. \n",
      "Epoch: 500/1000..  Training Loss: 12.707..  Test Loss: 22.917.. \n",
      "Epoch: 502/1000..  Training Loss: 12.609..  Test Loss: 22.671.. \n",
      "Epoch: 503/1000..  Training Loss: 12.573..  Test Loss: 22.835.. \n",
      "Epoch: 504/1000..  Training Loss: 13.164..  Test Loss: 23.530.. \n",
      "Epoch: 505/1000..  Training Loss: 13.041..  Test Loss: 23.952.. \n",
      "Epoch: 506/1000..  Training Loss: 12.607..  Test Loss: 22.563.. \n",
      "Epoch: 507/1000..  Training Loss: 12.510..  Test Loss: 21.999.. \n",
      "Epoch: 508/1000..  Training Loss: 12.108..  Test Loss: 21.788.. \n",
      "Epoch: 509/1000..  Training Loss: 12.326..  Test Loss: 21.634.. \n",
      "Epoch: 510/1000..  Training Loss: 12.786..  Test Loss: 23.113.. \n",
      "Epoch: 512/1000..  Training Loss: 12.514..  Test Loss: 23.555.. \n",
      "Epoch: 513/1000..  Training Loss: 13.209..  Test Loss: 24.624.. \n",
      "Epoch: 514/1000..  Training Loss: 14.056..  Test Loss: 24.010.. \n",
      "Epoch: 515/1000..  Training Loss: 14.009..  Test Loss: 24.013.. \n",
      "Epoch: 516/1000..  Training Loss: 14.094..  Test Loss: 24.534.. \n",
      "Epoch: 517/1000..  Training Loss: 14.751..  Test Loss: 22.296.. \n",
      "Epoch: 518/1000..  Training Loss: 13.422..  Test Loss: 21.880.. \n",
      "Epoch: 519/1000..  Training Loss: 12.993..  Test Loss: 21.318.. \n",
      "Epoch: 520/1000..  Training Loss: 12.521..  Test Loss: 21.660.. \n",
      "Epoch: 522/1000..  Training Loss: 12.437..  Test Loss: 23.406.. \n",
      "Epoch: 523/1000..  Training Loss: 12.835..  Test Loss: 22.903.. \n",
      "Epoch: 524/1000..  Training Loss: 12.486..  Test Loss: 21.599.. \n",
      "Epoch: 525/1000..  Training Loss: 12.283..  Test Loss: 22.349.. \n",
      "Epoch: 526/1000..  Training Loss: 12.162..  Test Loss: 22.674.. \n",
      "Epoch: 527/1000..  Training Loss: 12.236..  Test Loss: 25.908.. \n",
      "Epoch: 528/1000..  Training Loss: 11.837..  Test Loss: 22.382.. \n",
      "Epoch: 529/1000..  Training Loss: 11.988..  Test Loss: 23.108.. \n",
      "Epoch: 530/1000..  Training Loss: 12.032..  Test Loss: 22.107.. \n",
      "Epoch: 532/1000..  Training Loss: 11.562..  Test Loss: 21.769.. \n",
      "Epoch: 533/1000..  Training Loss: 11.379..  Test Loss: 22.258.. \n",
      "Epoch: 534/1000..  Training Loss: 11.266..  Test Loss: 23.691.. \n",
      "Epoch: 535/1000..  Training Loss: 11.016..  Test Loss: 23.866.. \n",
      "Epoch: 536/1000..  Training Loss: 10.990..  Test Loss: 22.811.. \n",
      "Epoch: 537/1000..  Training Loss: 11.325..  Test Loss: 22.710.. \n",
      "Epoch: 538/1000..  Training Loss: 11.242..  Test Loss: 22.604.. \n",
      "Epoch: 539/1000..  Training Loss: 10.993..  Test Loss: 23.142.. \n",
      "Epoch: 540/1000..  Training Loss: 11.439..  Test Loss: 23.639.. \n",
      "Epoch: 542/1000..  Training Loss: 10.981..  Test Loss: 23.864.. \n",
      "Epoch: 543/1000..  Training Loss: 11.495..  Test Loss: 22.703.. \n",
      "Epoch: 544/1000..  Training Loss: 10.890..  Test Loss: 22.958.. \n",
      "Epoch: 545/1000..  Training Loss: 11.030..  Test Loss: 22.504.. \n",
      "Epoch: 546/1000..  Training Loss: 10.434..  Test Loss: 22.255.. \n",
      "Epoch: 547/1000..  Training Loss: 11.113..  Test Loss: 21.474.. \n",
      "Epoch: 548/1000..  Training Loss: 12.433..  Test Loss: 23.870.. \n",
      "Epoch: 549/1000..  Training Loss: 12.117..  Test Loss: 27.601.. \n",
      "Epoch: 550/1000..  Training Loss: 12.760..  Test Loss: 23.379.. \n",
      "Epoch: 552/1000..  Training Loss: 12.799..  Test Loss: 22.207.. \n",
      "Epoch: 553/1000..  Training Loss: 12.396..  Test Loss: 22.570.. \n",
      "Epoch: 554/1000..  Training Loss: 11.742..  Test Loss: 23.171.. \n",
      "Epoch: 555/1000..  Training Loss: 11.511..  Test Loss: 22.576.. \n",
      "Epoch: 556/1000..  Training Loss: 11.228..  Test Loss: 22.886.. \n",
      "Epoch: 557/1000..  Training Loss: 11.030..  Test Loss: 22.091.. \n",
      "Epoch: 558/1000..  Training Loss: 11.855..  Test Loss: 22.422.. \n",
      "Epoch: 559/1000..  Training Loss: 11.549..  Test Loss: 25.475.. \n",
      "Epoch: 560/1000..  Training Loss: 11.363..  Test Loss: 22.128.. \n",
      "Epoch: 562/1000..  Training Loss: 11.277..  Test Loss: 22.059.. \n",
      "Epoch: 563/1000..  Training Loss: 12.395..  Test Loss: 22.982.. \n",
      "Epoch: 564/1000..  Training Loss: 12.084..  Test Loss: 22.294.. \n",
      "Epoch: 565/1000..  Training Loss: 11.472..  Test Loss: 22.013.. \n",
      "Epoch: 566/1000..  Training Loss: 11.275..  Test Loss: 22.627.. \n",
      "Epoch: 567/1000..  Training Loss: 11.103..  Test Loss: 21.783.. \n",
      "Epoch: 568/1000..  Training Loss: 10.706..  Test Loss: 22.119.. \n",
      "Epoch: 569/1000..  Training Loss: 10.341..  Test Loss: 24.117.. \n",
      "Epoch: 570/1000..  Training Loss: 10.223..  Test Loss: 24.412.. \n",
      "Epoch: 572/1000..  Training Loss: 10.070..  Test Loss: 23.075.. \n",
      "Epoch: 573/1000..  Training Loss: 10.677..  Test Loss: 22.260.. \n",
      "Epoch: 574/1000..  Training Loss: 10.298..  Test Loss: 21.507.. \n",
      "Epoch: 575/1000..  Training Loss: 9.979..  Test Loss: 21.451.. \n",
      "Epoch: 576/1000..  Training Loss: 10.019..  Test Loss: 22.527.. \n",
      "Epoch: 577/1000..  Training Loss: 10.092..  Test Loss: 22.663.. \n",
      "Epoch: 578/1000..  Training Loss: 9.812..  Test Loss: 23.157.. \n",
      "Epoch: 579/1000..  Training Loss: 9.417..  Test Loss: 22.821.. \n",
      "Epoch: 580/1000..  Training Loss: 9.614..  Test Loss: 22.529.. \n",
      "Epoch: 582/1000..  Training Loss: 9.470..  Test Loss: 23.320.. \n",
      "Epoch: 583/1000..  Training Loss: 10.067..  Test Loss: 23.544.. \n",
      "Epoch: 584/1000..  Training Loss: 9.955..  Test Loss: 24.343.. \n",
      "Epoch: 585/1000..  Training Loss: 10.086..  Test Loss: 26.520.. \n",
      "Epoch: 586/1000..  Training Loss: 10.151..  Test Loss: 23.214.. \n",
      "Epoch: 587/1000..  Training Loss: 10.010..  Test Loss: 22.383.. \n",
      "Epoch: 588/1000..  Training Loss: 9.890..  Test Loss: 22.489.. \n",
      "Epoch: 589/1000..  Training Loss: 9.626..  Test Loss: 22.905.. \n",
      "Epoch: 590/1000..  Training Loss: 10.699..  Test Loss: 22.782.. \n",
      "Epoch: 592/1000..  Training Loss: 10.459..  Test Loss: 22.226.. \n",
      "Epoch: 593/1000..  Training Loss: 10.474..  Test Loss: 22.705.. \n",
      "Epoch: 594/1000..  Training Loss: 10.519..  Test Loss: 22.503.. \n",
      "Epoch: 595/1000..  Training Loss: 10.549..  Test Loss: 22.980.. \n",
      "Epoch: 596/1000..  Training Loss: 10.995..  Test Loss: 22.755.. \n",
      "Epoch: 597/1000..  Training Loss: 11.576..  Test Loss: 23.167.. \n",
      "Epoch: 598/1000..  Training Loss: 12.636..  Test Loss: 22.537.. \n",
      "Epoch: 599/1000..  Training Loss: 12.130..  Test Loss: 22.190.. \n",
      "Epoch: 600/1000..  Training Loss: 12.048..  Test Loss: 22.211.. \n",
      "Epoch: 602/1000..  Training Loss: 12.083..  Test Loss: 22.317.. \n",
      "Epoch: 603/1000..  Training Loss: 11.323..  Test Loss: 22.443.. \n",
      "Epoch: 604/1000..  Training Loss: 11.268..  Test Loss: 22.870.. \n",
      "Epoch: 605/1000..  Training Loss: 10.814..  Test Loss: 23.257.. \n",
      "Epoch: 606/1000..  Training Loss: 9.903..  Test Loss: 21.973.. \n",
      "Epoch: 607/1000..  Training Loss: 10.378..  Test Loss: 22.874.. \n",
      "Epoch: 608/1000..  Training Loss: 10.092..  Test Loss: 23.679.. \n",
      "Epoch: 609/1000..  Training Loss: 9.639..  Test Loss: 23.031.. \n",
      "Epoch: 610/1000..  Training Loss: 9.705..  Test Loss: 22.736.. \n",
      "Epoch: 612/1000..  Training Loss: 9.241..  Test Loss: 25.428.. \n",
      "Epoch: 613/1000..  Training Loss: 9.198..  Test Loss: 22.504.. \n",
      "Epoch: 614/1000..  Training Loss: 9.399..  Test Loss: 22.452.. \n",
      "Epoch: 615/1000..  Training Loss: 9.209..  Test Loss: 22.932.. \n",
      "Epoch: 616/1000..  Training Loss: 9.504..  Test Loss: 23.197.. \n",
      "Epoch: 617/1000..  Training Loss: 9.975..  Test Loss: 23.435.. \n",
      "Epoch: 618/1000..  Training Loss: 9.251..  Test Loss: 22.601.. \n",
      "Epoch: 619/1000..  Training Loss: 9.260..  Test Loss: 22.798.. \n",
      "Epoch: 620/1000..  Training Loss: 9.074..  Test Loss: 22.997.. \n",
      "Epoch: 622/1000..  Training Loss: 8.913..  Test Loss: 22.487.. \n",
      "Epoch: 623/1000..  Training Loss: 8.564..  Test Loss: 22.034.. \n",
      "Epoch: 624/1000..  Training Loss: 8.970..  Test Loss: 21.694.. \n",
      "Epoch: 625/1000..  Training Loss: 9.324..  Test Loss: 23.561.. \n",
      "Epoch: 626/1000..  Training Loss: 9.164..  Test Loss: 23.200.. \n",
      "Epoch: 627/1000..  Training Loss: 8.975..  Test Loss: 23.241.. \n",
      "Epoch: 628/1000..  Training Loss: 9.940..  Test Loss: 22.670.. \n",
      "Epoch: 629/1000..  Training Loss: 9.763..  Test Loss: 21.962.. \n",
      "Epoch: 630/1000..  Training Loss: 8.831..  Test Loss: 22.736.. \n",
      "Epoch: 632/1000..  Training Loss: 8.481..  Test Loss: 22.997.. \n",
      "Epoch: 633/1000..  Training Loss: 8.108..  Test Loss: 23.324.. \n",
      "Epoch: 634/1000..  Training Loss: 7.665..  Test Loss: 22.248.. \n",
      "Epoch: 635/1000..  Training Loss: 7.702..  Test Loss: 23.473.. \n",
      "Epoch: 636/1000..  Training Loss: 7.434..  Test Loss: 23.065.. \n",
      "Epoch: 637/1000..  Training Loss: 7.647..  Test Loss: 23.295.. \n",
      "Epoch: 638/1000..  Training Loss: 7.873..  Test Loss: 23.688.. \n",
      "Epoch: 639/1000..  Training Loss: 7.482..  Test Loss: 24.287.. \n",
      "Epoch: 640/1000..  Training Loss: 7.140..  Test Loss: 23.812.. \n",
      "Epoch: 642/1000..  Training Loss: 8.680..  Test Loss: 23.066.. \n",
      "Epoch: 643/1000..  Training Loss: 9.478..  Test Loss: 23.066.. \n",
      "Epoch: 644/1000..  Training Loss: 9.228..  Test Loss: 24.087.. \n",
      "Epoch: 645/1000..  Training Loss: 8.554..  Test Loss: 24.823.. \n",
      "Epoch: 646/1000..  Training Loss: 8.176..  Test Loss: 22.763.. \n",
      "Epoch: 647/1000..  Training Loss: 8.143..  Test Loss: 22.367.. \n",
      "Epoch: 648/1000..  Training Loss: 7.128..  Test Loss: 21.897.. \n",
      "Epoch: 649/1000..  Training Loss: 7.122..  Test Loss: 22.896.. \n",
      "Epoch: 650/1000..  Training Loss: 7.371..  Test Loss: 22.899.. \n",
      "Epoch: 652/1000..  Training Loss: 7.360..  Test Loss: 22.119.. \n",
      "Epoch: 653/1000..  Training Loss: 7.237..  Test Loss: 23.483.. \n",
      "Epoch: 654/1000..  Training Loss: 7.285..  Test Loss: 22.498.. \n",
      "Epoch: 655/1000..  Training Loss: 8.576..  Test Loss: 25.130.. \n",
      "Epoch: 656/1000..  Training Loss: 9.487..  Test Loss: 28.252.. \n",
      "Epoch: 657/1000..  Training Loss: 9.642..  Test Loss: 25.810.. \n",
      "Epoch: 658/1000..  Training Loss: 8.003..  Test Loss: 23.366.. \n",
      "Epoch: 659/1000..  Training Loss: 7.930..  Test Loss: 22.321.. \n",
      "Epoch: 660/1000..  Training Loss: 8.448..  Test Loss: 22.988.. \n",
      "Epoch: 662/1000..  Training Loss: 7.688..  Test Loss: 23.016.. \n",
      "Epoch: 663/1000..  Training Loss: 7.151..  Test Loss: 22.614.. \n",
      "Epoch: 664/1000..  Training Loss: 6.935..  Test Loss: 23.646.. \n",
      "Epoch: 665/1000..  Training Loss: 7.533..  Test Loss: 23.422.. \n",
      "Epoch: 666/1000..  Training Loss: 6.795..  Test Loss: 23.786.. \n",
      "Epoch: 667/1000..  Training Loss: 6.846..  Test Loss: 23.227.. \n",
      "Epoch: 668/1000..  Training Loss: 7.930..  Test Loss: 24.313.. \n",
      "Epoch: 669/1000..  Training Loss: 6.989..  Test Loss: 22.443.. \n",
      "Epoch: 670/1000..  Training Loss: 6.611..  Test Loss: 22.620.. \n",
      "Epoch: 672/1000..  Training Loss: 6.501..  Test Loss: 22.663.. \n",
      "Epoch: 673/1000..  Training Loss: 6.073..  Test Loss: 22.753.. \n",
      "Epoch: 674/1000..  Training Loss: 6.351..  Test Loss: 23.222.. \n",
      "Epoch: 675/1000..  Training Loss: 6.880..  Test Loss: 26.133.. \n",
      "Epoch: 676/1000..  Training Loss: 7.814..  Test Loss: 24.283.. \n",
      "Epoch: 677/1000..  Training Loss: 7.319..  Test Loss: 22.979.. \n",
      "Epoch: 678/1000..  Training Loss: 7.022..  Test Loss: 23.146.. \n",
      "Epoch: 679/1000..  Training Loss: 6.142..  Test Loss: 23.073.. \n",
      "Epoch: 680/1000..  Training Loss: 5.706..  Test Loss: 23.224.. \n",
      "Epoch: 682/1000..  Training Loss: 5.768..  Test Loss: 24.462.. \n",
      "Epoch: 683/1000..  Training Loss: 6.188..  Test Loss: 23.197.. \n",
      "Epoch: 684/1000..  Training Loss: 5.676..  Test Loss: 23.495.. \n",
      "Epoch: 685/1000..  Training Loss: 5.494..  Test Loss: 23.555.. \n",
      "Epoch: 686/1000..  Training Loss: 4.979..  Test Loss: 23.848.. \n",
      "Epoch: 687/1000..  Training Loss: 4.569..  Test Loss: 23.826.. \n",
      "Epoch: 688/1000..  Training Loss: 4.845..  Test Loss: 24.695.. \n",
      "Epoch: 689/1000..  Training Loss: 5.507..  Test Loss: 22.709.. \n",
      "Epoch: 690/1000..  Training Loss: 5.127..  Test Loss: 23.272.. \n",
      "Epoch: 692/1000..  Training Loss: 4.539..  Test Loss: 23.365.. \n",
      "Epoch: 693/1000..  Training Loss: 5.154..  Test Loss: 24.266.. \n",
      "Epoch: 694/1000..  Training Loss: 5.731..  Test Loss: 24.572.. \n",
      "Epoch: 695/1000..  Training Loss: 5.000..  Test Loss: 23.383.. \n",
      "Epoch: 696/1000..  Training Loss: 4.845..  Test Loss: 23.553.. \n",
      "Epoch: 697/1000..  Training Loss: 4.668..  Test Loss: 24.173.. \n",
      "Epoch: 698/1000..  Training Loss: 4.380..  Test Loss: 23.704.. \n",
      "Epoch: 699/1000..  Training Loss: 4.481..  Test Loss: 23.336.. \n",
      "Epoch: 700/1000..  Training Loss: 4.291..  Test Loss: 24.824.. \n",
      "Epoch: 702/1000..  Training Loss: 4.310..  Test Loss: 22.947.. \n",
      "Epoch: 703/1000..  Training Loss: 5.489..  Test Loss: 26.159.. \n",
      "Epoch: 704/1000..  Training Loss: 6.723..  Test Loss: 27.582.. \n",
      "Epoch: 705/1000..  Training Loss: 7.582..  Test Loss: 24.325.. \n",
      "Epoch: 706/1000..  Training Loss: 7.303..  Test Loss: 24.071.. \n",
      "Epoch: 707/1000..  Training Loss: 8.207..  Test Loss: 23.789.. \n",
      "Epoch: 708/1000..  Training Loss: 7.496..  Test Loss: 24.486.. \n",
      "Epoch: 709/1000..  Training Loss: 6.558..  Test Loss: 24.123.. \n",
      "Epoch: 710/1000..  Training Loss: 5.806..  Test Loss: 23.762.. \n",
      "Epoch: 712/1000..  Training Loss: 5.121..  Test Loss: 23.276.. \n",
      "Epoch: 713/1000..  Training Loss: 5.332..  Test Loss: 23.644.. \n",
      "Epoch: 714/1000..  Training Loss: 4.709..  Test Loss: 23.214.. \n",
      "Epoch: 715/1000..  Training Loss: 4.797..  Test Loss: 24.301.. \n",
      "Epoch: 716/1000..  Training Loss: 5.717..  Test Loss: 23.383.. \n",
      "Epoch: 717/1000..  Training Loss: 5.217..  Test Loss: 23.380.. \n",
      "Epoch: 718/1000..  Training Loss: 4.929..  Test Loss: 23.965.. \n",
      "Epoch: 719/1000..  Training Loss: 4.909..  Test Loss: 23.969.. \n",
      "Epoch: 720/1000..  Training Loss: 5.045..  Test Loss: 24.108.. \n",
      "Epoch: 722/1000..  Training Loss: 5.039..  Test Loss: 24.181.. \n",
      "Epoch: 723/1000..  Training Loss: 5.392..  Test Loss: 24.657.. \n",
      "Epoch: 724/1000..  Training Loss: 5.638..  Test Loss: 23.505.. \n",
      "Epoch: 725/1000..  Training Loss: 6.501..  Test Loss: 23.751.. \n",
      "Epoch: 726/1000..  Training Loss: 6.376..  Test Loss: 24.209.. \n",
      "Epoch: 727/1000..  Training Loss: 6.560..  Test Loss: 24.352.. \n",
      "Epoch: 728/1000..  Training Loss: 5.779..  Test Loss: 25.250.. \n",
      "Epoch: 729/1000..  Training Loss: 5.676..  Test Loss: 23.725.. \n",
      "Epoch: 730/1000..  Training Loss: 5.049..  Test Loss: 25.567.. \n",
      "Epoch: 732/1000..  Training Loss: 4.596..  Test Loss: 24.325.. \n",
      "Epoch: 733/1000..  Training Loss: 4.337..  Test Loss: 24.716.. \n",
      "Epoch: 734/1000..  Training Loss: 4.379..  Test Loss: 24.082.. \n",
      "Epoch: 735/1000..  Training Loss: 3.781..  Test Loss: 23.107.. \n",
      "Epoch: 736/1000..  Training Loss: 3.797..  Test Loss: 24.190.. \n",
      "Epoch: 737/1000..  Training Loss: 5.045..  Test Loss: 23.906.. \n",
      "Epoch: 738/1000..  Training Loss: 5.035..  Test Loss: 24.067.. \n",
      "Epoch: 739/1000..  Training Loss: 4.624..  Test Loss: 24.595.. \n",
      "Epoch: 740/1000..  Training Loss: 4.075..  Test Loss: 25.119.. \n",
      "Epoch: 742/1000..  Training Loss: 3.815..  Test Loss: 23.961.. \n",
      "Epoch: 743/1000..  Training Loss: 3.325..  Test Loss: 23.193.. \n",
      "Epoch: 744/1000..  Training Loss: 2.994..  Test Loss: 23.615.. \n",
      "Epoch: 745/1000..  Training Loss: 2.886..  Test Loss: 22.901.. \n",
      "Epoch: 746/1000..  Training Loss: 3.194..  Test Loss: 24.896.. \n",
      "Epoch: 747/1000..  Training Loss: 3.323..  Test Loss: 23.860.. \n",
      "Epoch: 748/1000..  Training Loss: 3.070..  Test Loss: 23.539.. \n",
      "Epoch: 749/1000..  Training Loss: 3.844..  Test Loss: 25.109.. \n",
      "Epoch: 750/1000..  Training Loss: 3.993..  Test Loss: 24.343.. \n",
      "Epoch: 752/1000..  Training Loss: 3.320..  Test Loss: 24.723.. \n",
      "Epoch: 753/1000..  Training Loss: 3.034..  Test Loss: 23.993.. \n",
      "Epoch: 754/1000..  Training Loss: 2.674..  Test Loss: 24.191.. \n",
      "Epoch: 755/1000..  Training Loss: 2.724..  Test Loss: 23.678.. \n",
      "Epoch: 756/1000..  Training Loss: 2.583..  Test Loss: 24.054.. \n",
      "Epoch: 757/1000..  Training Loss: 2.490..  Test Loss: 23.814.. \n",
      "Epoch: 758/1000..  Training Loss: 3.266..  Test Loss: 24.605.. \n",
      "Epoch: 759/1000..  Training Loss: 3.026..  Test Loss: 24.472.. \n",
      "Epoch: 760/1000..  Training Loss: 3.240..  Test Loss: 24.121.. \n",
      "Epoch: 762/1000..  Training Loss: 2.795..  Test Loss: 24.190.. \n",
      "Epoch: 763/1000..  Training Loss: 2.846..  Test Loss: 23.901.. \n",
      "Epoch: 764/1000..  Training Loss: 2.520..  Test Loss: 23.824.. \n",
      "Epoch: 765/1000..  Training Loss: 2.857..  Test Loss: 26.153.. \n",
      "Epoch: 766/1000..  Training Loss: 5.208..  Test Loss: 24.969.. \n",
      "Epoch: 767/1000..  Training Loss: 4.744..  Test Loss: 25.660.. \n",
      "Epoch: 768/1000..  Training Loss: 4.898..  Test Loss: 27.209.. \n",
      "Epoch: 769/1000..  Training Loss: 4.530..  Test Loss: 23.432.. \n",
      "Epoch: 770/1000..  Training Loss: 4.519..  Test Loss: 23.992.. \n",
      "Epoch: 772/1000..  Training Loss: 5.272..  Test Loss: 25.451.. \n",
      "Epoch: 773/1000..  Training Loss: 5.002..  Test Loss: 24.610.. \n",
      "Epoch: 774/1000..  Training Loss: 4.519..  Test Loss: 23.274.. \n",
      "Epoch: 775/1000..  Training Loss: 4.232..  Test Loss: 23.985.. \n",
      "Epoch: 776/1000..  Training Loss: 4.018..  Test Loss: 24.851.. \n",
      "Epoch: 777/1000..  Training Loss: 3.802..  Test Loss: 24.340.. \n",
      "Epoch: 778/1000..  Training Loss: 3.681..  Test Loss: 25.180.. \n",
      "Epoch: 779/1000..  Training Loss: 3.384..  Test Loss: 24.061.. \n",
      "Epoch: 780/1000..  Training Loss: 2.972..  Test Loss: 23.824.. \n",
      "Epoch: 782/1000..  Training Loss: 2.894..  Test Loss: 24.296.. \n",
      "Epoch: 783/1000..  Training Loss: 2.856..  Test Loss: 24.507.. \n",
      "Epoch: 784/1000..  Training Loss: 2.494..  Test Loss: 24.326.. \n",
      "Epoch: 785/1000..  Training Loss: 2.179..  Test Loss: 24.235.. \n",
      "Epoch: 786/1000..  Training Loss: 2.352..  Test Loss: 24.583.. \n",
      "Epoch: 787/1000..  Training Loss: 2.271..  Test Loss: 23.899.. \n",
      "Epoch: 788/1000..  Training Loss: 2.487..  Test Loss: 23.831.. \n",
      "Epoch: 789/1000..  Training Loss: 2.406..  Test Loss: 24.480.. \n",
      "Epoch: 790/1000..  Training Loss: 2.064..  Test Loss: 25.283.. \n",
      "Epoch: 792/1000..  Training Loss: 1.993..  Test Loss: 24.080.. \n",
      "Epoch: 793/1000..  Training Loss: 2.142..  Test Loss: 24.988.. \n",
      "Epoch: 794/1000..  Training Loss: 2.887..  Test Loss: 24.241.. \n",
      "Epoch: 795/1000..  Training Loss: 2.971..  Test Loss: 24.537.. \n",
      "Epoch: 796/1000..  Training Loss: 2.603..  Test Loss: 23.798.. \n",
      "Epoch: 797/1000..  Training Loss: 2.487..  Test Loss: 24.215.. \n",
      "Epoch: 798/1000..  Training Loss: 2.245..  Test Loss: 24.022.. \n",
      "Epoch: 799/1000..  Training Loss: 2.190..  Test Loss: 24.514.. \n",
      "Epoch: 800/1000..  Training Loss: 1.973..  Test Loss: 23.915.. \n",
      "Epoch: 802/1000..  Training Loss: 1.780..  Test Loss: 24.709.. \n",
      "Epoch: 803/1000..  Training Loss: 1.743..  Test Loss: 24.439.. \n",
      "Epoch: 804/1000..  Training Loss: 1.873..  Test Loss: 24.115.. \n",
      "Epoch: 805/1000..  Training Loss: 1.824..  Test Loss: 24.560.. \n",
      "Epoch: 806/1000..  Training Loss: 1.704..  Test Loss: 24.524.. \n",
      "Epoch: 807/1000..  Training Loss: 1.630..  Test Loss: 24.062.. \n",
      "Epoch: 808/1000..  Training Loss: 1.687..  Test Loss: 24.451.. \n",
      "Epoch: 809/1000..  Training Loss: 1.569..  Test Loss: 24.641.. \n",
      "Epoch: 810/1000..  Training Loss: 1.553..  Test Loss: 24.447.. \n",
      "Epoch: 812/1000..  Training Loss: 1.548..  Test Loss: 24.277.. \n",
      "Epoch: 813/1000..  Training Loss: 1.491..  Test Loss: 24.346.. \n",
      "Epoch: 814/1000..  Training Loss: 1.437..  Test Loss: 24.844.. \n",
      "Epoch: 815/1000..  Training Loss: 1.643..  Test Loss: 24.127.. \n",
      "Epoch: 816/1000..  Training Loss: 1.542..  Test Loss: 24.091.. \n",
      "Epoch: 817/1000..  Training Loss: 1.750..  Test Loss: 25.572.. \n",
      "Epoch: 818/1000..  Training Loss: 1.848..  Test Loss: 24.583.. \n",
      "Epoch: 819/1000..  Training Loss: 1.626..  Test Loss: 24.994.. \n",
      "Epoch: 820/1000..  Training Loss: 1.597..  Test Loss: 24.444.. \n",
      "Epoch: 822/1000..  Training Loss: 1.536..  Test Loss: 24.974.. \n",
      "Epoch: 823/1000..  Training Loss: 1.772..  Test Loss: 24.858.. \n",
      "Epoch: 824/1000..  Training Loss: 1.515..  Test Loss: 24.919.. \n",
      "Epoch: 825/1000..  Training Loss: 1.504..  Test Loss: 24.995.. \n",
      "Epoch: 826/1000..  Training Loss: 1.591..  Test Loss: 24.681.. \n",
      "Epoch: 827/1000..  Training Loss: 1.563..  Test Loss: 25.041.. \n",
      "Epoch: 828/1000..  Training Loss: 1.592..  Test Loss: 24.797.. \n",
      "Epoch: 829/1000..  Training Loss: 1.786..  Test Loss: 25.088.. \n",
      "Epoch: 830/1000..  Training Loss: 1.512..  Test Loss: 24.900.. \n",
      "Epoch: 832/1000..  Training Loss: 1.503..  Test Loss: 25.443.. \n",
      "Epoch: 833/1000..  Training Loss: 1.408..  Test Loss: 24.732.. \n",
      "Epoch: 834/1000..  Training Loss: 2.745..  Test Loss: 25.623.. \n",
      "Epoch: 835/1000..  Training Loss: 2.925..  Test Loss: 25.291.. \n",
      "Epoch: 836/1000..  Training Loss: 2.405..  Test Loss: 25.724.. \n",
      "Epoch: 837/1000..  Training Loss: 3.007..  Test Loss: 24.514.. \n",
      "Epoch: 838/1000..  Training Loss: 3.814..  Test Loss: 24.439.. \n",
      "Epoch: 839/1000..  Training Loss: 3.217..  Test Loss: 24.515.. \n",
      "Epoch: 840/1000..  Training Loss: 2.543..  Test Loss: 24.537.. \n",
      "Epoch: 842/1000..  Training Loss: 3.762..  Test Loss: 24.828.. \n",
      "Epoch: 843/1000..  Training Loss: 3.555..  Test Loss: 26.770.. \n",
      "Epoch: 844/1000..  Training Loss: 4.788..  Test Loss: 25.210.. \n",
      "Epoch: 845/1000..  Training Loss: 4.094..  Test Loss: 24.614.. \n",
      "Epoch: 846/1000..  Training Loss: 2.987..  Test Loss: 24.612.. \n",
      "Epoch: 847/1000..  Training Loss: 2.433..  Test Loss: 24.721.. \n",
      "Epoch: 848/1000..  Training Loss: 2.217..  Test Loss: 24.818.. \n",
      "Epoch: 849/1000..  Training Loss: 2.477..  Test Loss: 24.242.. \n",
      "Epoch: 850/1000..  Training Loss: 2.127..  Test Loss: 24.335.. \n",
      "Epoch: 852/1000..  Training Loss: 2.248..  Test Loss: 24.573.. \n",
      "Epoch: 853/1000..  Training Loss: 2.191..  Test Loss: 25.220.. \n",
      "Epoch: 854/1000..  Training Loss: 1.961..  Test Loss: 25.347.. \n",
      "Epoch: 855/1000..  Training Loss: 2.103..  Test Loss: 24.554.. \n",
      "Epoch: 856/1000..  Training Loss: 2.269..  Test Loss: 24.272.. \n",
      "Epoch: 857/1000..  Training Loss: 1.903..  Test Loss: 24.511.. \n",
      "Epoch: 858/1000..  Training Loss: 1.601..  Test Loss: 24.964.. \n",
      "Epoch: 859/1000..  Training Loss: 1.497..  Test Loss: 24.472.. \n",
      "Epoch: 860/1000..  Training Loss: 1.427..  Test Loss: 24.989.. \n",
      "Epoch: 862/1000..  Training Loss: 1.464..  Test Loss: 24.757.. \n",
      "Epoch: 863/1000..  Training Loss: 1.358..  Test Loss: 24.685.. \n",
      "Epoch: 864/1000..  Training Loss: 1.635..  Test Loss: 25.878.. \n",
      "Epoch: 865/1000..  Training Loss: 1.574..  Test Loss: 25.456.. \n",
      "Epoch: 866/1000..  Training Loss: 1.421..  Test Loss: 24.651.. \n",
      "Epoch: 867/1000..  Training Loss: 1.557..  Test Loss: 24.852.. \n",
      "Epoch: 868/1000..  Training Loss: 1.505..  Test Loss: 24.427.. \n",
      "Epoch: 869/1000..  Training Loss: 1.536..  Test Loss: 24.363.. \n",
      "Epoch: 870/1000..  Training Loss: 1.478..  Test Loss: 24.645.. \n",
      "Epoch: 872/1000..  Training Loss: 1.240..  Test Loss: 24.489.. \n",
      "Epoch: 873/1000..  Training Loss: 1.223..  Test Loss: 24.489.. \n",
      "Epoch: 874/1000..  Training Loss: 1.133..  Test Loss: 24.026.. \n",
      "Epoch: 875/1000..  Training Loss: 1.101..  Test Loss: 24.778.. \n",
      "Epoch: 876/1000..  Training Loss: 1.214..  Test Loss: 24.137.. \n",
      "Epoch: 877/1000..  Training Loss: 1.107..  Test Loss: 24.108.. \n",
      "Epoch: 878/1000..  Training Loss: 1.157..  Test Loss: 24.419.. \n",
      "Epoch: 879/1000..  Training Loss: 1.049..  Test Loss: 24.129.. \n",
      "Epoch: 880/1000..  Training Loss: 1.143..  Test Loss: 24.417.. \n",
      "Epoch: 882/1000..  Training Loss: 1.062..  Test Loss: 24.916.. \n",
      "Epoch: 883/1000..  Training Loss: 1.141..  Test Loss: 24.476.. \n",
      "Epoch: 884/1000..  Training Loss: 1.020..  Test Loss: 24.697.. \n",
      "Epoch: 885/1000..  Training Loss: 1.009..  Test Loss: 24.196.. \n",
      "Epoch: 886/1000..  Training Loss: 1.038..  Test Loss: 24.104.. \n",
      "Epoch: 887/1000..  Training Loss: 1.249..  Test Loss: 23.988.. \n",
      "Epoch: 888/1000..  Training Loss: 1.376..  Test Loss: 24.774.. \n",
      "Epoch: 889/1000..  Training Loss: 1.296..  Test Loss: 25.358.. \n",
      "Epoch: 890/1000..  Training Loss: 1.214..  Test Loss: 24.337.. \n",
      "Epoch: 892/1000..  Training Loss: 1.031..  Test Loss: 24.947.. \n",
      "Epoch: 893/1000..  Training Loss: 1.277..  Test Loss: 25.114.. \n",
      "Epoch: 894/1000..  Training Loss: 1.174..  Test Loss: 24.694.. \n",
      "Epoch: 895/1000..  Training Loss: 1.140..  Test Loss: 24.749.. \n",
      "Epoch: 896/1000..  Training Loss: 1.133..  Test Loss: 24.880.. \n",
      "Epoch: 897/1000..  Training Loss: 1.003..  Test Loss: 24.511.. \n",
      "Epoch: 898/1000..  Training Loss: 1.070..  Test Loss: 24.515.. \n",
      "Epoch: 899/1000..  Training Loss: 0.946..  Test Loss: 24.488.. \n",
      "Epoch: 900/1000..  Training Loss: 1.114..  Test Loss: 25.141.. \n",
      "Epoch: 902/1000..  Training Loss: 1.070..  Test Loss: 24.959.. \n",
      "Epoch: 903/1000..  Training Loss: 1.130..  Test Loss: 24.670.. \n",
      "Epoch: 904/1000..  Training Loss: 1.224..  Test Loss: 24.669.. \n",
      "Epoch: 905/1000..  Training Loss: 1.250..  Test Loss: 25.753.. \n",
      "Epoch: 906/1000..  Training Loss: 1.424..  Test Loss: 24.691.. \n",
      "Epoch: 907/1000..  Training Loss: 1.209..  Test Loss: 24.498.. \n",
      "Epoch: 908/1000..  Training Loss: 1.215..  Test Loss: 24.514.. \n",
      "Epoch: 909/1000..  Training Loss: 1.641..  Test Loss: 25.127.. \n",
      "Epoch: 910/1000..  Training Loss: 1.732..  Test Loss: 25.268.. \n",
      "Epoch: 912/1000..  Training Loss: 1.741..  Test Loss: 24.723.. \n",
      "Epoch: 913/1000..  Training Loss: 1.423..  Test Loss: 24.397.. \n",
      "Epoch: 914/1000..  Training Loss: 1.168..  Test Loss: 24.353.. \n",
      "Epoch: 915/1000..  Training Loss: 0.971..  Test Loss: 24.370.. \n",
      "Epoch: 916/1000..  Training Loss: 0.967..  Test Loss: 24.656.. \n",
      "Epoch: 917/1000..  Training Loss: 1.030..  Test Loss: 24.402.. \n",
      "Epoch: 918/1000..  Training Loss: 0.996..  Test Loss: 24.585.. \n",
      "Epoch: 919/1000..  Training Loss: 0.920..  Test Loss: 24.231.. \n",
      "Epoch: 920/1000..  Training Loss: 1.037..  Test Loss: 24.568.. \n",
      "Epoch: 922/1000..  Training Loss: 1.172..  Test Loss: 25.057.. \n",
      "Epoch: 923/1000..  Training Loss: 1.149..  Test Loss: 24.697.. \n",
      "Epoch: 924/1000..  Training Loss: 1.123..  Test Loss: 24.855.. \n",
      "Epoch: 925/1000..  Training Loss: 0.904..  Test Loss: 24.589.. \n",
      "Epoch: 926/1000..  Training Loss: 1.015..  Test Loss: 24.817.. \n",
      "Epoch: 927/1000..  Training Loss: 0.776..  Test Loss: 24.592.. \n",
      "Epoch: 928/1000..  Training Loss: 0.806..  Test Loss: 24.740.. \n",
      "Epoch: 929/1000..  Training Loss: 0.797..  Test Loss: 24.602.. \n",
      "Epoch: 930/1000..  Training Loss: 2.378..  Test Loss: 24.872.. \n",
      "Epoch: 932/1000..  Training Loss: 3.839..  Test Loss: 24.188.. \n",
      "Epoch: 933/1000..  Training Loss: 2.593..  Test Loss: 23.262.. \n",
      "Epoch: 934/1000..  Training Loss: 1.894..  Test Loss: 24.830.. \n",
      "Epoch: 935/1000..  Training Loss: 1.698..  Test Loss: 24.230.. \n",
      "Epoch: 936/1000..  Training Loss: 1.395..  Test Loss: 24.430.. \n",
      "Epoch: 937/1000..  Training Loss: 1.763..  Test Loss: 24.451.. \n",
      "Epoch: 938/1000..  Training Loss: 1.786..  Test Loss: 24.218.. \n",
      "Epoch: 939/1000..  Training Loss: 2.052..  Test Loss: 31.790.. \n",
      "Epoch: 940/1000..  Training Loss: 4.777..  Test Loss: 30.191.. \n",
      "Epoch: 942/1000..  Training Loss: 4.597..  Test Loss: 25.511.. \n",
      "Epoch: 943/1000..  Training Loss: 3.279..  Test Loss: 24.821.. \n",
      "Epoch: 944/1000..  Training Loss: 2.505..  Test Loss: 23.988.. \n",
      "Epoch: 945/1000..  Training Loss: 2.171..  Test Loss: 23.720.. \n",
      "Epoch: 946/1000..  Training Loss: 1.777..  Test Loss: 24.086.. \n",
      "Epoch: 947/1000..  Training Loss: 1.515..  Test Loss: 23.683.. \n",
      "Epoch: 948/1000..  Training Loss: 1.170..  Test Loss: 23.937.. \n",
      "Epoch: 949/1000..  Training Loss: 1.102..  Test Loss: 23.867.. \n",
      "Epoch: 950/1000..  Training Loss: 1.014..  Test Loss: 23.545.. \n",
      "Epoch: 952/1000..  Training Loss: 0.986..  Test Loss: 23.950.. \n",
      "Epoch: 953/1000..  Training Loss: 0.958..  Test Loss: 23.972.. \n",
      "Epoch: 954/1000..  Training Loss: 1.274..  Test Loss: 25.486.. \n",
      "Epoch: 955/1000..  Training Loss: 2.662..  Test Loss: 24.395.. \n",
      "Epoch: 956/1000..  Training Loss: 2.361..  Test Loss: 22.280.. \n",
      "Epoch: 957/1000..  Training Loss: 2.451..  Test Loss: 25.106.. \n",
      "Epoch: 958/1000..  Training Loss: 2.660..  Test Loss: 26.016.. \n",
      "Epoch: 959/1000..  Training Loss: 2.250..  Test Loss: 23.647.. \n",
      "Epoch: 960/1000..  Training Loss: 1.631..  Test Loss: 24.646.. \n",
      "Epoch: 962/1000..  Training Loss: 1.410..  Test Loss: 24.493.. \n",
      "Epoch: 963/1000..  Training Loss: 1.194..  Test Loss: 23.968.. \n",
      "Epoch: 964/1000..  Training Loss: 1.082..  Test Loss: 24.073.. \n",
      "Epoch: 965/1000..  Training Loss: 0.992..  Test Loss: 24.250.. \n",
      "Epoch: 966/1000..  Training Loss: 0.853..  Test Loss: 24.273.. \n",
      "Epoch: 967/1000..  Training Loss: 0.727..  Test Loss: 23.938.. \n",
      "Epoch: 968/1000..  Training Loss: 1.040..  Test Loss: 24.037.. \n",
      "Epoch: 969/1000..  Training Loss: 0.902..  Test Loss: 24.138.. \n",
      "Epoch: 970/1000..  Training Loss: 0.814..  Test Loss: 24.407.. \n",
      "Epoch: 972/1000..  Training Loss: 0.890..  Test Loss: 24.210.. \n",
      "Epoch: 973/1000..  Training Loss: 0.708..  Test Loss: 24.100.. \n",
      "Epoch: 974/1000..  Training Loss: 0.723..  Test Loss: 24.645.. \n",
      "Epoch: 975/1000..  Training Loss: 0.622..  Test Loss: 24.277.. \n",
      "Epoch: 976/1000..  Training Loss: 0.695..  Test Loss: 24.126.. \n",
      "Epoch: 977/1000..  Training Loss: 0.721..  Test Loss: 24.247.. \n",
      "Epoch: 978/1000..  Training Loss: 0.701..  Test Loss: 24.408.. \n",
      "Epoch: 979/1000..  Training Loss: 0.579..  Test Loss: 24.432.. \n",
      "Epoch: 980/1000..  Training Loss: 0.697..  Test Loss: 24.651.. \n",
      "Epoch: 982/1000..  Training Loss: 0.839..  Test Loss: 24.711.. \n",
      "Epoch: 983/1000..  Training Loss: 0.853..  Test Loss: 24.114.. \n",
      "Epoch: 984/1000..  Training Loss: 0.873..  Test Loss: 24.844.. \n",
      "Epoch: 985/1000..  Training Loss: 0.917..  Test Loss: 24.367.. \n",
      "Epoch: 986/1000..  Training Loss: 0.828..  Test Loss: 24.533.. \n",
      "Epoch: 987/1000..  Training Loss: 0.810..  Test Loss: 24.444.. \n",
      "Epoch: 988/1000..  Training Loss: 0.909..  Test Loss: 24.617.. \n",
      "Epoch: 989/1000..  Training Loss: 0.747..  Test Loss: 24.310.. \n",
      "Epoch: 990/1000..  Training Loss: 0.658..  Test Loss: 24.441.. \n",
      "Epoch: 992/1000..  Training Loss: 0.595..  Test Loss: 24.785.. \n",
      "Epoch: 993/1000..  Training Loss: 0.789..  Test Loss: 24.285.. \n",
      "Epoch: 994/1000..  Training Loss: 1.239..  Test Loss: 24.578.. \n",
      "Epoch: 995/1000..  Training Loss: 1.218..  Test Loss: 24.743.. \n",
      "Epoch: 996/1000..  Training Loss: 1.921..  Test Loss: 24.230.. \n",
      "Epoch: 997/1000..  Training Loss: 1.826..  Test Loss: 25.489.. \n",
      "Epoch: 998/1000..  Training Loss: 1.437..  Test Loss: 24.353.. \n",
      "Epoch: 999/1000..  Training Loss: 1.252..  Test Loss: 25.160.. \n",
      "Epoch: 1000/1000..  Training Loss: 1.042..  Test Loss: 24.905.. \n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = Network()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "train(\n",
    "    model=model, \n",
    "    trainloader=train_set, \n",
    "    testloader=test_set, \n",
    "    criterion=model.criterion, \n",
    "    optimizer=optimizer, \n",
    "    epochs=1000, \n",
    "    print_every=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 5.6839, -8.4675, -6.5452,  ...,  4.6852, -3.2719, -3.1595],\n",
      "        [-5.3567, -0.5322, -4.8961,  ..., -3.0446,  5.8485, -5.7947],\n",
      "        [-4.7836,  5.0825,  5.5676,  ..., -1.0429, -2.1280, -1.4842],\n",
      "        ...,\n",
      "        [-0.3442,  4.1249,  3.0249,  ...,  2.6847, -3.2513, -0.5949],\n",
      "        [-7.0210,  3.4221, -9.0158,  ..., -7.6089, -5.1991, -8.2352],\n",
      "        [ 8.1332, -2.9440, -5.1433,  ...,  6.6750, -4.3378,  3.3127]],\n",
      "       device='cuda:0', grad_fn=<LeakyReluBackward0>)\n",
      "tensor([[  5, -10,  -4,  ...,   7,  -9,  -7],\n",
      "        [ -5,  -2,  -6,  ...,  -4,   8,  -9],\n",
      "        [ -9,   9,   0,  ...,  -8,   6, -10],\n",
      "        ...,\n",
      "        [  2,   4,   3,  ...,   9, -10,  -1],\n",
      "        [-10,   0,  -5,  ...,   4,  -8, -10],\n",
      "        [  8,  -1,  -5,  ...,   8,  -4,   3]], device='cuda:0',\n",
      "       dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "test_batch = next(iter(test_set))\n",
    "img, lbls = test_batch\n",
    "img, lbls = img.to(device), lbls.to(device)\n",
    "# print(img.shape, lbls.shape)\n",
    "\n",
    "output = model.forward(img)\n",
    "print(output)\n",
    "print(lbls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the network\n",
    "model.save('C:\\\\Users\\\\the_3\\\\Desktop\\\\poly-curve-detector\\\\Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network(\n",
       "  (cnn_layers): Sequential(\n",
       "    (0): Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): LeakyReLU(negative_slope=0.01)\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (4): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): LeakyReLU(negative_slope=0.01)\n",
       "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (8): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (9): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): LeakyReLU(negative_slope=0.01)\n",
       "    (11): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (12): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (13): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (14): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (15): LeakyReLU(negative_slope=0.01)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (fc_layers): Sequential(\n",
       "    (0): Linear(in_features=1568, out_features=4096, bias=True)\n",
       "    (1): LeakyReLU(negative_slope=0.01)\n",
       "    (2): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (3): LeakyReLU(negative_slope=0.01)\n",
       "    (4): Linear(in_features=4096, out_features=128, bias=True)\n",
       "    (5): LeakyReLU(negative_slope=0.01)\n",
       "    (6): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (7): LeakyReLU(negative_slope=0.01)\n",
       "    (8): Linear(in_features=64, out_features=8, bias=True)\n",
       "    (9): LeakyReLU(negative_slope=0.01)\n",
       "  )\n",
       "  (criterion): MSELoss()\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# laod network from save\n",
    "new_model = Network()\n",
    "Network.load_checkpoint(new_model, \"C:\\\\Users\\\\the_3\\\\Desktop\\\\poly-curve-detector\\\\Model\\\\model_checkpoint.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 5.6839, -8.4675, -6.5452,  ...,  4.6852, -3.2719, -3.1595],\n",
      "        [-5.3567, -0.5322, -4.8961,  ..., -3.0446,  5.8485, -5.7947],\n",
      "        [-4.7836,  5.0824,  5.5676,  ..., -1.0429, -2.1280, -1.4842],\n",
      "        ...,\n",
      "        [-0.3442,  4.1249,  3.0249,  ...,  2.6847, -3.2513, -0.5949],\n",
      "        [-7.0210,  3.4221, -9.0158,  ..., -7.6089, -5.1991, -8.2352],\n",
      "        [ 8.1332, -2.9440, -5.1433,  ...,  6.6750, -4.3378,  3.3127]],\n",
      "       grad_fn=<LeakyReluBackward0>)\n",
      "tensor([[  5, -10,  -4,  ...,   7,  -9,  -7],\n",
      "        [ -5,  -2,  -6,  ...,  -4,   8,  -9],\n",
      "        [ -9,   9,   0,  ...,  -8,   6, -10],\n",
      "        ...,\n",
      "        [  2,   4,   3,  ...,   9, -10,  -1],\n",
      "        [-10,   0,  -5,  ...,   4,  -8, -10],\n",
      "        [  8,  -1,  -5,  ...,   8,  -4,   3]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "# loaded network from save\n",
    "test_batch = next(iter(test_set))\n",
    "img, lbls = test_batch\n",
    "# img, lbls = img.to(device), lbls.to(device)\n",
    "# print(img.shape, lbls.shape)\n",
    "\n",
    "output = new_model.forward(img)\n",
    "print(output)\n",
    "print(lbls)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3a1829a56db40c3ca63cc5d173ccaf89ee3791672440d362287656aaeb413643"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit (conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
