{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to develop the model for the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All our imports\n",
    "import torch \n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from random import randint\n",
    "import PIL\n",
    "import os\n",
    "\n",
    "#for all the plots to be inline\n",
    "%matplotlib inline \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, csv_path, images_folder, transform):\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.images_folder = images_folder\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        filename = self.df.values[index][0]\n",
    "        labels = np.array([1, 1, 1, 1, 1, 1, 1, 1])\n",
    "        for x in range(0, 8):\n",
    "            labels[x] = self.df.values[index][x+1]\n",
    "        image = PIL.Image.open(os.path.join(self.images_folder, filename))\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        return image, labels\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Data Set manipulation'''\n",
    "transform = transforms.Compose([transforms.Grayscale(num_output_channels=1),\n",
    "                                transforms.ToTensor()])\n",
    "\n",
    "root = \"C:\\\\Users\\\\the_3\\\\Desktop\"\n",
    "trainDataset = CustomDataset(root + \"\\\\poly-curve-detector\\\\data\\\\plotData\\\\labels\\\\trainPlots.csv\",\n",
    "                               root + \"\\\\poly-curve-detector\\\\data\\\\plotData\\\\trainPlots\", transform)\n",
    "\n",
    "testDataset = CustomDataset(root + \"\\\\poly-curve-detector\\\\data\\\\plotData\\\\labels\\\\testPlots.csv\",\n",
    "                               root + \"\\\\poly-curve-detector\\\\data\\\\plotData\\\\testPlots\", transform)\n",
    "\n",
    "#print first label in each dataset\n",
    "#labels in order [a1,a2,a3,a4,a5,a6,a7,a8]\n",
    "# image, labels = trainDataset[0]\n",
    "# print(labels[0:9])\n",
    "# image, labels = testDataset[0]\n",
    "# print(labels[0:9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = torch.utils.data.DataLoader(trainDataset, shuffle=True, batch_size=128)\n",
    "test_set = torch.utils.data.DataLoader(testDataset, shuffle=False, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 1, 96, 96]) torch.Size([128, 8])\n",
      "Train Set 0\n",
      "Image: tensor([[[0.6745, 0.8235, 0.8235,  ..., 0.8235, 0.8235, 0.6745],\n",
      "         [0.8235, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 0.8235],\n",
      "         [0.8235, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 0.8235],\n",
      "         ...,\n",
      "         [0.8235, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 0.8235],\n",
      "         [0.8235, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 0.8235],\n",
      "         [0.6745, 0.8235, 0.8235,  ..., 0.8235, 0.8235, 0.6745]]])\n",
      "Labels: tensor([ 2,  8,  9,  5,  8,  4, 10, -3], dtype=torch.int32)\n",
      "\n",
      "torch.Size([1, 96, 96]) torch.Size([8])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 1, 96, 96]) torch.Size([128, 8])\n",
      "Test Set 0\n",
      "Image: tensor([[[0.6745, 0.8235, 0.8235,  ..., 0.8235, 0.8235, 0.6745],\n",
      "         [0.8235, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 0.8235],\n",
      "         [0.8235, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 0.8235],\n",
      "         ...,\n",
      "         [0.8235, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 0.8235],\n",
      "         [0.8235, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 0.8235],\n",
      "         [0.6745, 0.8235, 0.8235,  ..., 0.8235, 0.8235, 0.6745]]])\n",
      "Labels: tensor([  5, -10,  -4,   3,  -7,   7,  -9,  -7], dtype=torch.int32)\n",
      "\n",
      "torch.Size([1, 96, 96]) torch.Size([8])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#one way to see batch size\n",
    "train_batch = next(iter(train_set))\n",
    "img, lbls = train_batch\n",
    "print(img.shape, lbls.shape)\n",
    "\n",
    "#display the first image in train_set\n",
    "#note: image changes each time run because shuffle is set to true\n",
    "for images, labels in train_set:\n",
    "    image, label = images[0], labels[0]\n",
    "    print(f\"Train Set 0\\nImage: {image}\\nLabels: {label}\\n\")\n",
    "    print(image.shape, label.shape)\n",
    "    figure = plt.figure(figsize=(4,4))\n",
    "    figure.add_subplot()\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(image.permute(1,2,0), cmap=\"gray\")\n",
    "    plt.show()\n",
    "    break\n",
    "    \n",
    "#test batch size\n",
    "test_batch = next(iter(test_set))\n",
    "img, lbls = test_batch\n",
    "print(img.shape, lbls.shape)\n",
    "\n",
    "#display the first image in test_set\n",
    "for images, labels in test_set:\n",
    "    image, label = images[0], labels[0]\n",
    "    print(f\"Test Set 0\\nImage: {image}\\nLabels: {label}\\n\")\n",
    "    print(image.shape, label.shape)\n",
    "    figure = plt.figure(figsize=(4,4))\n",
    "    figure.add_subplot()\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(image.permute(1,2,0), cmap=\"gray\")\n",
    "    plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "        # more analysis required to determine the specifics of the architecture\n",
    "       \n",
    "        self.n_output = 8\n",
    "        self.n_channel = 1\n",
    "    \n",
    "        self.cnn_layers = nn.Sequential(\n",
    "                nn.Conv2d(1, 8, 3), # convolution2dLayer(3,8,'Padding','same')\n",
    "                nn.BatchNorm2d(8),   # batchNormalizationLayer\n",
    "                nn.LeakyReLU(), # reluLayer\n",
    "                nn.MaxPool2d(2, 2), # averagePooling2dLayer(2,'Stride',2)\n",
    "                nn.Conv2d(8, 16, 3), # convolution2dLayer(3,16,'Padding','same')\n",
    "                nn.BatchNorm2d(16), # batchNormalizationLayer\n",
    "                nn.LeakyReLU(), # reluLayer\n",
    "                nn.MaxPool2d(2, 2), # averagePooling2dLayer(2,'Stride',2)\n",
    "                nn.Conv2d(16, 32, 3), # convolution2dLayer(3,32,'Padding','same')\n",
    "                nn.BatchNorm2d(32), # batchNormalizationLayer\n",
    "                nn.LeakyReLU(), # reluLayer\n",
    "                nn.Conv2d(32, 32, 3), # convolution2dLayer(3,32,'Padding','same')\n",
    "                nn.Conv2d(32, 32, 3), # convolution2dLayer(3,32,'Padding','same')\n",
    "                nn.Conv2d(32, 32, 3), # convolution2dLayer(3,32,'Padding','same')\n",
    "                nn.BatchNorm2d(32), # batchNormalizationLayer\n",
    "                nn.LeakyReLU(), # reluLayer\n",
    "                nn.MaxPool2d(2, 2) # Max pooling layer\n",
    "        )\n",
    "\n",
    "        self.n_input = 1568 # the output of maxpool 96*96 \n",
    "        #TODO:actual value might be determined from the computed output of the cnn layers\n",
    "        # https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n",
    "\n",
    "        self.fc_layers = nn.Sequential(\n",
    "#         fullyConnectedLayer(1)\n",
    "                nn.Linear(self.n_input,  4096),\n",
    "                nn.LeakyReLU(),\n",
    "                nn.Linear(4096, 4096),\n",
    "                nn.LeakyReLU(),\n",
    "                nn.Linear(4096, 128),\n",
    "                nn.LeakyReLU(),\n",
    "                nn.Linear(128, 64),\n",
    "                nn.LeakyReLU(),\n",
    "                nn.Linear(64, self.n_output),\n",
    "                nn.LeakyReLU()\n",
    "        )\n",
    "\n",
    "\n",
    "#         regressionLayer\n",
    "        self.criterion = nn.MSELoss()       \n",
    "#         dropoutLayer(0.2)\n",
    "        self.dropout =  nn.Dropout(p=0.2)\n",
    "        \n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #feedword pass through our network\n",
    "        x = self.cnn_layers(x)\n",
    "        x = x.view(x.shape[0], -1) #flatten the input tensor\n",
    "        x = self.fc_layers(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def load_checkpoint(new_model, filepath):\n",
    "        checkpoint = torch.load(filepath)\n",
    "        # model = Network(\n",
    "        #         checkpoint['input_size'], \n",
    "        #         checkpoint['output_size'],\n",
    "        #         # checkpoint['cnn_layers'],\n",
    "        #         # checkpoint['fc_layers']\n",
    "        # )\n",
    "        new_model.load_state_dict(checkpoint['state_dict'])\n",
    "        return new_model\n",
    "    \n",
    "    def save(self, dirpath):\n",
    "        self.checkpoint = {\n",
    "        #     'input_size': self.n_input, \n",
    "        #     'output_size': self.n_output,\n",
    "        #     'cnn_layers': [each. for each in model.cnn_layers],\n",
    "        #     'fc_layers': [each.out_features for each in model.fc_layers],\n",
    "            'state_dict': model.state_dict()\n",
    "        }\n",
    "        torch.save(self.checkpoint, f'{dirpath}\\\\model_checkpoint.pth')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "def validation(model, testloader, criterion):\n",
    "    accuracy = 0\n",
    "    test_loss = 0\n",
    "    for images, labels in testloader:\n",
    "        # if(labels.shape != torch.Size([128, 1, 96, 96])): continue\n",
    "        # images = images.view(images.shape[0], -1)\n",
    "        labels = labels.float()\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        output = model.forward(images)\n",
    "\n",
    "        # print(f'output={output.shape}')\n",
    "        # print(f'label={labels.shape}')\n",
    "        test_loss += criterion(output, labels)\n",
    "\n",
    "        # Calculate accuracy\n",
    "        # ps = torch.exp(output)\n",
    "        # top_p, top_class = ps.topk(1, dim=1)\n",
    "        # equals = top_class == labels.view(*top_class.shape)\n",
    "        # accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "        # accuracy += 1 - test_loss\n",
    "        ## Calculating the accuracy \n",
    "        # Model's output is log-softmax, take exponential to get the probabilities\n",
    "        # ps = torch.exp(output)\n",
    "        # Class with highest probability is our predicted class, compare with true label\n",
    "        # equality = (labels.data == ps.max(1)[1])\n",
    "        # Accuracy is number of correct predictions divided by all predictions, just take the mean\n",
    "        # accuracy += equality.type_as(torch.FloatTensor()).mean()\n",
    "        # accuracy = r2_score(labels, output)   # r2_score is the scikit learn r2 score function.\n",
    "        # print(\"accuracy = \", accuracy)   # here i get wierd values and it doesn't get better over time, in contrast the loss decreased over time\n",
    "\n",
    "    return test_loss, accuracy\n",
    "\n",
    "\n",
    "def train(model, trainloader, testloader, criterion, optimizer, epochs=5, print_every=40):\n",
    "\n",
    "    steps = 0\n",
    "    running_loss = 0\n",
    "    test_losses = []\n",
    "    train_losses = []\n",
    "    for e in range(epochs):\n",
    "        # Model in training mode, dropout is on\n",
    "        model.train()\n",
    "        for images, labels in trainloader:\n",
    "            steps += 1\n",
    "            # if(labels.shape != torch.Size([128, 1, 96, 96])): continue\n",
    "            # Flatten images into a 784 long vector\n",
    "            # images = images.view(images.shape[0], -1)\n",
    "            labels = labels.float()\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            output = model.forward(images)\n",
    "            # print(f'output={output.shape}')\n",
    "            # print(f'label={labels.shape}')\n",
    "            loss = criterion(output, labels)\n",
    "            loss.backward() # computes gradient and backpropagation\n",
    "            optimizer.step() # update of weights and biases happenss\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if steps % print_every == 0:\n",
    "                # Model in inference mode, dropout is off\n",
    "                model.eval()\n",
    "                \n",
    "                # Turn off gradients for validation, will speed up inference\n",
    "                with torch.no_grad():\n",
    "                    test_loss, accuracy = validation(model, testloader, criterion)\n",
    "                \n",
    "                train_losses.append(running_loss/len(trainloader))\n",
    "                test_losses.append(test_loss/len(testloader))\n",
    "\n",
    "                print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
    "                      \"Training Loss: {:.3f}.. \".format(running_loss/print_every),\n",
    "                      \"Test Loss: {:.3f}.. \".format(test_loss/len(testloader)),\n",
    "                    #   \"Test Accuracy: {:.3f}\".format(accuracy/len(testloader))\n",
    "                      )\n",
    "                \n",
    "                running_loss = 0\n",
    "                \n",
    "                # Make sure dropout and grads are on for training\n",
    "                model.train()\n",
    "    plt.xlim([0, 100])          \n",
    "    plt.plot(train_losses, label='Training loss')\n",
    "    plt.plot(test_losses, label='Validation loss')\n",
    "    plt.legend(frameon=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Network()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "# output = model.forward(img)\n",
    "# print(output)\n",
    "# print(lbls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(output.shape)\n",
    "# output[0,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DONE A: Train model on the data set\n",
    "    # DONE 1: dataloader be able load (image: tensor, label:string) example of the label tensor([10,-4,-5,3,-2,-3,-2,2]) \n",
    "    # DONE 2: fix dimensionality so the net can feedforward a batch of images correctly, move the training to gpu when available\n",
    "    # DONE 3: make trainloader and testloader dataloaders\n",
    "# DONE B: Improve model\n",
    "    # DONE 4: update checkpoint structure to save model and load model\n",
    "    # DONE 5: use better graphics to display the results (Josias)\n",
    "    # DONE 6: Find out if cnn layers learn (Josias)\n",
    "    # DONE 7: Fix the testloader issue where 8 size expected but found 96 (Brandon)\n",
    "    # DONE 8: Build a list of ideas to improve the nework (Brandon and Josias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List of ideas to improve the network\n",
    "\n",
    "1. ## Add a grid with finer blocks to the input as transform\n",
    "2. ## Revise the architecture\n",
    "3. ## Add dropout to the network\n",
    "4. ## Find a way to measure accuracy of multiple regression network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/500..  Training Loss: 99.593..  Test Loss: 37.148.. \n",
      "Epoch: 3/500..  Training Loss: 51.348..  Test Loss: 36.784.. \n",
      "Epoch: 4/500..  Training Loss: 47.728..  Test Loss: 37.669.. \n",
      "Epoch: 5/500..  Training Loss: 39.301..  Test Loss: 37.156.. \n",
      "Epoch: 6/500..  Training Loss: 38.669..  Test Loss: 36.849.. \n",
      "Epoch: 7/500..  Training Loss: 36.515..  Test Loss: 36.874.. \n",
      "Epoch: 8/500..  Training Loss: 36.176..  Test Loss: 36.285.. \n",
      "Epoch: 9/500..  Training Loss: 35.536..  Test Loss: 36.251.. \n",
      "Epoch: 10/500..  Training Loss: 35.253..  Test Loss: 41.153.. \n",
      "Epoch: 12/500..  Training Loss: 35.681..  Test Loss: 36.596.. \n",
      "Epoch: 13/500..  Training Loss: 35.449..  Test Loss: 35.557.. \n",
      "Epoch: 14/500..  Training Loss: 34.842..  Test Loss: 35.406.. \n",
      "Epoch: 15/500..  Training Loss: 34.653..  Test Loss: 35.209.. \n",
      "Epoch: 16/500..  Training Loss: 34.755..  Test Loss: 35.730.. \n",
      "Epoch: 17/500..  Training Loss: 34.424..  Test Loss: 34.777.. \n",
      "Epoch: 18/500..  Training Loss: 34.832..  Test Loss: 34.609.. \n",
      "Epoch: 19/500..  Training Loss: 33.698..  Test Loss: 35.401.. \n",
      "Epoch: 20/500..  Training Loss: 33.837..  Test Loss: 36.223.. \n",
      "Epoch: 22/500..  Training Loss: 33.204..  Test Loss: 37.601.. \n",
      "Epoch: 23/500..  Training Loss: 32.868..  Test Loss: 50.603.. \n",
      "Epoch: 24/500..  Training Loss: 33.033..  Test Loss: 35.287.. \n",
      "Epoch: 25/500..  Training Loss: 32.755..  Test Loss: 34.144.. \n",
      "Epoch: 26/500..  Training Loss: 32.685..  Test Loss: 33.428.. \n",
      "Epoch: 27/500..  Training Loss: 32.101..  Test Loss: 33.886.. \n",
      "Epoch: 28/500..  Training Loss: 32.491..  Test Loss: 33.191.. \n",
      "Epoch: 29/500..  Training Loss: 31.869..  Test Loss: 33.259.. \n",
      "Epoch: 30/500..  Training Loss: 32.026..  Test Loss: 33.008.. \n",
      "Epoch: 32/500..  Training Loss: 32.192..  Test Loss: 32.984.. \n",
      "Epoch: 33/500..  Training Loss: 31.821..  Test Loss: 40.044.. \n",
      "Epoch: 34/500..  Training Loss: 31.677..  Test Loss: 32.136.. \n",
      "Epoch: 35/500..  Training Loss: 31.347..  Test Loss: 43.371.. \n",
      "Epoch: 36/500..  Training Loss: 31.555..  Test Loss: 41.421.. \n",
      "Epoch: 37/500..  Training Loss: 32.738..  Test Loss: 57.098.. \n",
      "Epoch: 38/500..  Training Loss: 31.831..  Test Loss: 34.106.. \n",
      "Epoch: 39/500..  Training Loss: 31.545..  Test Loss: 33.986.. \n",
      "Epoch: 40/500..  Training Loss: 31.218..  Test Loss: 32.546.. \n",
      "Epoch: 42/500..  Training Loss: 30.583..  Test Loss: 31.648.. \n",
      "Epoch: 43/500..  Training Loss: 30.020..  Test Loss: 32.153.. \n",
      "Epoch: 44/500..  Training Loss: 30.374..  Test Loss: 30.346.. \n",
      "Epoch: 45/500..  Training Loss: 30.605..  Test Loss: 33.998.. \n",
      "Epoch: 46/500..  Training Loss: 32.139..  Test Loss: 36.992.. \n",
      "Epoch: 47/500..  Training Loss: 30.805..  Test Loss: 35.259.. \n",
      "Epoch: 48/500..  Training Loss: 30.383..  Test Loss: 30.520.. \n",
      "Epoch: 49/500..  Training Loss: 30.169..  Test Loss: 30.874.. \n",
      "Epoch: 50/500..  Training Loss: 29.927..  Test Loss: 31.297.. \n",
      "Epoch: 52/500..  Training Loss: 29.324..  Test Loss: 34.012.. \n",
      "Epoch: 53/500..  Training Loss: 29.128..  Test Loss: 30.270.. \n",
      "Epoch: 54/500..  Training Loss: 28.641..  Test Loss: 30.178.. \n",
      "Epoch: 55/500..  Training Loss: 28.496..  Test Loss: 30.388.. \n",
      "Epoch: 56/500..  Training Loss: 28.147..  Test Loss: 33.288.. \n",
      "Epoch: 57/500..  Training Loss: 27.996..  Test Loss: 29.455.. \n",
      "Epoch: 58/500..  Training Loss: 28.395..  Test Loss: 28.521.. \n",
      "Epoch: 59/500..  Training Loss: 28.196..  Test Loss: 35.743.. \n",
      "Epoch: 60/500..  Training Loss: 28.039..  Test Loss: 35.147.. \n",
      "Epoch: 62/500..  Training Loss: 27.928..  Test Loss: 34.490.. \n",
      "Epoch: 63/500..  Training Loss: 27.453..  Test Loss: 33.225.. \n",
      "Epoch: 64/500..  Training Loss: 27.351..  Test Loss: 27.517.. \n",
      "Epoch: 65/500..  Training Loss: 26.758..  Test Loss: 28.180.. \n",
      "Epoch: 66/500..  Training Loss: 26.613..  Test Loss: 32.943.. \n",
      "Epoch: 67/500..  Training Loss: 26.512..  Test Loss: 33.596.. \n",
      "Epoch: 68/500..  Training Loss: 26.553..  Test Loss: 27.990.. \n",
      "Epoch: 69/500..  Training Loss: 26.715..  Test Loss: 28.083.. \n",
      "Epoch: 70/500..  Training Loss: 26.404..  Test Loss: 30.105.. \n",
      "Epoch: 72/500..  Training Loss: 26.899..  Test Loss: 28.756.. \n",
      "Epoch: 73/500..  Training Loss: 26.384..  Test Loss: 36.895.. \n",
      "Epoch: 74/500..  Training Loss: 26.419..  Test Loss: 28.178.. \n",
      "Epoch: 75/500..  Training Loss: 26.006..  Test Loss: 32.515.. \n",
      "Epoch: 76/500..  Training Loss: 25.536..  Test Loss: 33.208.. \n",
      "Epoch: 77/500..  Training Loss: 25.500..  Test Loss: 36.971.. \n",
      "Epoch: 78/500..  Training Loss: 25.807..  Test Loss: 37.125.. \n",
      "Epoch: 79/500..  Training Loss: 24.604..  Test Loss: 35.880.. \n",
      "Epoch: 80/500..  Training Loss: 25.161..  Test Loss: 30.081.. \n",
      "Epoch: 82/500..  Training Loss: 24.957..  Test Loss: 27.091.. \n",
      "Epoch: 83/500..  Training Loss: 24.948..  Test Loss: 36.613.. \n",
      "Epoch: 84/500..  Training Loss: 25.385..  Test Loss: 29.232.. \n",
      "Epoch: 85/500..  Training Loss: 24.587..  Test Loss: 33.207.. \n",
      "Epoch: 86/500..  Training Loss: 24.544..  Test Loss: 28.042.. \n",
      "Epoch: 87/500..  Training Loss: 24.114..  Test Loss: 26.161.. \n",
      "Epoch: 88/500..  Training Loss: 24.458..  Test Loss: 33.910.. \n",
      "Epoch: 89/500..  Training Loss: 23.975..  Test Loss: 31.128.. \n",
      "Epoch: 90/500..  Training Loss: 24.109..  Test Loss: 25.804.. \n",
      "Epoch: 92/500..  Training Loss: 23.460..  Test Loss: 25.527.. \n",
      "Epoch: 93/500..  Training Loss: 23.349..  Test Loss: 26.568.. \n",
      "Epoch: 94/500..  Training Loss: 22.889..  Test Loss: 27.383.. \n",
      "Epoch: 95/500..  Training Loss: 23.217..  Test Loss: 32.643.. \n",
      "Epoch: 96/500..  Training Loss: 22.954..  Test Loss: 27.463.. \n",
      "Epoch: 97/500..  Training Loss: 29.569..  Test Loss: 91.989.. \n",
      "Epoch: 98/500..  Training Loss: 32.887..  Test Loss: 84.767.. \n",
      "Epoch: 99/500..  Training Loss: 30.122..  Test Loss: 38.522.. \n",
      "Epoch: 100/500..  Training Loss: 28.269..  Test Loss: 30.256.. \n",
      "Epoch: 102/500..  Training Loss: 27.048..  Test Loss: 29.081.. \n",
      "Epoch: 103/500..  Training Loss: 26.014..  Test Loss: 30.178.. \n",
      "Epoch: 104/500..  Training Loss: 25.506..  Test Loss: 29.084.. \n",
      "Epoch: 105/500..  Training Loss: 24.717..  Test Loss: 27.114.. \n",
      "Epoch: 106/500..  Training Loss: 25.013..  Test Loss: 26.090.. \n",
      "Epoch: 107/500..  Training Loss: 24.052..  Test Loss: 27.325.. \n",
      "Epoch: 108/500..  Training Loss: 23.709..  Test Loss: 25.564.. \n",
      "Epoch: 109/500..  Training Loss: 23.528..  Test Loss: 28.379.. \n",
      "Epoch: 110/500..  Training Loss: 23.580..  Test Loss: 32.433.. \n",
      "Epoch: 112/500..  Training Loss: 23.220..  Test Loss: 27.209.. \n",
      "Epoch: 113/500..  Training Loss: 23.719..  Test Loss: 25.773.. \n",
      "Epoch: 114/500..  Training Loss: 23.141..  Test Loss: 25.525.. \n",
      "Epoch: 115/500..  Training Loss: 22.776..  Test Loss: 25.476.. \n",
      "Epoch: 116/500..  Training Loss: 22.818..  Test Loss: 32.900.. \n",
      "Epoch: 117/500..  Training Loss: 22.672..  Test Loss: 27.641.. \n",
      "Epoch: 118/500..  Training Loss: 22.147..  Test Loss: 25.623.. \n",
      "Epoch: 119/500..  Training Loss: 22.449..  Test Loss: 25.093.. \n",
      "Epoch: 120/500..  Training Loss: 21.574..  Test Loss: 24.501.. \n",
      "Epoch: 122/500..  Training Loss: 21.461..  Test Loss: 27.376.. \n",
      "Epoch: 123/500..  Training Loss: 21.876..  Test Loss: 24.564.. \n",
      "Epoch: 124/500..  Training Loss: 21.208..  Test Loss: 32.252.. \n",
      "Epoch: 125/500..  Training Loss: 21.824..  Test Loss: 26.072.. \n",
      "Epoch: 126/500..  Training Loss: 21.732..  Test Loss: 35.926.. \n",
      "Epoch: 127/500..  Training Loss: 22.009..  Test Loss: 29.009.. \n",
      "Epoch: 128/500..  Training Loss: 21.430..  Test Loss: 25.010.. \n",
      "Epoch: 129/500..  Training Loss: 21.573..  Test Loss: 27.218.. \n",
      "Epoch: 130/500..  Training Loss: 21.307..  Test Loss: 24.936.. \n",
      "Epoch: 132/500..  Training Loss: 20.763..  Test Loss: 29.833.. \n",
      "Epoch: 133/500..  Training Loss: 21.283..  Test Loss: 26.227.. \n",
      "Epoch: 134/500..  Training Loss: 22.303..  Test Loss: 36.370.. \n",
      "Epoch: 135/500..  Training Loss: 21.678..  Test Loss: 33.267.. \n",
      "Epoch: 136/500..  Training Loss: 20.976..  Test Loss: 25.668.. \n",
      "Epoch: 137/500..  Training Loss: 21.803..  Test Loss: 30.451.. \n",
      "Epoch: 138/500..  Training Loss: 20.461..  Test Loss: 25.746.. \n",
      "Epoch: 139/500..  Training Loss: 20.678..  Test Loss: 24.216.. \n",
      "Epoch: 140/500..  Training Loss: 20.036..  Test Loss: 30.178.. \n",
      "Epoch: 142/500..  Training Loss: 20.079..  Test Loss: 26.303.. \n",
      "Epoch: 143/500..  Training Loss: 19.689..  Test Loss: 25.625.. \n",
      "Epoch: 144/500..  Training Loss: 19.980..  Test Loss: 30.122.. \n",
      "Epoch: 145/500..  Training Loss: 19.277..  Test Loss: 30.775.. \n",
      "Epoch: 146/500..  Training Loss: 19.531..  Test Loss: 28.781.. \n",
      "Epoch: 147/500..  Training Loss: 19.524..  Test Loss: 27.920.. \n",
      "Epoch: 148/500..  Training Loss: 20.000..  Test Loss: 24.474.. \n",
      "Epoch: 149/500..  Training Loss: 19.579..  Test Loss: 23.817.. \n",
      "Epoch: 150/500..  Training Loss: 19.414..  Test Loss: 28.413.. \n",
      "Epoch: 152/500..  Training Loss: 19.085..  Test Loss: 26.989.. \n",
      "Epoch: 153/500..  Training Loss: 19.085..  Test Loss: 25.669.. \n",
      "Epoch: 154/500..  Training Loss: 18.871..  Test Loss: 33.812.. \n",
      "Epoch: 155/500..  Training Loss: 19.261..  Test Loss: 24.340.. \n",
      "Epoch: 156/500..  Training Loss: 19.147..  Test Loss: 23.625.. \n",
      "Epoch: 157/500..  Training Loss: 20.220..  Test Loss: 36.465.. \n",
      "Epoch: 158/500..  Training Loss: 18.894..  Test Loss: 27.688.. \n",
      "Epoch: 159/500..  Training Loss: 18.659..  Test Loss: 23.530.. \n",
      "Epoch: 160/500..  Training Loss: 18.580..  Test Loss: 25.659.. \n",
      "Epoch: 162/500..  Training Loss: 18.510..  Test Loss: 33.049.. \n",
      "Epoch: 163/500..  Training Loss: 18.553..  Test Loss: 23.846.. \n",
      "Epoch: 164/500..  Training Loss: 18.325..  Test Loss: 35.335.. \n",
      "Epoch: 165/500..  Training Loss: 18.270..  Test Loss: 22.780.. \n",
      "Epoch: 166/500..  Training Loss: 18.242..  Test Loss: 36.069.. \n",
      "Epoch: 167/500..  Training Loss: 18.457..  Test Loss: 35.318.. \n",
      "Epoch: 168/500..  Training Loss: 18.385..  Test Loss: 23.808.. \n",
      "Epoch: 169/500..  Training Loss: 18.264..  Test Loss: 25.018.. \n",
      "Epoch: 170/500..  Training Loss: 17.597..  Test Loss: 23.580.. \n",
      "Epoch: 172/500..  Training Loss: 17.633..  Test Loss: 23.979.. \n",
      "Epoch: 173/500..  Training Loss: 17.245..  Test Loss: 23.590.. \n",
      "Epoch: 174/500..  Training Loss: 17.731..  Test Loss: 26.021.. \n",
      "Epoch: 175/500..  Training Loss: 18.115..  Test Loss: 24.246.. \n",
      "Epoch: 176/500..  Training Loss: 17.545..  Test Loss: 31.717.. \n",
      "Epoch: 177/500..  Training Loss: 17.483..  Test Loss: 23.980.. \n",
      "Epoch: 178/500..  Training Loss: 17.528..  Test Loss: 26.991.. \n",
      "Epoch: 179/500..  Training Loss: 17.366..  Test Loss: 27.724.. \n",
      "Epoch: 180/500..  Training Loss: 17.134..  Test Loss: 22.681.. \n",
      "Epoch: 182/500..  Training Loss: 17.359..  Test Loss: 23.239.. \n",
      "Epoch: 183/500..  Training Loss: 16.971..  Test Loss: 25.239.. \n",
      "Epoch: 184/500..  Training Loss: 17.548..  Test Loss: 35.971.. \n",
      "Epoch: 185/500..  Training Loss: 17.736..  Test Loss: 35.526.. \n",
      "Epoch: 186/500..  Training Loss: 17.445..  Test Loss: 27.469.. \n",
      "Epoch: 187/500..  Training Loss: 21.416..  Test Loss: 32.177.. \n",
      "Epoch: 188/500..  Training Loss: 24.327..  Test Loss: 28.746.. \n",
      "Epoch: 189/500..  Training Loss: 22.592..  Test Loss: 36.104.. \n",
      "Epoch: 190/500..  Training Loss: 21.804..  Test Loss: 25.649.. \n",
      "Epoch: 192/500..  Training Loss: 20.748..  Test Loss: 27.781.. \n",
      "Epoch: 193/500..  Training Loss: 19.811..  Test Loss: 24.612.. \n",
      "Epoch: 194/500..  Training Loss: 19.189..  Test Loss: 33.701.. \n",
      "Epoch: 195/500..  Training Loss: 18.650..  Test Loss: 39.560.. \n",
      "Epoch: 196/500..  Training Loss: 18.689..  Test Loss: 37.371.. \n",
      "Epoch: 197/500..  Training Loss: 18.685..  Test Loss: 23.327.. \n",
      "Epoch: 198/500..  Training Loss: 17.861..  Test Loss: 25.158.. \n",
      "Epoch: 199/500..  Training Loss: 18.283..  Test Loss: 37.159.. \n",
      "Epoch: 200/500..  Training Loss: 18.592..  Test Loss: 35.816.. \n",
      "Epoch: 202/500..  Training Loss: 18.324..  Test Loss: 26.875.. \n",
      "Epoch: 203/500..  Training Loss: 17.751..  Test Loss: 25.432.. \n",
      "Epoch: 204/500..  Training Loss: 16.947..  Test Loss: 35.870.. \n",
      "Epoch: 205/500..  Training Loss: 17.262..  Test Loss: 27.208.. \n",
      "Epoch: 206/500..  Training Loss: 16.993..  Test Loss: 38.358.. \n",
      "Epoch: 207/500..  Training Loss: 16.856..  Test Loss: 27.212.. \n",
      "Epoch: 208/500..  Training Loss: 16.892..  Test Loss: 24.968.. \n",
      "Epoch: 209/500..  Training Loss: 16.870..  Test Loss: 22.494.. \n",
      "Epoch: 210/500..  Training Loss: 16.329..  Test Loss: 23.605.. \n",
      "Epoch: 212/500..  Training Loss: 16.606..  Test Loss: 23.943.. \n",
      "Epoch: 213/500..  Training Loss: 16.622..  Test Loss: 34.690.. \n",
      "Epoch: 214/500..  Training Loss: 17.986..  Test Loss: 38.098.. \n",
      "Epoch: 215/500..  Training Loss: 17.590..  Test Loss: 26.358.. \n",
      "Epoch: 216/500..  Training Loss: 17.444..  Test Loss: 25.741.. \n",
      "Epoch: 217/500..  Training Loss: 16.797..  Test Loss: 26.720.. \n",
      "Epoch: 218/500..  Training Loss: 16.227..  Test Loss: 37.948.. \n",
      "Epoch: 219/500..  Training Loss: 16.645..  Test Loss: 42.933.. \n",
      "Epoch: 220/500..  Training Loss: 17.483..  Test Loss: 23.744.. \n",
      "Epoch: 222/500..  Training Loss: 17.458..  Test Loss: 28.424.. \n",
      "Epoch: 223/500..  Training Loss: 16.929..  Test Loss: 24.587.. \n",
      "Epoch: 224/500..  Training Loss: 16.411..  Test Loss: 23.015.. \n",
      "Epoch: 225/500..  Training Loss: 16.401..  Test Loss: 33.711.. \n",
      "Epoch: 226/500..  Training Loss: 16.044..  Test Loss: 22.777.. \n",
      "Epoch: 227/500..  Training Loss: 15.593..  Test Loss: 36.655.. \n",
      "Epoch: 228/500..  Training Loss: 15.730..  Test Loss: 23.193.. \n",
      "Epoch: 229/500..  Training Loss: 15.430..  Test Loss: 39.679.. \n",
      "Epoch: 230/500..  Training Loss: 14.907..  Test Loss: 42.719.. \n",
      "Epoch: 232/500..  Training Loss: 14.611..  Test Loss: 33.071.. \n",
      "Epoch: 233/500..  Training Loss: 14.647..  Test Loss: 31.159.. \n",
      "Epoch: 234/500..  Training Loss: 14.435..  Test Loss: 21.760.. \n",
      "Epoch: 235/500..  Training Loss: 14.152..  Test Loss: 25.142.. \n",
      "Epoch: 236/500..  Training Loss: 14.315..  Test Loss: 22.825.. \n",
      "Epoch: 237/500..  Training Loss: 14.174..  Test Loss: 31.029.. \n",
      "Epoch: 238/500..  Training Loss: 14.345..  Test Loss: 39.722.. \n",
      "Epoch: 239/500..  Training Loss: 14.957..  Test Loss: 32.377.. \n",
      "Epoch: 240/500..  Training Loss: 14.604..  Test Loss: 22.762.. \n",
      "Epoch: 242/500..  Training Loss: 14.439..  Test Loss: 21.700.. \n",
      "Epoch: 243/500..  Training Loss: 13.941..  Test Loss: 27.110.. \n",
      "Epoch: 244/500..  Training Loss: 13.879..  Test Loss: 48.992.. \n",
      "Epoch: 245/500..  Training Loss: 13.665..  Test Loss: 24.460.. \n",
      "Epoch: 246/500..  Training Loss: 14.142..  Test Loss: 24.056.. \n",
      "Epoch: 247/500..  Training Loss: 14.386..  Test Loss: 28.841.. \n",
      "Epoch: 248/500..  Training Loss: 13.839..  Test Loss: 58.096.. \n",
      "Epoch: 249/500..  Training Loss: 13.729..  Test Loss: 21.708.. \n",
      "Epoch: 250/500..  Training Loss: 13.867..  Test Loss: 45.073.. \n",
      "Epoch: 252/500..  Training Loss: 13.677..  Test Loss: 26.777.. \n",
      "Epoch: 253/500..  Training Loss: 13.636..  Test Loss: 42.053.. \n",
      "Epoch: 254/500..  Training Loss: 13.393..  Test Loss: 39.630.. \n",
      "Epoch: 255/500..  Training Loss: 13.277..  Test Loss: 22.132.. \n",
      "Epoch: 256/500..  Training Loss: 12.947..  Test Loss: 21.117.. \n",
      "Epoch: 257/500..  Training Loss: 13.311..  Test Loss: 24.466.. \n",
      "Epoch: 258/500..  Training Loss: 13.160..  Test Loss: 22.468.. \n",
      "Epoch: 259/500..  Training Loss: 12.579..  Test Loss: 24.049.. \n",
      "Epoch: 260/500..  Training Loss: 12.567..  Test Loss: 28.264.. \n",
      "Epoch: 262/500..  Training Loss: 12.710..  Test Loss: 20.692.. \n",
      "Epoch: 263/500..  Training Loss: 12.748..  Test Loss: 22.626.. \n",
      "Epoch: 264/500..  Training Loss: 12.989..  Test Loss: 43.690.. \n",
      "Epoch: 265/500..  Training Loss: 12.469..  Test Loss: 43.261.. \n",
      "Epoch: 266/500..  Training Loss: 12.809..  Test Loss: 26.377.. \n",
      "Epoch: 267/500..  Training Loss: 12.738..  Test Loss: 22.672.. \n",
      "Epoch: 268/500..  Training Loss: 12.811..  Test Loss: 27.029.. \n",
      "Epoch: 269/500..  Training Loss: 12.373..  Test Loss: 24.128.. \n",
      "Epoch: 270/500..  Training Loss: 11.992..  Test Loss: 22.087.. \n",
      "Epoch: 272/500..  Training Loss: 11.938..  Test Loss: 19.856.. \n",
      "Epoch: 273/500..  Training Loss: 11.550..  Test Loss: 43.076.. \n",
      "Epoch: 274/500..  Training Loss: 11.429..  Test Loss: 31.867.. \n",
      "Epoch: 275/500..  Training Loss: 11.134..  Test Loss: 25.247.. \n",
      "Epoch: 276/500..  Training Loss: 11.730..  Test Loss: 25.546.. \n",
      "Epoch: 277/500..  Training Loss: 11.517..  Test Loss: 44.675.. \n",
      "Epoch: 278/500..  Training Loss: 11.492..  Test Loss: 24.723.. \n",
      "Epoch: 279/500..  Training Loss: 11.264..  Test Loss: 19.634.. \n",
      "Epoch: 280/500..  Training Loss: 11.418..  Test Loss: 23.822.. \n",
      "Epoch: 282/500..  Training Loss: 11.036..  Test Loss: 43.754.. \n",
      "Epoch: 283/500..  Training Loss: 11.267..  Test Loss: 68.275.. \n",
      "Epoch: 284/500..  Training Loss: 12.555..  Test Loss: 41.317.. \n",
      "Epoch: 285/500..  Training Loss: 13.911..  Test Loss: 28.357.. \n",
      "Epoch: 286/500..  Training Loss: 15.378..  Test Loss: 36.137.. \n",
      "Epoch: 287/500..  Training Loss: 14.122..  Test Loss: 41.956.. \n",
      "Epoch: 288/500..  Training Loss: 12.895..  Test Loss: 22.257.. \n",
      "Epoch: 289/500..  Training Loss: 12.581..  Test Loss: 22.081.. \n",
      "Epoch: 290/500..  Training Loss: 12.117..  Test Loss: 26.861.. \n",
      "Epoch: 292/500..  Training Loss: 11.669..  Test Loss: 25.489.. \n",
      "Epoch: 293/500..  Training Loss: 11.275..  Test Loss: 36.244.. \n",
      "Epoch: 294/500..  Training Loss: 11.657..  Test Loss: 32.830.. \n",
      "Epoch: 295/500..  Training Loss: 11.739..  Test Loss: 23.372.. \n",
      "Epoch: 296/500..  Training Loss: 11.607..  Test Loss: 20.462.. \n",
      "Epoch: 297/500..  Training Loss: 11.023..  Test Loss: 44.984.. \n",
      "Epoch: 298/500..  Training Loss: 10.673..  Test Loss: 23.249.. \n",
      "Epoch: 299/500..  Training Loss: 10.010..  Test Loss: 26.239.. \n",
      "Epoch: 300/500..  Training Loss: 9.559..  Test Loss: 20.246.. \n",
      "Epoch: 302/500..  Training Loss: 9.773..  Test Loss: 36.309.. \n",
      "Epoch: 303/500..  Training Loss: 9.877..  Test Loss: 23.996.. \n",
      "Epoch: 304/500..  Training Loss: 9.594..  Test Loss: 52.192.. \n",
      "Epoch: 305/500..  Training Loss: 10.438..  Test Loss: 26.678.. \n",
      "Epoch: 306/500..  Training Loss: 9.626..  Test Loss: 23.303.. \n",
      "Epoch: 307/500..  Training Loss: 9.884..  Test Loss: 22.532.. \n",
      "Epoch: 308/500..  Training Loss: 9.355..  Test Loss: 19.373.. \n",
      "Epoch: 309/500..  Training Loss: 9.635..  Test Loss: 28.498.. \n",
      "Epoch: 310/500..  Training Loss: 9.202..  Test Loss: 28.192.. \n",
      "Epoch: 312/500..  Training Loss: 8.841..  Test Loss: 38.529.. \n",
      "Epoch: 313/500..  Training Loss: 8.980..  Test Loss: 24.232.. \n",
      "Epoch: 314/500..  Training Loss: 9.954..  Test Loss: 26.406.. \n",
      "Epoch: 315/500..  Training Loss: 9.562..  Test Loss: 31.709.. \n",
      "Epoch: 316/500..  Training Loss: 9.318..  Test Loss: 37.730.. \n",
      "Epoch: 317/500..  Training Loss: 8.772..  Test Loss: 20.847.. \n",
      "Epoch: 318/500..  Training Loss: 8.380..  Test Loss: 20.539.. \n",
      "Epoch: 319/500..  Training Loss: 8.079..  Test Loss: 18.719.. \n",
      "Epoch: 320/500..  Training Loss: 8.119..  Test Loss: 99.558.. \n",
      "Epoch: 322/500..  Training Loss: 8.243..  Test Loss: 19.324.. \n",
      "Epoch: 323/500..  Training Loss: 7.805..  Test Loss: 18.707.. \n",
      "Epoch: 324/500..  Training Loss: 7.985..  Test Loss: 21.394.. \n",
      "Epoch: 325/500..  Training Loss: 7.797..  Test Loss: 22.578.. \n",
      "Epoch: 326/500..  Training Loss: 8.342..  Test Loss: 31.132.. \n",
      "Epoch: 327/500..  Training Loss: 9.205..  Test Loss: 29.004.. \n",
      "Epoch: 328/500..  Training Loss: 8.166..  Test Loss: 29.787.. \n",
      "Epoch: 329/500..  Training Loss: 8.356..  Test Loss: 19.490.. \n",
      "Epoch: 330/500..  Training Loss: 8.699..  Test Loss: 143.582.. \n",
      "Epoch: 332/500..  Training Loss: 7.810..  Test Loss: 23.740.. \n",
      "Epoch: 333/500..  Training Loss: 7.484..  Test Loss: 68.495.. \n",
      "Epoch: 334/500..  Training Loss: 7.229..  Test Loss: 23.482.. \n",
      "Epoch: 335/500..  Training Loss: 7.088..  Test Loss: 20.999.. \n",
      "Epoch: 336/500..  Training Loss: 7.487..  Test Loss: 26.595.. \n",
      "Epoch: 337/500..  Training Loss: 7.144..  Test Loss: 66.049.. \n",
      "Epoch: 338/500..  Training Loss: 6.770..  Test Loss: 57.315.. \n",
      "Epoch: 339/500..  Training Loss: 6.282..  Test Loss: 18.309.. \n",
      "Epoch: 340/500..  Training Loss: 6.678..  Test Loss: 29.703.. \n",
      "Epoch: 342/500..  Training Loss: 7.384..  Test Loss: 64.322.. \n",
      "Epoch: 343/500..  Training Loss: 8.110..  Test Loss: 30.683.. \n",
      "Epoch: 344/500..  Training Loss: 8.324..  Test Loss: 19.710.. \n",
      "Epoch: 345/500..  Training Loss: 8.368..  Test Loss: 20.202.. \n",
      "Epoch: 346/500..  Training Loss: 7.855..  Test Loss: 20.926.. \n",
      "Epoch: 347/500..  Training Loss: 7.794..  Test Loss: 30.529.. \n",
      "Epoch: 348/500..  Training Loss: 7.719..  Test Loss: 19.686.. \n",
      "Epoch: 349/500..  Training Loss: 7.330..  Test Loss: 92.662.. \n",
      "Epoch: 350/500..  Training Loss: 7.154..  Test Loss: 19.685.. \n",
      "Epoch: 352/500..  Training Loss: 7.356..  Test Loss: 18.227.. \n",
      "Epoch: 353/500..  Training Loss: 7.155..  Test Loss: 18.139.. \n",
      "Epoch: 354/500..  Training Loss: 6.943..  Test Loss: 26.294.. \n",
      "Epoch: 355/500..  Training Loss: 7.051..  Test Loss: 20.205.. \n",
      "Epoch: 356/500..  Training Loss: 7.005..  Test Loss: 44.764.. \n",
      "Epoch: 357/500..  Training Loss: 6.804..  Test Loss: 19.068.. \n",
      "Epoch: 358/500..  Training Loss: 8.122..  Test Loss: 23.258.. \n",
      "Epoch: 359/500..  Training Loss: 8.506..  Test Loss: 27.891.. \n",
      "Epoch: 360/500..  Training Loss: 7.854..  Test Loss: 32.229.. \n",
      "Epoch: 362/500..  Training Loss: 7.192..  Test Loss: 41.876.. \n",
      "Epoch: 363/500..  Training Loss: 7.042..  Test Loss: 20.413.. \n",
      "Epoch: 364/500..  Training Loss: 6.652..  Test Loss: 19.559.. \n",
      "Epoch: 365/500..  Training Loss: 7.057..  Test Loss: 29.879.. \n",
      "Epoch: 366/500..  Training Loss: 7.156..  Test Loss: 32.351.. \n",
      "Epoch: 367/500..  Training Loss: 6.753..  Test Loss: 41.431.. \n",
      "Epoch: 368/500..  Training Loss: 6.185..  Test Loss: 26.957.. \n",
      "Epoch: 369/500..  Training Loss: 5.928..  Test Loss: 18.552.. \n",
      "Epoch: 370/500..  Training Loss: 5.503..  Test Loss: 19.823.. \n",
      "Epoch: 372/500..  Training Loss: 5.605..  Test Loss: 18.442.. \n",
      "Epoch: 373/500..  Training Loss: 5.827..  Test Loss: 18.662.. \n",
      "Epoch: 374/500..  Training Loss: 5.693..  Test Loss: 87.624.. \n",
      "Epoch: 375/500..  Training Loss: 6.513..  Test Loss: 21.713.. \n",
      "Epoch: 376/500..  Training Loss: 6.429..  Test Loss: 44.551.. \n",
      "Epoch: 377/500..  Training Loss: 8.377..  Test Loss: 54.268.. \n",
      "Epoch: 378/500..  Training Loss: 8.027..  Test Loss: 22.309.. \n",
      "Epoch: 379/500..  Training Loss: 7.369..  Test Loss: 4134.084.. \n",
      "Epoch: 380/500..  Training Loss: 7.136..  Test Loss: 102.389.. \n",
      "Epoch: 382/500..  Training Loss: 8.013..  Test Loss: 50.150.. \n",
      "Epoch: 383/500..  Training Loss: 7.351..  Test Loss: 24.222.. \n",
      "Epoch: 384/500..  Training Loss: 6.484..  Test Loss: 20.812.. \n",
      "Epoch: 385/500..  Training Loss: 5.833..  Test Loss: 20.295.. \n",
      "Epoch: 386/500..  Training Loss: 5.280..  Test Loss: 22.078.. \n",
      "Epoch: 387/500..  Training Loss: 5.872..  Test Loss: 23.157.. \n",
      "Epoch: 388/500..  Training Loss: 6.114..  Test Loss: 19.632.. \n",
      "Epoch: 389/500..  Training Loss: 5.077..  Test Loss: 18.857.. \n",
      "Epoch: 390/500..  Training Loss: 4.816..  Test Loss: 18.814.. \n",
      "Epoch: 392/500..  Training Loss: 4.882..  Test Loss: 19.768.. \n",
      "Epoch: 393/500..  Training Loss: 5.766..  Test Loss: 40.147.. \n",
      "Epoch: 394/500..  Training Loss: 6.618..  Test Loss: 21.363.. \n",
      "Epoch: 395/500..  Training Loss: 6.493..  Test Loss: 36.296.. \n",
      "Epoch: 396/500..  Training Loss: 5.600..  Test Loss: 50.273.. \n",
      "Epoch: 397/500..  Training Loss: 5.016..  Test Loss: 19.237.. \n",
      "Epoch: 398/500..  Training Loss: 4.837..  Test Loss: 35.854.. \n",
      "Epoch: 399/500..  Training Loss: 5.454..  Test Loss: 18.953.. \n",
      "Epoch: 400/500..  Training Loss: 5.136..  Test Loss: 20.875.. \n",
      "Epoch: 402/500..  Training Loss: 4.823..  Test Loss: 26.272.. \n",
      "Epoch: 403/500..  Training Loss: 4.411..  Test Loss: 19.333.. \n",
      "Epoch: 404/500..  Training Loss: 4.685..  Test Loss: 19.067.. \n",
      "Epoch: 405/500..  Training Loss: 4.614..  Test Loss: 18.584.. \n",
      "Epoch: 406/500..  Training Loss: 5.008..  Test Loss: 19.569.. \n",
      "Epoch: 407/500..  Training Loss: 4.610..  Test Loss: 19.562.. \n",
      "Epoch: 408/500..  Training Loss: 4.687..  Test Loss: 21.129.. \n",
      "Epoch: 409/500..  Training Loss: 4.471..  Test Loss: 21.143.. \n",
      "Epoch: 410/500..  Training Loss: 4.604..  Test Loss: 20.084.. \n",
      "Epoch: 412/500..  Training Loss: 4.356..  Test Loss: 18.498.. \n",
      "Epoch: 413/500..  Training Loss: 5.219..  Test Loss: 26.662.. \n",
      "Epoch: 414/500..  Training Loss: 7.684..  Test Loss: 29.053.. \n",
      "Epoch: 415/500..  Training Loss: 6.527..  Test Loss: 148.781.. \n",
      "Epoch: 416/500..  Training Loss: 5.747..  Test Loss: 19.766.. \n",
      "Epoch: 417/500..  Training Loss: 4.606..  Test Loss: 26.008.. \n",
      "Epoch: 418/500..  Training Loss: 4.131..  Test Loss: 19.744.. \n",
      "Epoch: 419/500..  Training Loss: 4.544..  Test Loss: 30.789.. \n",
      "Epoch: 420/500..  Training Loss: 5.045..  Test Loss: 19.554.. \n",
      "Epoch: 422/500..  Training Loss: 4.695..  Test Loss: 22.076.. \n",
      "Epoch: 423/500..  Training Loss: 4.486..  Test Loss: 18.414.. \n",
      "Epoch: 424/500..  Training Loss: 4.222..  Test Loss: 20.029.. \n",
      "Epoch: 425/500..  Training Loss: 4.311..  Test Loss: 19.602.. \n",
      "Epoch: 426/500..  Training Loss: 4.015..  Test Loss: 19.317.. \n",
      "Epoch: 427/500..  Training Loss: 3.733..  Test Loss: 18.476.. \n",
      "Epoch: 428/500..  Training Loss: 3.729..  Test Loss: 18.730.. \n",
      "Epoch: 429/500..  Training Loss: 3.377..  Test Loss: 31.099.. \n",
      "Epoch: 430/500..  Training Loss: 3.322..  Test Loss: 19.396.. \n",
      "Epoch: 432/500..  Training Loss: 3.111..  Test Loss: 25.980.. \n",
      "Epoch: 433/500..  Training Loss: 3.032..  Test Loss: 22.117.. \n",
      "Epoch: 434/500..  Training Loss: 3.361..  Test Loss: 23.260.. \n",
      "Epoch: 435/500..  Training Loss: 3.128..  Test Loss: 19.161.. \n",
      "Epoch: 436/500..  Training Loss: 3.140..  Test Loss: 19.961.. \n",
      "Epoch: 437/500..  Training Loss: 2.982..  Test Loss: 20.526.. \n",
      "Epoch: 438/500..  Training Loss: 2.790..  Test Loss: 19.142.. \n",
      "Epoch: 439/500..  Training Loss: 2.878..  Test Loss: 18.907.. \n",
      "Epoch: 440/500..  Training Loss: 2.597..  Test Loss: 19.482.. \n",
      "Epoch: 442/500..  Training Loss: 2.531..  Test Loss: 18.658.. \n",
      "Epoch: 443/500..  Training Loss: 2.508..  Test Loss: 18.983.. \n",
      "Epoch: 444/500..  Training Loss: 2.291..  Test Loss: 18.866.. \n",
      "Epoch: 445/500..  Training Loss: 2.486..  Test Loss: 20.235.. \n",
      "Epoch: 446/500..  Training Loss: 2.481..  Test Loss: 19.714.. \n",
      "Epoch: 447/500..  Training Loss: 2.672..  Test Loss: 24.819.. \n",
      "Epoch: 448/500..  Training Loss: 2.620..  Test Loss: 21.064.. \n",
      "Epoch: 449/500..  Training Loss: 3.218..  Test Loss: 20.745.. \n",
      "Epoch: 450/500..  Training Loss: 3.037..  Test Loss: 26.528.. \n",
      "Epoch: 452/500..  Training Loss: 3.218..  Test Loss: 21.542.. \n",
      "Epoch: 453/500..  Training Loss: 3.338..  Test Loss: 119.574.. \n",
      "Epoch: 454/500..  Training Loss: 4.309..  Test Loss: 20.276.. \n",
      "Epoch: 455/500..  Training Loss: 3.960..  Test Loss: 19.135.. \n",
      "Epoch: 456/500..  Training Loss: 3.732..  Test Loss: 26.016.. \n",
      "Epoch: 457/500..  Training Loss: 3.000..  Test Loss: 19.536.. \n",
      "Epoch: 458/500..  Training Loss: 2.913..  Test Loss: 21.841.. \n",
      "Epoch: 459/500..  Training Loss: 2.508..  Test Loss: 19.411.. \n",
      "Epoch: 460/500..  Training Loss: 2.345..  Test Loss: 19.224.. \n",
      "Epoch: 462/500..  Training Loss: 2.429..  Test Loss: 19.770.. \n",
      "Epoch: 463/500..  Training Loss: 7.375..  Test Loss: 8979.974.. \n",
      "Epoch: 464/500..  Training Loss: 8.377..  Test Loss: 26.121.. \n",
      "Epoch: 465/500..  Training Loss: 7.623..  Test Loss: 39.882.. \n",
      "Epoch: 466/500..  Training Loss: 7.575..  Test Loss: 29.150.. \n",
      "Epoch: 467/500..  Training Loss: 8.723..  Test Loss: 22.192.. \n",
      "Epoch: 468/500..  Training Loss: 8.469..  Test Loss: 49.123.. \n",
      "Epoch: 469/500..  Training Loss: 6.601..  Test Loss: 76.031.. \n",
      "Epoch: 470/500..  Training Loss: 6.430..  Test Loss: 19.750.. \n",
      "Epoch: 472/500..  Training Loss: 5.335..  Test Loss: 19.539.. \n",
      "Epoch: 473/500..  Training Loss: 4.746..  Test Loss: 31.864.. \n",
      "Epoch: 474/500..  Training Loss: 4.697..  Test Loss: 19.260.. \n",
      "Epoch: 475/500..  Training Loss: 4.934..  Test Loss: 69.452.. \n",
      "Epoch: 476/500..  Training Loss: 4.422..  Test Loss: 19.292.. \n",
      "Epoch: 477/500..  Training Loss: 4.788..  Test Loss: 51.632.. \n",
      "Epoch: 478/500..  Training Loss: 6.584..  Test Loss: 20.243.. \n",
      "Epoch: 479/500..  Training Loss: 6.394..  Test Loss: 114.674.. \n",
      "Epoch: 480/500..  Training Loss: 5.177..  Test Loss: 61.074.. \n",
      "Epoch: 482/500..  Training Loss: 3.988..  Test Loss: 32.728.. \n",
      "Epoch: 483/500..  Training Loss: 3.797..  Test Loss: 19.088.. \n",
      "Epoch: 484/500..  Training Loss: 3.579..  Test Loss: 19.340.. \n",
      "Epoch: 485/500..  Training Loss: 3.026..  Test Loss: 18.776.. \n",
      "Epoch: 486/500..  Training Loss: 2.830..  Test Loss: 18.500.. \n",
      "Epoch: 487/500..  Training Loss: 2.774..  Test Loss: 19.561.. \n",
      "Epoch: 488/500..  Training Loss: 3.584..  Test Loss: 19.894.. \n",
      "Epoch: 489/500..  Training Loss: 3.535..  Test Loss: 19.732.. \n",
      "Epoch: 490/500..  Training Loss: 4.168..  Test Loss: 19.421.. \n",
      "Epoch: 492/500..  Training Loss: 4.580..  Test Loss: 19.997.. \n",
      "Epoch: 493/500..  Training Loss: 4.840..  Test Loss: 20.490.. \n",
      "Epoch: 494/500..  Training Loss: 4.262..  Test Loss: 20.744.. \n",
      "Epoch: 495/500..  Training Loss: 3.770..  Test Loss: 18.877.. \n",
      "Epoch: 496/500..  Training Loss: 2.967..  Test Loss: 18.655.. \n",
      "Epoch: 497/500..  Training Loss: 2.421..  Test Loss: 26.669.. \n",
      "Epoch: 498/500..  Training Loss: 2.170..  Test Loss: 18.804.. \n",
      "Epoch: 499/500..  Training Loss: 1.925..  Test Loss: 18.084.. \n",
      "Epoch: 500/500..  Training Loss: 1.832..  Test Loss: 18.303.. \n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD4CAYAAAAAczaOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAgzUlEQVR4nO3de5hU9Z3n8fe3qvpKd3O/2ajABmVAtMGWEFGClxmJOkoYfYQnIxBczThOTHRHxdwkybKT3WGyLrvRHaOjmLhDHB0NMWoSUENMMiEIRrkqykUEuV+66e7qrqrv/lHHthr6Bt10d9X5vJ6nnjr1q3NO/c6v6U99+dWp0+buiIhIOES6uwMiItJ1FPoiIiGi0BcRCRGFvohIiCj0RURCJNbdHWjLgAEDfPjw4d3dDRGRrPLGG2/sd/eBx7f3+NAfPnw4q1ev7u5uiIhkFTPb3ly7pndEREJEoS8iEiIKfRGREFHoi4iEiEJfRCREFPoiIiGi0BcRCRGFvojIqTjyIWx+ubt7cdIU+qfgwIEDVFRUUFFRwZAhQygvL298XF9f3+q2q1ev5s4772zzNS6++OJO6etrr73Gtdde2yn7EpEMj14J/3pTd/fipPX4b+T2RP379+fNN98EYMGCBZSUlPD3f//3jc8nEgliseaHtrKyksrKyjZf43e/+12n9FVETpOqXd3dg1OiSr+TzJ07l7vvvpvLLruM++67j1WrVnHxxRczfvx4Lr74YjZv3gw0rbwXLFjAvHnzmDp1KiNHjmTx4sWN+yspKWlcf+rUqdxwww2MHj2aL3zhC3z8185efPFFRo8ezSWXXMKdd97ZZkV/8OBBpk+fzvnnn8+kSZN46623APj1r3/d+D+V8ePHU1VVxe7du5kyZQoVFRWcd955/OY3v+n0MRORrpf1lf63f7aeDbuOduo+x5xRxgN/Ofakt3vnnXdYvnw50WiUo0ePsnLlSmKxGMuXL+drX/sazz777AnbbNq0iVdffZWqqirOPfdcbr/9dvLy8pqss3btWtavX88ZZ5zB5MmT+e1vf0tlZSVf+tKXWLlyJSNGjGDWrFlt9u+BBx5g/PjxPP/887zyyivMnj2bN998k0WLFvGDH/yAyZMnU11dTWFhIY888ghXXXUVX//610kmk9TU1Jz0eIhIz5P1od+T3HjjjUSjUQCOHDnCnDlzePfddzEzGhoamt3mmmuuoaCggIKCAgYNGsSePXsYNmxYk3UmTpzY2FZRUcG2bdsoKSlh5MiRjBgxAoBZs2bxyCOPtNq/119/vfGN5/LLL+fAgQMcOXKEyZMnc/fdd/OFL3yBGTNmMGzYMC666CLmzZtHQ0MD06dPp6KioiNDIyI9RNaH/qlU5KdLr169Gpe/+c1vctlll/Hcc8+xbds2pk6d2uw2BQUFjcvRaJREItGudU7lD9o3t42ZMX/+fK655hpefPFFJk2axPLly5kyZQorV67k5z//OTfffDP33HMPs2fPPunXFMl57mDW3b1oN83pnyZHjhyhvLwcgCeeeKLT9z969Gjef/99tm3bBsBPfvKTNreZMmUKTz31FJD+rGDAgAGUlZXx3nvvMW7cOO677z4qKyvZtGkT27dvZ9CgQdx6663ccsstrFmzptOPQUS6XtZX+j3Vvffey5w5c/j+97/P5Zdf3un7Lyoq4qGHHmLatGkMGDCAiRMntrnNggUL+OIXv8j5559PcXExS5YsAeDBBx/k1VdfJRqNMmbMGD73uc+xdOlS/vEf/5G8vDxKSkp48sknO/0YRHJCllX6dirTBF2psrLS9UdUmlddXU1JSQnuzh133MGoUaO46667urtbIuGwoHf6/lsHIRLt3r40w8zecPcTzg/X9E4W++EPf0hFRQVjx47lyJEjfOlLX+ruLomETw8vnI+n6Z0sdtddd6myF+l22RX6qvRFRDoiyyp9hb6ISIco9EVEwkOVvoiI9FQK/VMwdepUfvGLXzRpe/DBB/nbv/3bVrf5+NTTq6++msOHD5+wzoIFC1i0aFGrr/3888+zYcOGxsff+ta3WL58+Un0vnm6BLPIqVKln/NmzZrF0qVLm7QtXbq0XRc9g/TVMfv06XNKr3186H/nO9/hyiuvPKV9iUgn0PRO7rvhhht44YUXiMfjAGzbto1du3ZxySWXcPvtt1NZWcnYsWN54IEHmt1++PDh7N+/H4CFCxdy7rnncuWVVzZefhnS5+BfdNFFXHDBBfzVX/0VNTU1/O53v2PZsmXcc889VFRU8N577zF37lyeeeYZAFasWMH48eMZN24c8+bNa+zf8OHDeeCBB5gwYQLjxo1j06ZNrR6fLsEscjKyK/Sz/zz9l+bDR2937j6HjIPPfa/Fp/v378/EiRN5+eWXuf7661m6dCk33XQTZsbChQvp168fyWSSK664grfeeovzzz+/2f288cYbLF26lLVr15JIJJgwYQIXXnghADNmzODWW28F4Bvf+AaPPfYYX/7yl7nuuuu49tprueGGG5rsq66ujrlz57JixQrOOeccZs+ezcMPP8xXv/pVAAYMGMCaNWt46KGHWLRoEY8++miLx6dLMIuchFys9M3sLjNbb2brzOxfzazQzPqZ2a/M7N3gvm/G+veb2RYz22xmV2W0X2hmbwfPLTbLogtWHCdziidzaufpp59mwoQJjB8/nvXr1zeZijneb37zGz7/+c9TXFxMWVkZ1113XeNz69at49JLL2XcuHE89dRTrF+/vtX+bN68mREjRnDOOecAMGfOHFauXNn4/IwZMwC48MILGy/S1pLXX3+dm2++GWj+EsyLFy/m8OHDxGIxLrroIh5//HEWLFjA22+/TWlpaav7Fsk92RX6bVb6ZlYO3AmMcfdaM3samAmMAVa4+/fMbD4wH7jPzMYEz48FzgCWm9k57p4EHgZuA/4DeBGYBrzUoSNopSI/naZPn87dd9/NmjVrqK2tZcKECWzdupVFixbxxz/+kb59+zJ37lzq6upa3U9L73tz587l+eef54ILLuCJJ57gtddea3U/bV1D6ePLM7d0+ea29qVLMIu0IBcrfdJvDkVmFgOKgV3A9cCS4PklwPRg+XpgqbvH3X0rsAWYaGZDgTJ3/72nU+XJjG2yTklJCVOnTmXevHmNVf7Ro0fp1asXvXv3Zs+ePbz0UuvvZ1OmTOG5556jtraWqqoqfvaznzU+V1VVxdChQ2loaGi8HDJAaWkpVVVVJ+xr9OjRbNu2jS1btgDwox/9iM9+9rOndGy6BLPIyciu0G+z0nf3D81sEbADqAV+6e6/NLPB7r47WGe3mQ0KNiknXcl/bGfQ1hAsH99+AjO7jfT/CDjrrLNO7oi60KxZs5gxY0bjNM8FF1zA+PHjGTt2LCNHjmTy5Mmtbj9hwgRuuukmKioqOPvss7n00ksbn/vud7/Lpz/9ac4++2zGjRvXGPQzZ87k1ltvZfHixY0f4AIUFhby+OOPc+ONN5JIJLjooov4m7/5m1M6Ll2CWSR3tXlp5WCu/lngJuAw8G/AM8D/cfc+Gesdcve+ZvYD4Pfu/uOg/THSUzk7gH9w9yuD9kuBe939L1t7fV1aWUR6pI8vrXzfdijq061daU5HLq18JbDV3fe5ewPw78DFwJ5gyobgfm+w/k7gzIzth5GeDtoZLB/fLiKSxbJreqc9ob8DmGRmxcHZNlcAG4FlwJxgnTnAT4PlZcBMMyswsxHAKGBVMBVUZWaTgv3MzthGRCQ7ZdkHue2Z0/+DmT0DrAESwFrgEaAEeNrMbiH9xnBjsP764AyfDcH6dwRn7gDcDjwBFJE+a6djZ+6IiMhJadeXs9z9AeD4r5fGSVf9za2/EFjYTPtq4LyT7KOISM+VZZW+LsMgItIhCn0RkfBQpS8iIj2VQl9EpENU6YuIhIemd0REwkShLyISHqr0RUTCRKEvIhIeqvRFRMJEoS8iEh6q9EVEpKdS6IuIdIgqfRGR8ND0johImCj0RUTCQ5W+iEiYKPRFRMJDlb6IiPRUCn0RkRBR6IuIdISmd0REwkShLyISHqr0RUTCRKEvIhIeqvRFRMJEoS8iEh6q9EVEpKdS6IuIdIgqfRGR8ND0johImCj0RUTCQ5W+iEiYKPRFRMJDlb6ISJjkYOibWR8ze8bMNpnZRjP7jJn1M7Nfmdm7wX3fjPXvN7MtZrbZzK7KaL/QzN4OnltsZnY6DkpERJrX3kr/fwEvu/to4AJgIzAfWOHuo4AVwWPMbAwwExgLTAMeMrNosJ+HgduAUcFtWicdh4hI98i16R0zKwOmAI8BuHu9ux8GrgeWBKstAaYHy9cDS9097u5bgS3ARDMbCpS5++/d3YEnM7YREclSORb6wEhgH/C4ma01s0fNrBcw2N13AwT3g4L1y4EPMrbfGbSVB8vHt5/AzG4zs9Vmtnrfvn0ndUAiIl0quzK/XaEfAyYAD7v7eOAYwVROC5qbp/dW2k9sdH/E3SvdvXLgwIHt6KKISHfJrtRvT+jvBHa6+x+Cx8+QfhPYE0zZENzvzVj/zIzthwG7gvZhzbSLiGSvXJvTd/ePgA/M7Nyg6QpgA7AMmBO0zQF+GiwvA2aaWYGZjSD9ge2qYAqoyswmBWftzM7YRkQkS2VX6Mfaud6XgafMLB94H/gi6TeMp83sFmAHcCOAu683s6dJvzEkgDvcPRns53bgCaAIeCm4iYhkryyr9NsV+u7+JlDZzFNXtLD+QmBhM+2rgfNOon8iItKJ9I1cEZEOya5KX6EvItIRWTa9o9AXEekQhb6ISHio0hcRCROFvohIeKjSFxEJE4W+iIj0UAp9EZGO0PSOiEiYKPRFRMJDlb6ISJgo9EVEwkOVvohImCj0RUTCI7syX6EvItIx2ZX6Cn0RkRBR6IuIdIQ+yBURCROFvohIeKjSFxEJE4W+iEh4qNIXEQkThb6ISHio0hcRkZ5KoS8i0iGq9EVEwkPTOyIiYaLQFxEJD1X6IiJhotAXEQkPVfoiImGi0BcRkR5KoS8i0hGa3hERacH+d+FHM6C+prt7ElrtDn0zi5rZWjN7IXjcz8x+ZWbvBvd9M9a938y2mNlmM7sqo/1CM3s7eG6xmVnnHo6I9Ggvz4f3VsD233Z3TzpPDlf6XwE2ZjyeD6xw91HAiuAxZjYGmAmMBaYBD5lZNNjmYeA2YFRwm9ah3otIdsqyoGxddh1Lu0LfzIYB1wCPZjRfDywJlpcA0zPal7p73N23AluAiWY2FChz99+7uwNPZmwjIpKdsuwNrL2V/oPAvUAqo22wu+8GCO4HBe3lwAcZ6+0M2sqD5ePbT2Bmt5nZajNbvW/fvnZ2UUSyRk7N7OZY6JvZtcBed3+jnfts7qfprbSf2Oj+iLtXunvlwIED2/myIiLdIMsq/Vg71pkMXGdmVwOFQJmZ/RjYY2ZD3X13MHWzN1h/J3BmxvbDgF1B+7Bm2kVEpIu0Wem7+/3uPszdh5P+gPYVd/9rYBkwJ1htDvDTYHkZMNPMCsxsBOkPbFcFU0BVZjYpOGtndsY2IiJZKvcq/ZZ8D3jazG4BdgA3Arj7ejN7GtgAJIA73D0ZbHM78ARQBLwU3EREslcOTu80cvfXgNeC5QPAFS2stxBY2Ez7auC8k+2kiEjPlV2hr2/kioh0RJZV+gp9EZEOUeiLiISHKn0RkTBR6IuISA+l0BeRrpdlUyKtyrJjUeiLSBfKpWvufEyhLyISHqr0RUTCRKEvIhIeqvRFRNqSXUGZSxT6ItL1PNX2OtlClb6ISEuCgMyl0M8yCn0R6Xo5Ffqq9EVEWpdLoa/pHRGRNuRS6KvSFxFpQ5ZVx63KsmNR6ItI11Ol320U+iLS9bKsOm5Vlh2LQl9EulBwwTVV+t1GoS8iXS+XQl+VvohIG3Ip9LOMQl9Eul5Ohb4qfRGR1uVS6Gt6R0SkDbkU+qr0RUTakO2hn1ndq9IXEWlDLoW+Kn0RkTZkWXV8IlX6IiLtp0q/2yj0RaTrZXvoZ1nQZ1Loi0jXy/bQb/JBbvd141Qo9EWk62V76KPpHRGR9sv60M+gD3JFRNqQ7aGvD3JFRE5Ctod+Lp+yaWZnmtmrZrbRzNab2VeC9n5m9iszeze475uxzf1mtsXMNpvZVRntF5rZ28Fzi83MTs9hiUiPlmVBeYIcr/QTwH9x9z8DJgF3mNkYYD6wwt1HASuCxwTPzQTGAtOAh8wsGuzrYeA2YFRwm9aJxyIi2UKVfrdpM/Tdfbe7rwmWq4CNQDlwPbAkWG0JMD1Yvh5Y6u5xd98KbAEmmtlQoMzdf+/uDjyZsY2IhEp2BeUJsizoM53UnL6ZDQfGA38ABrv7bki/MQCDgtXKgQ8yNtsZtJUHy8e3N/c6t5nZajNbvW/fvpPpooj0aEFY5lKln2VvYO0OfTMrAZ4FvuruR1tbtZk2b6X9xEb3R9y90t0rBw4c2N4uikhP93HYZ3vo5/pVNs0sj3TgP+Xu/x407wmmbAju9wbtO4EzMzYfBuwK2oc10y4iYZErod9EjoV+cIbNY8BGd/9+xlPLgDnB8hzgpxntM82swMxGkP7AdlUwBVRlZpOCfc7O2EZEwiBnQj97K/1YO9aZDNwMvG1mbwZtXwO+BzxtZrcAO4AbAdx9vZk9DWwgfebPHe6eDLa7HXgCKAJeCm4iEhaeI3P6WXzKZpuh7+6v0/x8PMAVLWyzEFjYTPtq4LyT6aCI5BBV+t1O38gVka7TWOlnV1CeIIsrfYW+iHSdnKn0s5dCX0S6Tq6Efq6fsiki0ilyJfTD8OUsEZEOy5XQV6UvItIOuRL6WUyhLyJdJ2dCX5W+iEjb9OWsbqfQF5Guo0q/2yn0RaTrNIZ+dgXlCbK4/wp9Eek6uVjpa3pHRKQFuRL6OmVTRKQdciX0VemLiLRHjpy9k0mVvohIC3Kl0tcpmyIi7ZArZ+/olE0RkXbQl7O6nUJfRLpOrkzvZFnQZ1Loi0jXyZXpHZ2yKSLSDjlZ6Sv0RUSalyuhr0pfRKQdciX0m1Doi4g0LxdDX5W+iEgLciX0dcqmiEg75Mp5+k2+nNV9vTgVCn0R6Tqq9LudQl9Euk5jpZ9dQXmi7O2/Ql9Euk4uVvpZ9gam0BeRrpMroa8vZ4mItEOuhL4qfRGRdsiV0G9CoS8i0rycCX1V+iIirXMnZ/5cok7ZFBFpQ5N58CwPfVX6Pc+OAzVs2VvV3d0Q6ZmqPoLF42H3n7ruNTODvrOD8t/mwv+u7Nx9fuzwDti5umnbyfQ/mYAP3+jcPnVAl4e+mU0zs81mtsXM5p+O10gkU8z64X9w7zNv4Z39jyvVwyqUDcvgiWvhnV90d0/CI5Xs3P0lG+DDNaenYtz1JvzP82D/lqbt65+Dg+/Dz74CtYfTbfvegXhVuh9Ve5quv+qH8PLXTq0PbyyB1Y83Df3aQ503ju7p4znw7ifH0pkevgQevQIS8cwXPXH56C7Y+MInzXs3pX+2q/4Zfnh5+ne1B4h15YuZWRT4AfDnwE7gj2a2zN03dObrxKIR/nnYy7y96R2e/XaEPsX5mBkRoMSrqImWkbIYUUsRAaKkiFiKofGtHM0bzOH8waQsChbBLUpx4ggxr6dv/UeUH9vApj5TqCoYjGNgBhhuEcDAIuk2i2CAWxSz9HqW+Zx9vG6EaKqeooaDxPP6UFR/gFiyjmSsiNqCARQ1HCQRLSERKyaWqifiDTTklZHM64UBo9/5v/Sq3UXD9lWsOvOLRApLsWg+Hs3DI3nkJ45RkKwiXjgQjxZQHN+LRwtoKOiLRSKYQ0HtR0QTx0hF8qkrGkK0oQY8QVHtbjyST0NBXxoK+5LfUA3RPMyMvMQxkrFikvmlWCKOkSIWP0R94QAKju3CI3nUFp9BYc0uLJXEPEksfoiashHECwcBDg6WaiC/bj/1Bf2wVIKGvFIKaz6kuHoH8aLBNCSS5DVUUV96FrG8GH0Ob8BSSaoHjafXoU14rIj6kqEUHNtFNFFHfe/hJAv7QzSPwoMbidYdpm7AGGK1+yna9xbx/mNoKBmKGcRqDhBpqCYaP0zs2EfU9xtNfb9ReKyQ/MPvk3dkGw39zyVVNIDS9T+ibtilROqPUPanx0j0Gkr9kApwp+7sy0iWlkMqSbTuIKnC3hRuf43YkR3UfupqyCvG84qw+FEKd/ya+iETiB3ZBu7UD7+ckv/4J/L2vEntp64hWdgPjxXRcNZkYolj5O9bB6VDsGQ90b3rSH3qL4jUHiCy/bdweDupz9wJpYOxqj3Yoa0QiUBRX+zQNvyMCdjaJ7EjH8Cr/xUunAuHtqUDd8V30r8su9bC4gr4zN/BK9+FvGIYcE76fwBX/TcYegHsWQcv3Ztef+Rn0+H9wSo4thfOmQYH3oN9m+Gcv4DqfbB3A/QaCKP+PL3uz+5Mb/tyUOP1H5UO6F9+A865Kgh/h4LeUF8F65+HaD5s/y0MPBf+0xVQ2Bvye6XvMTCCe4MtKz755f/l1+HMSVDcD4r6Qqzwkzebg1uD3z+Dwj5QUJaRGse94dYcTP9+HtsH8SPpttf+Idh3fziQ8Sa67lkYPDb9xli1G6bcC4e3w1s/abrPF++BSAxKBqcfW+aTwQOzpo+HnJ/+mXYi6/RKuLUXM/sMsMDdrwoe3w/g7v/Q0jaVlZW+evXqlp5ukT9+DbUfvUNDIhFU+467UR3pRUmqmghJUkSCm5Fyo4YiyqiigHqipIiSJEKKWgqJkGIHQ4mSpNz3EiOJ4U1uUTv1sUy5ETGn2guppYBi6uhl8bY3BL6VvIW5kZcZaR+e8us3J+4xIjh51jkVWdzzKLCGdq1b7YWUWN0J7VVehANlVkud5xEl1WL/km40EKMweM39XsYAO9rsett8CCPsIyIZP8OW1s+0x/sw2A6f0J7wCHvoS7kdaNJ+1Isps5ombUe8mK0+hNH2ASkiFGf83Bs82mnj35Y93pdC6ultxzq0n5Z+zrWeT5HV812/hbG8zwx7tdX9HKSMfrQ+/j3JLu9PigjDbN8Jz70S+QxjU+8wmAPNbNmyuvt2UVjU65T6Y2ZvuPsJc15dWukD5cAHGY93Ap8+fiUzuw24DeCss846pReyL/6c4mba+57CvkqD+z9rz8run5yl4CncU3gqRerjW+NjJ+VJPOm4GZ5XBMl6PK+EKFCXSlFTe5BUfhmeTEKiBncjlV+C1x0l0lBNyoFYEd8cWk5e5J9Ixqs5dqyWRKKOVKIBEvUkYkUk8suw6n14sp6Ggj54KgW1h/BUEk8lSRYPgkgUi0SI1ewnlV+KmZPKLyMVKYD4EazmAMloESlPkXInEeuFNdQSq6/CIzGIRknGismrO0Qivwxi+eTV7CFRNBBLJYgm49SXlFNQtYNIojYoaAwiUZL5ZUQTx8CiRBqqSRb0paGknFj9EfILColGozQc3UO8voGqwjMglaDwyFZqS88ilUoSqztIXUF/nAixugNEGmqIJOPUFQ8hES0mr24fqWgR8fx+FB37AMyIJGppiJWQjBQAEC/oy8ZkgtKj7+Jm1OX3J144gPz4AQpr9nC0dCRFNbtIRgqoz+tNJFlLfV4ZbjF6H9lEJFVPyiLU5/WmMH6A2oLB1BQNobT6fdyMaLKeaKKGg33GURA/QCoSw1IJSo59wOHSc6jP782mZD2RvDyK6/YQqztInHwO559BXv0hiuL7OVIwlML4PpwI9dFe1OT1oaz2QwrrDxHP683RgiHkJWtJEaEu1pvBVW+Tl6rhQNFI+tZuJ2n5VOf3pzp/EL3rdnIsrz8pd/ISNRQ0HGZP0acort9Hgcc5UHQ25cc2kp+qJZ7XGweSxOgT30U8Wkw8WkK9FVKQPMbBwnJSFqW8ah2HCodxOH8opfV7GVTzDoZTHe1HVf5AqmP96JU4CLF+rCPK7rpt9EocIkkUgMJkNU6EvUUjcU9RFy2lb3wn+ak66q2AwmQ1MY83/n6ZOw5U5/VjT/5wyhL7KUxWkSJKfrKGmDcQSTUE/wt3aiMlOBCPFFOQPEZhKv3mmwqq6sa3e4e6aC9SGE6EuBWQtBh9Gz4iYfnkp2rBoT5aRFX/CorzYxTW76cmUool4wys3kRVrC/VVkp+spo4hRzKG8gvUwmGH/sTEU8GfU/3n+A4DBoL1I87My+afwqJ1bqurvRvBK5y9/8cPL4ZmOjuX25pm1Ot9EVEwqylSr+rP8jdCZyZ8XgYsKuL+yAiElpdHfp/BEaZ2QgzywdmAj3jI20RkRDo0jl9d0+Y2d8BvwCiwL+4+/qu7IOISJh19Qe5uPuLwItd/boiIpLD38gVEZETKfRFREJEoS8iEiIKfRGREOnSL2edCjPbB2w/xc0HAPs7sTvZTuPxCY1FUxqPpnJhPM5294HHN/b40O8IM1vd3DfSwkrj8QmNRVMaj6ZyeTw0vSMiEiIKfRGREMn10H+kuzvQw2g8PqGxaErj0VTOjkdOz+mLiEhTuV7pi4hIBoW+iEiI5GTod8UfX+9pzOxfzGyvma3LaOtnZr8ys3eD+74Zz90fjM9mM7uqe3p9+pjZmWb2qpltNLP1ZvaVoD10Y2JmhWa2ysz+FIzFt4P20I1FJjOLmtlaM3sheByO8XD3nLqRvmTze8BIIB/4EzCmu/vVBcc9BZgArMto+x/A/GB5PvDfg+UxwbgUACOC8Yp29zF08ngMBSYEy6XAO8Fxh25MSP8lvpJgOQ/4AzApjGNx3LjcDfw/4IXgcSjGIxcr/YnAFnd/393rgaXA9d3cp9PO3VcCB49rvh5YEiwvAaZntC9197i7bwW2kB63nOHuu919TbBcBWwk/TeaQzcmnlYdPMwLbk4Ix+JjZjYMuAZ4NKM5FOORi6Hf3B9fL++mvnS3we6+G9IhCAwK2kM1RmY2HBhPusIN5ZgEUxlvAnuBX7l7aMci8CBwL5DKaAvFeORi6FszbTovtanQjJGZlQDPAl9196OtrdpMW86Mibsn3b2C9N+lnmhm57Wyek6PhZldC+x19zfau0kzbVk7HrkY+vrj65/YY2ZDAYL7vUF7KMbIzPJIB/5T7v7vQXOox8TdDwOvAdMI71hMBq4zs22kp38vN7MfE5LxyMXQ1x9f/8QyYE6wPAf4aUb7TDMrMLMRwChgVTf077QxMwMeAza6+/czngrdmJjZQDPrEywXAVcCmwjhWAC4+/3uPszdh5POh1fc/a8Jy3h09yfJp+MGXE36bI33gK93d3+66Jj/FdgNNJCuTG4B+gMrgHeD+34Z6389GJ/NwOe6u/+nYTwuIf1f8LeAN4Pb1WEcE+B8YG0wFuuAbwXtoRuLZsZmKp+cvROK8dBlGEREQiQXp3dERKQFCn0RkRBR6IuIhIhCX0QkRBT6IiIhotAXEQkRhb6ISIj8f0DfjMY2r2rbAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = Network()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "train(\n",
    "    model=model, \n",
    "    trainloader=train_set, \n",
    "    testloader=test_set, \n",
    "    criterion=model.criterion, \n",
    "    optimizer=optimizer, \n",
    "    epochs=500, \n",
    "    print_every=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 5.2008, -9.6824, -5.7073,  ...,  7.1544, -3.5886, -0.6744],\n",
      "        [-5.2907, -1.7381, -4.6693,  ..., -6.8853,  4.3071, -3.6909],\n",
      "        [-7.2411,  5.6466, -3.8650,  ...,  3.2651,  4.0605, -2.2999],\n",
      "        ...,\n",
      "        [ 2.3048,  4.8132,  0.8749,  ...,  3.8693, -6.5397,  2.2766],\n",
      "        [-9.2177,  1.0919, -3.6577,  ..., -3.3774,  3.8541, -6.0546],\n",
      "        [ 7.6002, -2.6192, -6.8091,  ...,  5.8757, -4.1277,  7.1936]],\n",
      "       device='cuda:0', grad_fn=<LeakyReluBackward0>)\n",
      "tensor([[  5, -10,  -4,  ...,   7,  -9,  -7],\n",
      "        [ -5,  -2,  -6,  ...,  -4,   8,  -9],\n",
      "        [ -9,   9,   0,  ...,  -8,   6, -10],\n",
      "        ...,\n",
      "        [  2,   4,   3,  ...,   9, -10,  -1],\n",
      "        [-10,   0,  -5,  ...,   4,  -8, -10],\n",
      "        [  8,  -1,  -5,  ...,   8,  -4,   3]], device='cuda:0',\n",
      "       dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "test_batch = next(iter(test_set))\n",
    "img, lbls = test_batch\n",
    "img, lbls = img.to(device), lbls.to(device)\n",
    "# print(img.shape, lbls.shape)\n",
    "\n",
    "output = model.forward(img)\n",
    "print(output)\n",
    "print(lbls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('C:\\\\Users\\\\the_3\\\\Desktop\\\\poly-curve-detector\\\\Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network(\n",
       "  (cnn_layers): Sequential(\n",
       "    (0): Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): LeakyReLU(negative_slope=0.01)\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (4): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): LeakyReLU(negative_slope=0.01)\n",
       "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (8): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (9): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): LeakyReLU(negative_slope=0.01)\n",
       "    (11): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (12): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (13): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (14): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (15): LeakyReLU(negative_slope=0.01)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (fc_layers): Sequential(\n",
       "    (0): Linear(in_features=1568, out_features=4096, bias=True)\n",
       "    (1): LeakyReLU(negative_slope=0.01)\n",
       "    (2): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (3): LeakyReLU(negative_slope=0.01)\n",
       "    (4): Linear(in_features=4096, out_features=128, bias=True)\n",
       "    (5): LeakyReLU(negative_slope=0.01)\n",
       "    (6): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (7): LeakyReLU(negative_slope=0.01)\n",
       "    (8): Linear(in_features=64, out_features=8, bias=True)\n",
       "    (9): LeakyReLU(negative_slope=0.01)\n",
       "  )\n",
       "  (criterion): MSELoss()\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model = Network()\n",
    "Network.load_checkpoint(new_model, \"C:\\\\Users\\\\the_3\\\\Desktop\\\\poly-curve-detector\\\\Model\\\\model_checkpoint.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 5.2008, -9.6824, -5.7073,  ...,  7.1544, -3.5886, -0.6744],\n",
      "        [-5.2907, -1.7381, -4.6693,  ..., -6.8853,  4.3071, -3.6909],\n",
      "        [-7.2411,  5.6466, -3.8649,  ...,  3.2651,  4.0605, -2.2999],\n",
      "        ...,\n",
      "        [ 2.3048,  4.8132,  0.8749,  ...,  3.8693, -6.5397,  2.2766],\n",
      "        [-9.2177,  1.0919, -3.6577,  ..., -3.3774,  3.8541, -6.0546],\n",
      "        [ 7.6002, -2.6192, -6.8090,  ...,  5.8757, -4.1277,  7.1936]],\n",
      "       grad_fn=<LeakyReluBackward0>)\n",
      "tensor([[  5, -10,  -4,  ...,   7,  -9,  -7],\n",
      "        [ -5,  -2,  -6,  ...,  -4,   8,  -9],\n",
      "        [ -9,   9,   0,  ...,  -8,   6, -10],\n",
      "        ...,\n",
      "        [  2,   4,   3,  ...,   9, -10,  -1],\n",
      "        [-10,   0,  -5,  ...,   4,  -8, -10],\n",
      "        [  8,  -1,  -5,  ...,   8,  -4,   3]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "test_batch = next(iter(test_set))\n",
    "img, lbls = test_batch\n",
    "# img, lbls = img.to(device), lbls.to(device)\n",
    "# print(img.shape, lbls.shape)\n",
    "\n",
    "output = new_model.forward(img)\n",
    "print(output)\n",
    "print(lbls)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3a1829a56db40c3ca63cc5d173ccaf89ee3791672440d362287656aaeb413643"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit (conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
