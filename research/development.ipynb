{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to develop the model for the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All our imports\n",
    "import torch \n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from random import randint\n",
    "import PIL\n",
    "import os\n",
    "\n",
    "#for all the plots to be inline\n",
    "%matplotlib inline \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, csv_path, images_folder, transform):\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.images_folder = images_folder\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        filename = self.df.values[index][0]\n",
    "        labels = np.array([1, 1, 1, 1, 1, 1, 1, 1])\n",
    "        for x in range(0, 8):\n",
    "            labels[x] = self.df.values[index][x+1]\n",
    "        image = PIL.Image.open(os.path.join(self.images_folder, filename))\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        return image, labels\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Data Set manipulation'''\n",
    "transform = transforms.Compose([transforms.Grayscale(num_output_channels=1),\n",
    "                                transforms.ToTensor()])\n",
    "\n",
    "root = \"C:\\\\Users\\\\the_3\\\\Desktop\"\n",
    "trainDataset = CustomDataset(root + \"\\\\poly-curve-detector\\\\data\\\\plotData\\\\labels\\\\trainPlots.csv\",\n",
    "                               root + \"\\\\poly-curve-detector\\\\data\\\\plotData\\\\trainPlots\", transform)\n",
    "\n",
    "testDataset = CustomDataset(root + \"\\\\poly-curve-detector\\\\data\\\\plotData\\\\labels\\\\testPlots.csv\",\n",
    "                               root + \"\\\\poly-curve-detector\\\\data\\\\plotData\\\\testPlots\", transform)\n",
    "\n",
    "#print first label in each dataset\n",
    "#labels in order [a1,a2,a3,a4,a5,a6,a7,a8]\n",
    "# image, labels = trainDataset[0]\n",
    "# print(labels[0:9])\n",
    "# image, labels = testDataset[0]\n",
    "# print(labels[0:9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = torch.utils.data.DataLoader(trainDataset, shuffle=True, batch_size=128)\n",
    "test_set = torch.utils.data.DataLoader(testDataset, shuffle=False, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 1, 96, 96]) torch.Size([128, 8])\n",
      "Train Set 0\n",
      "Image: tensor([[[0.6745, 0.8235, 0.8235,  ..., 0.8235, 0.8235, 0.6745],\n",
      "         [0.8235, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 0.8235],\n",
      "         [0.8235, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 0.8235],\n",
      "         ...,\n",
      "         [0.8235, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 0.8235],\n",
      "         [0.8235, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 0.8235],\n",
      "         [0.6745, 0.8235, 0.8235,  ..., 0.8235, 0.8235, 0.6745]]])\n",
      "Labels: tensor([-7,  2, -2,  8,  2,  8,  7,  7], dtype=torch.int32)\n",
      "\n",
      "torch.Size([1, 96, 96]) torch.Size([8])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 1, 96, 96]) torch.Size([128, 8])\n",
      "Test Set 0\n",
      "Image: tensor([[[0.6745, 0.8235, 0.8235,  ..., 0.8235, 0.8235, 0.6745],\n",
      "         [0.8235, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 0.8235],\n",
      "         [0.8235, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 0.8235],\n",
      "         ...,\n",
      "         [0.8235, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 0.8235],\n",
      "         [0.8235, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 0.8235],\n",
      "         [0.6745, 0.8235, 0.8235,  ..., 0.8235, 0.8235, 0.6745]]])\n",
      "Labels: tensor([  5, -10,  -4,   3,  -7,   7,  -9,  -7], dtype=torch.int32)\n",
      "\n",
      "torch.Size([1, 96, 96]) torch.Size([8])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#one way to see batch size\n",
    "train_batch = next(iter(train_set))\n",
    "img, lbls = train_batch\n",
    "print(img.shape, lbls.shape)\n",
    "\n",
    "#display the first image in train_set\n",
    "#note: image changes each time run because shuffle is set to true\n",
    "for images, labels in train_set:\n",
    "    image, label = images[0], labels[0]\n",
    "    print(f\"Train Set 0\\nImage: {image}\\nLabels: {label}\\n\")\n",
    "    print(image.shape, label.shape)\n",
    "    figure = plt.figure(figsize=(4,4))\n",
    "    figure.add_subplot()\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(image.permute(1,2,0), cmap=\"gray\")\n",
    "    plt.show()\n",
    "    break\n",
    "    \n",
    "#test batch size\n",
    "test_batch = next(iter(test_set))\n",
    "img, lbls = test_batch\n",
    "print(img.shape, lbls.shape)\n",
    "\n",
    "#display the first image in test_set\n",
    "for images, labels in test_set:\n",
    "    image, label = images[0], labels[0]\n",
    "    print(f\"Test Set 0\\nImage: {image}\\nLabels: {label}\\n\")\n",
    "    print(image.shape, label.shape)\n",
    "    figure = plt.figure(figsize=(4,4))\n",
    "    figure.add_subplot()\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(image.permute(1,2,0), cmap=\"gray\")\n",
    "    plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "        # more analysis required to determine the specifics of the architecture\n",
    "       \n",
    "        self.n_output = 8\n",
    "        self.n_channel = 1\n",
    "    \n",
    "        self.cnn_layers = nn.Sequential(\n",
    "                nn.Conv2d(1, 8, 3), # convolution2dLayer(3,8,'Padding','same')\n",
    "                nn.BatchNorm2d(8),   # batchNormalizationLayer\n",
    "                nn.LeakyReLU(), # reluLayer\n",
    "                nn.MaxPool2d(2, 2), # averagePooling2dLayer(2,'Stride',2)\n",
    "                nn.Conv2d(8, 16, 3), # convolution2dLayer(3,16,'Padding','same')\n",
    "                nn.BatchNorm2d(16), # batchNormalizationLayer\n",
    "                nn.LeakyReLU(), # reluLayer\n",
    "                nn.MaxPool2d(2, 2), # averagePooling2dLayer(2,'Stride',2)\n",
    "                nn.Conv2d(16, 32, 3), # convolution2dLayer(3,32,'Padding','same')\n",
    "                nn.BatchNorm2d(32), # batchNormalizationLayer\n",
    "                nn.LeakyReLU(), # reluLayer\n",
    "                nn.Conv2d(32, 32, 3), # convolution2dLayer(3,32,'Padding','same')\n",
    "                nn.Conv2d(32, 32, 3), # convolution2dLayer(3,32,'Padding','same')\n",
    "                nn.Conv2d(32, 32, 3), # convolution2dLayer(3,32,'Padding','same')\n",
    "                nn.BatchNorm2d(32), # batchNormalizationLayer\n",
    "                nn.LeakyReLU(), # reluLayer\n",
    "                nn.MaxPool2d(2, 2) # Max pooling layer\n",
    "        )\n",
    "\n",
    "        self.n_input = 1568 # the output of maxpool 96*96 \n",
    "        #TODO:actual value might be determined from the computed output of the cnn layers\n",
    "        # https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n",
    "\n",
    "        self.fc_layers = nn.Sequential(\n",
    "#         fullyConnectedLayer(1)\n",
    "                nn.Linear(self.n_input,  4096),\n",
    "                nn.LeakyReLU(),\n",
    "                nn.Linear(4096, 4096),\n",
    "                nn.LeakyReLU(),\n",
    "                nn.Linear(4096, 128),\n",
    "                nn.LeakyReLU(),\n",
    "                nn.Linear(128, 64),\n",
    "                nn.LeakyReLU(),\n",
    "                nn.Linear(64, self.n_output),\n",
    "                nn.LeakyReLU()\n",
    "        )\n",
    "\n",
    "\n",
    "#         regressionLayer\n",
    "        self.criterion = nn.MSELoss()       \n",
    "#         dropoutLayer(0.2)\n",
    "        self.dropout =  nn.Dropout(p=0.2)\n",
    "        \n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #feedword pass through our network\n",
    "        x = self.cnn_layers(x)\n",
    "        x = x.view(x.shape[0], -1) #flatten the input tensor\n",
    "        x = self.fc_layers(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def load_checkpoint(new_model, filepath):\n",
    "        checkpoint = torch.load(filepath)\n",
    "        # model = Network(\n",
    "        #         checkpoint['input_size'], \n",
    "        #         checkpoint['output_size'],\n",
    "        #         # checkpoint['cnn_layers'],\n",
    "        #         # checkpoint['fc_layers']\n",
    "        # )\n",
    "        new_model.load_state_dict(checkpoint['state_dict'])\n",
    "        return new_model\n",
    "    \n",
    "    def save(self, dirpath):\n",
    "        self.checkpoint = {\n",
    "        #     'input_size': self.n_input, \n",
    "        #     'output_size': self.n_output,\n",
    "        #     'cnn_layers': [each. for each in model.cnn_layers],\n",
    "        #     'fc_layers': [each.out_features for each in model.fc_layers],\n",
    "            'state_dict': model.state_dict()\n",
    "        }\n",
    "        torch.save(self.checkpoint, f'{dirpath}\\\\model_checkpoint.pth')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "def validation(model, testloader, criterion):\n",
    "    accuracy = 0\n",
    "    test_loss = 0\n",
    "    for images, labels in testloader:\n",
    "        # if(labels.shape != torch.Size([128, 1, 96, 96])): continue\n",
    "        # images = images.view(images.shape[0], -1)\n",
    "        labels = labels.float()\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        output = model.forward(images)\n",
    "\n",
    "        # print(f'output={output.shape}')\n",
    "        # print(f'label={labels.shape}')\n",
    "        test_loss += criterion(output, labels)\n",
    "\n",
    "        # Calculate accuracy\n",
    "        # ps = torch.exp(output)\n",
    "        # top_p, top_class = ps.topk(1, dim=1)\n",
    "        # equals = top_class == labels.view(*top_class.shape)\n",
    "        # accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "        # accuracy += 1 - test_loss\n",
    "        ## Calculating the accuracy \n",
    "        # Model's output is log-softmax, take exponential to get the probabilities\n",
    "        # ps = torch.exp(output)\n",
    "        # Class with highest probability is our predicted class, compare with true label\n",
    "        # equality = (labels.data == ps.max(1)[1])\n",
    "        # Accuracy is number of correct predictions divided by all predictions, just take the mean\n",
    "        # accuracy += equality.type_as(torch.FloatTensor()).mean()\n",
    "        # accuracy = r2_score(labels, output)   # r2_score is the scikit learn r2 score function.\n",
    "        # print(\"accuracy = \", accuracy)   # here i get wierd values and it doesn't get better over time, in contrast the loss decreased over time\n",
    "\n",
    "    return test_loss, accuracy\n",
    "\n",
    "\n",
    "def train(model, trainloader, testloader, criterion, optimizer, epochs=5, print_every=40):\n",
    "\n",
    "    steps = 0\n",
    "    running_loss = 0\n",
    "    test_losses = []\n",
    "    train_losses = []\n",
    "    for e in range(epochs):\n",
    "        # Model in training mode, dropout is on\n",
    "        model.train()\n",
    "        for images, labels in trainloader:\n",
    "            steps += 1\n",
    "            # if(labels.shape != torch.Size([128, 1, 96, 96])): continue\n",
    "            # Flatten images into a 784 long vector\n",
    "            # images = images.view(images.shape[0], -1)\n",
    "            labels = labels.float()\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            output = model.forward(images)\n",
    "            # print(f'output={output.shape}')\n",
    "            # print(f'label={labels.shape}')\n",
    "            loss = criterion(output, labels)\n",
    "            loss.backward() # computes gradient and backpropagation\n",
    "            optimizer.step() # update of weights and biases happenss\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if steps % print_every == 0:\n",
    "                # Model in inference mode, dropout is off\n",
    "                model.eval()\n",
    "                \n",
    "                # Turn off gradients for validation, will speed up inference\n",
    "                with torch.no_grad():\n",
    "                    test_loss, accuracy = validation(model, testloader, criterion)\n",
    "                \n",
    "                train_losses.append(running_loss/len(trainloader))\n",
    "                test_losses.append(test_loss/len(testloader))\n",
    "\n",
    "                print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
    "                      \"Training Loss: {:.3f}.. \".format(running_loss/print_every),\n",
    "                      \"Test Loss: {:.3f}.. \".format(test_loss/len(testloader)),\n",
    "                    #   \"Test Accuracy: {:.3f}\".format(accuracy/len(testloader))\n",
    "                      )\n",
    "                \n",
    "                running_loss = 0\n",
    "                \n",
    "                # Make sure dropout and grads are on for training\n",
    "                model.train()\n",
    "                \n",
    "    plt.plot(train_losses, label='Training loss')\n",
    "    plt.plot(test_losses, label='Validation loss')\n",
    "    plt.legend(frameon=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Network()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "# output = model.forward(img)\n",
    "# print(output)\n",
    "# print(lbls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(output.shape)\n",
    "# output[0,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DONE A: Train model on the data set\n",
    "    # DONE 1: dataloader be able load (image: tensor, label:string) example of the label tensor([10,-4,-5,3,-2,-3,-2,2]) \n",
    "    # DONE 2: fix dimensionality so the net can feedforward a batch of images correctly, move the training to gpu when available\n",
    "    # DONE 3: make trainloader and testloader dataloaders\n",
    "# DONE B: Improve model\n",
    "    # DONE 4: update checkpoint structure to save model and load model\n",
    "    # DONE 5: use better graphics to display the results (Josias)\n",
    "    # DONE 6: Find out if cnn layers learn (Josias)\n",
    "    # DONE 7: Fix the testloader issue where 8 size expected but found 96 (Brandon)\n",
    "    # DONE 8: Build a list of ideas to improve the nework (Brandon and Josias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List of ideas to improve the network\n",
    "\n",
    "1. ## Add a grid with finer blocks to the input as transform\n",
    "2. ## Revise the architecture\n",
    "3. ## Add dropout to the network\n",
    "4. ## Find a way to measure accuracy of multiple regression network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (128x1568 and 2592x4096)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-75-a1e61e671964>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m train(\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mtrainloader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-71-75f48d1e14aa>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, trainloader, testloader, criterion, optimizer, epochs, print_every)\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m             \u001b[1;31m# print(f'output={output.shape}')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m             \u001b[1;31m# print(f'label={labels.shape}')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-70-8d41f78dd79b>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcnn_layers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#flatten the input tensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc_layers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    137\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 139\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    140\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1845\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1846\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1847\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1848\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (128x1568 and 2592x4096)"
     ]
    }
   ],
   "source": [
    "model = Network()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "train(\n",
    "    model=model, \n",
    "    trainloader=train_set, \n",
    "    testloader=test_set, \n",
    "    criterion=model.criterion, \n",
    "    optimizer=optimizer, \n",
    "    epochs=500, \n",
    "    print_every=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.9188e-03, -1.0318e+01, -2.5205e+00,  ..., -4.0270e-01,\n",
      "         -6.3425e+00, -4.6076e-01],\n",
      "        [-5.8994e+00, -1.3170e+00, -1.3576e+00,  ..., -2.4494e+00,\n",
      "         -6.4682e-01, -6.3323e+00],\n",
      "        [-6.6151e-02,  9.4769e+00,  1.3831e+00,  ..., -8.9290e-01,\n",
      "         -1.8079e-01, -2.6459e+00],\n",
      "        ...,\n",
      "        [ 4.1966e+00,  5.2860e+00, -5.1674e-02,  ..., -2.2283e-01,\n",
      "         -2.2618e-01, -7.1484e-01],\n",
      "        [-7.2576e+00, -1.3206e+00, -1.9953e+00,  ..., -2.2589e+00,\n",
      "         -9.6927e-01, -5.5885e+00],\n",
      "        [ 7.2900e+00, -1.3182e+00, -6.4693e-02,  ...,  2.0132e+00,\n",
      "         -1.7100e-01,  7.1618e+00]], device='cuda:0',\n",
      "       grad_fn=<LeakyReluBackward0>)\n",
      "tensor([[  5, -10,  -4,  ...,   7,  -9,  -7],\n",
      "        [ -5,  -2,  -6,  ...,  -4,   8,  -9],\n",
      "        [ -9,   9,   0,  ...,  -8,   6, -10],\n",
      "        ...,\n",
      "        [  2,   4,   3,  ...,   9, -10,  -1],\n",
      "        [-10,   0,  -5,  ...,   4,  -8, -10],\n",
      "        [  8,  -1,  -5,  ...,   8,  -4,   3]], device='cuda:0',\n",
      "       dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "test_batch = next(iter(test_set))\n",
    "img, lbls = test_batch\n",
    "img, lbls = img.to(device), lbls.to(device)\n",
    "# print(img.shape, lbls.shape)\n",
    "\n",
    "output = model.forward(img)\n",
    "print(output)\n",
    "print(lbls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('C:\\\\Users\\\\the_3\\\\Desktop\\\\poly-curve-detector\\\\Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network(\n",
       "  (cnn_layers): Sequential(\n",
       "    (0): Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): LeakyReLU(negative_slope=0.01)\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (4): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): LeakyReLU(negative_slope=0.01)\n",
       "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (8): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (9): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): LeakyReLU(negative_slope=0.01)\n",
       "    (11): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (12): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (13): LeakyReLU(negative_slope=0.01)\n",
       "    (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (fc_layers): Sequential(\n",
       "    (0): Linear(in_features=2592, out_features=1024, bias=True)\n",
       "    (1): LeakyReLU(negative_slope=0.01)\n",
       "    (2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "    (3): LeakyReLU(negative_slope=0.01)\n",
       "    (4): Linear(in_features=256, out_features=64, bias=True)\n",
       "    (5): LeakyReLU(negative_slope=0.01)\n",
       "    (6): Linear(in_features=64, out_features=8, bias=True)\n",
       "    (7): LeakyReLU(negative_slope=0.01)\n",
       "  )\n",
       "  (criterion): MSELoss()\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model = Network()\n",
    "Network.load_checkpoint(new_model, \"C:\\\\Users\\\\the_3\\\\Desktop\\\\poly-curve-detector\\\\Model\\\\model_checkpoint.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.9186e-03, -1.0318e+01, -2.5205e+00,  ..., -4.0270e-01,\n",
      "         -6.3425e+00, -4.6076e-01],\n",
      "        [-5.8994e+00, -1.3170e+00, -1.3576e+00,  ..., -2.4494e+00,\n",
      "         -6.4682e-01, -6.3323e+00],\n",
      "        [-6.6151e-02,  9.4769e+00,  1.3831e+00,  ..., -8.9290e-01,\n",
      "         -1.8079e-01, -2.6459e+00],\n",
      "        ...,\n",
      "        [ 4.1966e+00,  5.2859e+00, -5.1674e-02,  ..., -2.2283e-01,\n",
      "         -2.2618e-01, -7.1484e-01],\n",
      "        [-7.2576e+00, -1.3206e+00, -1.9953e+00,  ..., -2.2589e+00,\n",
      "         -9.6927e-01, -5.5885e+00],\n",
      "        [ 7.2900e+00, -1.3182e+00, -6.4693e-02,  ...,  2.0132e+00,\n",
      "         -1.7100e-01,  7.1618e+00]], grad_fn=<LeakyReluBackward0>)\n",
      "tensor([[  5, -10,  -4,  ...,   7,  -9,  -7],\n",
      "        [ -5,  -2,  -6,  ...,  -4,   8,  -9],\n",
      "        [ -9,   9,   0,  ...,  -8,   6, -10],\n",
      "        ...,\n",
      "        [  2,   4,   3,  ...,   9, -10,  -1],\n",
      "        [-10,   0,  -5,  ...,   4,  -8, -10],\n",
      "        [  8,  -1,  -5,  ...,   8,  -4,   3]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "test_batch = next(iter(test_set))\n",
    "img, lbls = test_batch\n",
    "# img, lbls = img.to(device), lbls.to(device)\n",
    "# print(img.shape, lbls.shape)\n",
    "\n",
    "output = new_model.forward(img)\n",
    "print(output)\n",
    "print(lbls)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3a1829a56db40c3ca63cc5d173ccaf89ee3791672440d362287656aaeb413643"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit (conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
